{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOKENIZATION\n",
    "\n",
    "NLP includes a great variety of procedures. Tokenization is one of them. The main task is to split a sequence of characters into units, called tokens. Tokens are usually represented by words, numbers, or punctuation marks. Sometimes, they can be represented by sentences or morphemes (word parts). Tokenization is the first step in text preprocessing. It is a very important procedure; before going to more sophisticated NLP procedures, we need to identify words that can help us interpret the meaning.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization in NLTK\n",
    "\n",
    "NLTK has the tokenize module that consists of different sub-modules. We will take a look at the most significant ones. The chart below describes some of them. The first column contains the names of tokenizers. To import a particular one, use from nltk.tokenize import <tokenizer>. Here are some examples of importing:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/volkan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- word_tokenize()                                 Returns word and punctuation tokens.\n",
    "\n",
    "- WordPunctTokenizer()       Returns tokens from a string of alphabetic or non-alphabetic characters (like integers, $, @...).\n",
    "\n",
    "- regexp_tokenize()          Returns tokens using standard regular expressions.\n",
    "\n",
    "- TreebankWordTokenizer()    Returns the tokens as in the Penn Treebank using regular expressions.\n",
    "\n",
    "- sent_tokenize()            Returns tokenized sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word tokenization\n",
    "\n",
    "Let's take a look at an example. Imagine we have a string of three sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I have got a cat. My cat's name is C-3PO. He's golden.\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's have a look at each tokenization method from the table. Don't forget to import all of them in advance.\n",
    "In the example below, we pass the text variable to the word_tokenize() method:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'have', 'got', 'a', 'cat', '.', 'My', 'cat', \"'s\", 'name', 'is', 'C-3PO', '.', 'He', \"'s\", 'golden', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a list of strings (tokens). The function splits the string into words and punctuation marks. Mind the possessives and the contractions. The tokenizer transformes all 's into separate words. Of course, we understand that cat's could also be recognized as one token.\n",
    "\n",
    "The next code snippet introduces the WordPunctTokenizer(). This tokenizer is similar to the first one, but the result is a little bit different. All the punctuation marks including dashes and apostrophes are separate tokens. Now, C-3PO, the cat's name, is split into three tokens. In this case, this behavior is not optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'have', 'got', 'a', 'cat', '.', 'My', 'cat', \"'\", 's', 'name', 'is', 'C', '-', '3PO', '.', 'He', \"'\", 's', 'golden', '.']\n"
     ]
    }
   ],
   "source": [
    "wpt = nltk.WordPunctTokenizer()\n",
    "print(wpt.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I   h a v e   g o t   a   c a t .   M y   c a t' s   n a m e   i s   C - 3 P O .   H e' s   g o l d e n.\n"
     ]
    }
   ],
   "source": [
    "ybw = nltk.TreebankWordDetokenizer()\n",
    "print(ybw.tokenize(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TreebankWordTokenizer() works almost the same way as the word_tokenize(). Mind full stops â€“ they form a token with the previous word, but the last full stop is a separate token. Word_tokenize(), on the contrary, recognizes full stops as separate tokens in all cases. Moreover, the apostrophe and s are not separated as with WordPunctTokenizer()."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now move on to the next method. The regexp_tokenize() function uses regular expressions and accepts two arguments: a string and a pattern for tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'have', 'got', 'a', 'cat', 'My', 'cat', 's', 'name', 'is', 'C', 'PO', 'He', 's', 'golden']\n",
      "['I', 'have', 'got', 'a', 'cat', 'My', 'cat', 's', 'name', 'is', 'C', '3PO', 'He', 's', 'golden']\n",
      "['I', 'have', 'got', 'a', 'cat', 'My', \"cat's\", 'name', 'is', 'C', '3PO', \"He's\", 'golden']\n",
      "['I', 'have', 'got', 'a', 'cat', 'My', \"cat's\", 'name', 'is', 'C-3PO', \"He's\", 'golden']\n"
     ]
    }
   ],
   "source": [
    "# 1 \n",
    "print(nltk.regexp_tokenize(text, \"[A-z]+\"))\n",
    "# ['I', 'have', 'got', 'a', 'cat', 'My', 'cat', 's', 'name', 'is', 'C', 'PO', 'He', 's', 'golden']\n",
    "\n",
    "# 2\n",
    "print(nltk.regexp_tokenize(text, \"[0-9A-z]+\"))\n",
    "# ['I', 'have', 'got', 'a', 'cat', 'My', 'cat', 's', 'name', 'is', 'C', '3PO', 'He', 's', 'golden']\n",
    "\n",
    "# 3\n",
    "print(nltk.regexp_tokenize(text, \"[0-9A-z']+\"))\n",
    "# ['I', 'have', 'got', 'a', 'cat', 'My', \"cat's\", 'name', 'is', 'C', '3PO', \"He's\", 'golden']\n",
    "\n",
    "# 4\n",
    "print(nltk.regexp_tokenize(text, \"[0-9A-z'\\-]+\"))\n",
    "# ['I', 'have', 'got', 'a', 'cat', 'My', \"cat's\", 'name', 'is', 'C-3PO', \"He's\", 'golden']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENTENCE TOKENIZATION\n",
    "let's look at the sent_tokenize() module. It splits a string into sentences:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I have got a cat.', \"My cat's name is C-3PO.\", \"He's golden.\"]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.sent_tokenize(text))\n",
    "# ['I have got a cat.', \"My cat's name is C-3PO.\", \"He's golden.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mrs. Beam lives in the U.S.A., it is her motherland.', 'She lost about 9 kilos (20 lbs.)', 'last year.']\n"
     ]
    }
   ],
   "source": [
    "text_2 = \"Mrs. Beam lives in the U.S.A., it is her motherland. She lost about 9 kilos (20 lbs.) last year.\"\n",
    "print(sent_tokenize(text_2))\n",
    "# ['Mrs. Beam lives in the U.S.A., it is her motherland.', 'She lost about 9 kilos (20 lbs.)', 'last year.']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sent_tokenize() includes a list of typical abbreviations and contractions with dots, so they are not recognized as the end of a sentence. Sometimes, it still provides confusing results. For example, after tokenizing the text_2 above, .) was recognized as the end of the sentence. It is a mistake. The last part in the tokenizer output is 'last year.' but it should belong to the previous sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The plot of the film is cool!!!!!!!', \"but the characters leave much to be desired....i don't like them.\"]\n"
     ]
    }
   ],
   "source": [
    "text_3 = \"The plot of the film is cool!!!!!!! but the characters leave much to be desired....i don't like them.\"\n",
    "print(sent_tokenize(text_3))\n",
    "# ['The plot of the film is cool!!!!!!!', \"but the characters leave much to be desired....i don't like them.\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
