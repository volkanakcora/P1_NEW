* 
* ==> Audit <==
* |--------------|----------------|----------|--------|---------|---------------------|---------------------|
|   Command    |      Args      | Profile  |  User  | Version |     Start Time      |      End Time       |
|--------------|----------------|----------|--------|---------|---------------------|---------------------|
| start        |                | minikube | volkan | v1.32.0 | 14 Nov 23 20:06 CET |                     |
| delete       |                | minikube | volkan | v1.32.0 | 14 Nov 23 20:13 CET | 14 Nov 23 20:13 CET |
| start        |                | minikube | volkan | v1.32.0 | 14 Nov 23 20:13 CET |                     |
| start        |                | minikube | volkan | v1.32.0 | 14 Nov 23 20:21 CET |                     |
| delete       |                | minikube | volkan | v1.32.0 | 14 Nov 23 20:24 CET | 14 Nov 23 20:24 CET |
| start        |                | minikube | volkan | v1.32.0 | 14 Nov 23 20:24 CET | 14 Nov 23 20:26 CET |
| delete       |                | minikube | volkan | v1.32.0 | 14 Nov 23 20:33 CET | 14 Nov 23 20:33 CET |
| start        | --nodes=5      | minikube | volkan | v1.32.0 | 14 Nov 23 20:34 CET | 14 Nov 23 20:44 CET |
| update-check |                | minikube | volkan | v1.32.0 | 14 Nov 23 20:56 CET |                     |
| addons       | enable ingress | minikube | volkan | v1.32.0 | 14 Nov 23 21:03 CET |                     |
|--------------|----------------|----------|--------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2023/11/14 20:34:09
Running on machine: development
Binary: Built with gc go1.21.3 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1114 20:34:09.551856   12113 out.go:296] Setting OutFile to fd 1 ...
I1114 20:34:09.552080   12113 out.go:343] TERM=screen,COLORTERM=, which probably does not support color
I1114 20:34:09.552084   12113 out.go:309] Setting ErrFile to fd 2...
I1114 20:34:09.552087   12113 out.go:343] TERM=screen,COLORTERM=, which probably does not support color
I1114 20:34:09.552172   12113 root.go:338] Updating PATH: /home/volkan/.minikube/bin
I1114 20:34:09.552544   12113 out.go:303] Setting JSON to false
I1114 20:34:09.554203   12113 start.go:128] hostinfo: {"hostname":"development.myguest.virtualbox.org","uptime":614,"bootTime":1699989835,"procs":155,"os":"linux","platform":"rocky","platformFamily":"rhel","platformVersion":"8.8","kernelVersion":"4.18.0-477.27.1.el8_8.x86_64","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"daff999e-c18f-416c-a239-d990faee5023"}
I1114 20:34:09.554234   12113 start.go:138] virtualization:  
I1114 20:34:09.561676   12113 out.go:177] * minikube v1.32.0 on Rocky 8.8
I1114 20:34:09.567795   12113 driver.go:378] Setting default libvirt URI to qemu:///system
I1114 20:34:09.567908   12113 global.go:111] Querying for installed drivers using PATH=/home/volkan/.minikube/bin:/home/volkan/.local/bin:/usr/local/go/bin:/usr/bin:/home/volkan/.local/bin:/usr/local/go/bin:/usr/condabin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
I1114 20:34:09.568258   12113 notify.go:220] Checking for updates...
I1114 20:34:09.581674   12113 global.go:122] none default: false priority: 4, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:running the 'none' driver as a regular user requires sudo permissions Reason: Fix: Doc: Version:}
I1114 20:34:09.638157   12113 podman.go:123] podman version: 4.4.1
I1114 20:34:09.638175   12113 global.go:122] podman default: true priority: 7, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1114 20:34:09.638191   12113 global.go:122] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1114 20:34:09.638221   12113 global.go:122] kvm2 default: true priority: 8, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "virsh": executable file not found in $PATH Reason: Fix:Install libvirt Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/kvm2/ Version:}
I1114 20:34:09.638244   12113 global.go:122] qemu2 default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-x86_64": executable file not found in $PATH Reason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I1114 20:34:09.638272   12113 global.go:122] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I1114 20:34:09.638283   12113 global.go:122] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in $PATH Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I1114 20:34:09.638297   12113 global.go:122] docker default: true priority: 9, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "docker": executable file not found in $PATH Reason: Fix:Install Docker Doc:https://minikube.sigs.k8s.io/docs/drivers/docker/ Version:}
I1114 20:34:09.638308   12113 driver.go:313] not recommending "ssh" due to default: false
I1114 20:34:09.638317   12113 driver.go:348] Picked: podman
I1114 20:34:09.638323   12113 driver.go:349] Alternatives: [ssh]
I1114 20:34:09.638326   12113 driver.go:350] Rejects: [none kvm2 qemu2 virtualbox vmware docker]
I1114 20:34:09.642548   12113 out.go:177] * Automatically selected the podman driver
I1114 20:34:09.645195   12113 start.go:298] selected driver: podman
I1114 20:34:09.645200   12113 start.go:902] validating driver "podman" against <nil>
I1114 20:34:09.645207   12113 start.go:913] status for podman: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1114 20:34:09.645269   12113 cli_runner.go:164] Run: sudo -n podman system info --format json
I1114 20:34:09.790523   12113 info.go:288] podman info: {Host:{BuildahVersion:1.29.0 CgroupVersion:v1 Conmon:{Package:conmon-2.1.6-1.module+el8.8.0+1265+fa25dd7a.x86_64 Path:/usr/bin/conmon Version:conmon version 2.1.6, commit: a88a21e8953a6243d5f369f61a342bcaf0630aa1} Distribution:{Distribution:"rocky" Version:8.8} MemFree:7416619008 MemTotal:10082746368 OCIRuntime:{Name:runc Package:runc-1.1.4-1.module+el8.8.0+1265+fa25dd7a.x86_64 Path:/usr/bin/runc Version:runc version 1.1.4
spec: 1.0.2-dev
go: go1.19.4
libseccomp: 2.5.2} SwapFree:1073737728 SwapTotal:1073737728 Arch:amd64 Cpus:6 Eventlogger:file Hostname:development.myguest.virtualbox.org Kernel:4.18.0-477.27.1.el8_8.x86_64 Os:linux Security:{Rootless:false} Uptime:0h 10m 14.00s} Registries:{Search:[registry.access.redhat.com registry.redhat.io docker.io]} Store:{ConfigFile:/etc/containers/storage.conf ContainerStore:{Number:0} GraphDriverName:overlay GraphOptions:{} GraphRoot:/var/lib/containers/storage GraphStatus:{BackingFilesystem:extfs NativeOverlayDiff:false SupportsDType:true UsingMetacopy:true} ImageStore:{Number:1} RunRoot:/run/containers/storage VolumePath:/var/lib/containers/storage/volumes}}
I1114 20:34:09.790595   12113 start_flags.go:309] no existing cluster config was found, will generate one from the flags 
I1114 20:34:09.791377   12113 start_flags.go:394] Using suggested 2200MB memory alloc based on sys=9615MB, container=9615MB
I1114 20:34:09.791473   12113 start_flags.go:913] Wait components to verify : map[apiserver:true system_pods:true]
I1114 20:34:09.795497   12113 out.go:177] * Using Podman driver with root privileges
I1114 20:34:09.798364   12113 cni.go:84] Creating CNI manager for ""
I1114 20:34:09.798369   12113 cni.go:136] 0 nodes found, recommending kindnet
I1114 20:34:09.798373   12113 start_flags.go:318] Found "CNI" CNI - setting NetworkPlugin=cni
I1114 20:34:09.798383   12113 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:podman HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/volkan:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I1114 20:34:09.800665   12113 out.go:177] * Starting control plane node minikube in cluster minikube
I1114 20:34:09.804548   12113 cache.go:121] Beginning downloading kic base image for podman with docker
I1114 20:34:09.806742   12113 out.go:177] * Pulling base image ...
I1114 20:34:09.809433   12113 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I1114 20:34:09.809453   12113 preload.go:148] Found local preload: /home/volkan/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4
I1114 20:34:09.809457   12113 cache.go:56] Caching tarball of preloaded images
I1114 20:34:09.809496   12113 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 to local cache
I1114 20:34:09.809528   12113 preload.go:174] Found /home/volkan/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1114 20:34:09.809533   12113 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I1114 20:34:09.809618   12113 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local cache directory
I1114 20:34:09.809631   12113 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local cache directory, skipping pull
I1114 20:34:09.809691   12113 image.go:105] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in cache, skipping pull
I1114 20:34:09.809696   12113 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 as a tarball
I1114 20:34:09.809805   12113 profile.go:148] Saving config to /home/volkan/.minikube/profiles/minikube/config.json ...
I1114 20:34:09.809815   12113 lock.go:35] WriteFile acquiring /home/volkan/.minikube/profiles/minikube/config.json: {Name:mk7e4f5e599d40ccd77ae08820a4db97a4aac67d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
E1114 20:34:09.809931   12113 cache.go:189] Error downloading kic artifacts:  not yet implemented, see issue #8426
I1114 20:34:09.809962   12113 cache.go:194] Successfully downloaded all kic artifacts
I1114 20:34:09.809972   12113 start.go:365] acquiring machines lock for minikube: {Name:mk41df64ae503945e1d8aeb2d9ca79bb9dbe5343 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1114 20:34:09.810271   12113 start.go:369] acquired machines lock for "minikube" in 293.043µs
I1114 20:34:09.810277   12113 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:podman HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/volkan:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:} &{Name: IP: Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I1114 20:34:09.810305   12113 start.go:125] createHost starting for "" (driver="podman")
I1114 20:34:09.817035   12113 out.go:204] * Creating podman container (CPUs=2, Memory=2200MB) ...
I1114 20:34:09.817701   12113 start.go:159] libmachine.API.Create for "minikube" (driver="podman")
I1114 20:34:09.817744   12113 client.go:168] LocalClient.Create starting
I1114 20:34:09.817772   12113 main.go:141] libmachine: Reading certificate data from /home/volkan/.minikube/certs/ca.pem
I1114 20:34:09.817787   12113 main.go:141] libmachine: Decoding PEM data...
I1114 20:34:09.817794   12113 main.go:141] libmachine: Parsing certificate...
I1114 20:34:09.817833   12113 main.go:141] libmachine: Reading certificate data from /home/volkan/.minikube/certs/cert.pem
I1114 20:34:09.817840   12113 main.go:141] libmachine: Decoding PEM data...
I1114 20:34:09.817844   12113 main.go:141] libmachine: Parsing certificate...
I1114 20:34:09.818232   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:34:09.899348   12113 cli_runner.go:164] Run: sudo -n podman network inspect minikube --format "{{range .}}{{if eq .Driver "bridge"}}{{(index .Subnets 0).Subnet}},{{(index .Subnets 0).Gateway}}{{end}}{{end}}"
W1114 20:34:09.961156   12113 cli_runner.go:211] sudo -n podman network inspect minikube --format "{{range .}}{{if eq .Driver "bridge"}}{{(index .Subnets 0).Subnet}},{{(index .Subnets 0).Gateway}}{{end}}{{end}}" returned with exit code 125
I1114 20:34:09.961203   12113 network_create.go:281] running [podman network inspect minikube] to gather additional debugging logs...
I1114 20:34:09.961220   12113 cli_runner.go:164] Run: sudo -n podman network inspect minikube
W1114 20:34:10.016730   12113 cli_runner.go:211] sudo -n podman network inspect minikube returned with exit code 125
I1114 20:34:10.016754   12113 network_create.go:284] error running [sudo -n podman network inspect minikube]: sudo -n podman network inspect minikube: exit status 125
stdout:
[]

stderr:
Error: inspecting object: network minikube: unable to find network with name or ID minikube: network not found
I1114 20:34:10.016761   12113 network_create.go:286] output of [sudo -n podman network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error: inspecting object: network minikube: unable to find network with name or ID minikube: network not found

** /stderr **
I1114 20:34:10.016804   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:34:10.096537   12113 cli_runner.go:164] Run: sudo -n podman network inspect podman --format "{{range .}}{{if eq .Driver "bridge"}}{{(index .Subnets 0).Subnet}},{{(index .Subnets 0).Gateway}}{{end}}{{end}}"
I1114 20:34:10.154626   12113 network.go:209] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc0022b4390}
I1114 20:34:10.154644   12113 network_create.go:124] attempt to create podman network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 0 ...
I1114 20:34:10.154673   12113 cli_runner.go:164] Run: sudo -n podman network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I1114 20:34:10.218282   12113 network_create.go:108] podman network minikube 192.168.49.0/24 created
I1114 20:34:10.218297   12113 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I1114 20:34:10.218331   12113 cli_runner.go:164] Run: sudo -n podman ps -a --format {{.Names}}
I1114 20:34:10.309366   12113 cli_runner.go:164] Run: sudo -n podman volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I1114 20:34:10.372526   12113 oci.go:103] Successfully created a podman volume minikube
I1114 20:34:10.372674   12113 cli_runner.go:164] Run: sudo -n podman run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.42 -d /var/lib
I1114 20:34:12.144489   12113 cli_runner.go:217] Completed: sudo -n podman run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.42 -d /var/lib: (1.771790517s)
I1114 20:34:12.144500   12113 oci.go:107] Successfully prepared a podman volume minikube
I1114 20:34:12.144653   12113 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I1114 20:34:12.144666   12113 kic.go:194] Starting extracting preloaded images to volume ...
I1114 20:34:12.144716   12113 cli_runner.go:164] Run: sudo -n podman run --rm --entrypoint /usr/bin/tar --security-opt label=disable -v /home/volkan/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.42 -I lz4 -xf /preloaded.tar -C /extractDir
I1114 20:34:15.605363   12113 cli_runner.go:217] Completed: sudo -n podman run --rm --entrypoint /usr/bin/tar --security-opt label=disable -v /home/volkan/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.42 -I lz4 -xf /preloaded.tar -C /extractDir: (3.460625669s)
I1114 20:34:15.605377   12113 kic.go:203] duration metric: took 3.460709 seconds to extract preloaded images to volume
W1114 20:34:15.605615   12113 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
I1114 20:34:15.605664   12113 cli_runner.go:164] Run: sudo -n podman info --format "'{{json .SecurityOptions}}'"
W1114 20:34:15.810536   12113 cli_runner.go:211] sudo -n podman info --format "'{{json .SecurityOptions}}'" returned with exit code 125
I1114 20:34:15.810588   12113 cli_runner.go:164] Run: sudo -n podman run --cgroup-manager cgroupfs -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var:exec --memory=2200mb --cpus=2 -e container=podman --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.42
I1114 20:34:16.375634   12113 cli_runner.go:164] Run: sudo -n podman container inspect minikube --format={{.State.Running}}
I1114 20:34:16.485530   12113 cli_runner.go:164] Run: sudo -n podman container inspect minikube --format={{.State.Status}}
I1114 20:34:16.582926   12113 cli_runner.go:164] Run: sudo -n podman exec minikube stat /var/lib/dpkg/alternatives/iptables
I1114 20:34:16.909192   12113 oci.go:144] the created container "minikube" has a running status.
I1114 20:34:16.909205   12113 kic.go:225] Creating ssh key for kic: /home/volkan/.minikube/machines/minikube/id_rsa...
I1114 20:34:17.059225   12113 kic_runner.go:191] podman (temp): /home/volkan/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I1114 20:34:17.061039   12113 kic_runner.go:261] Run: /usr/bin/sudo -n podman cp /tmp/tmpf-memory-asset3841351457 minikube:/home/docker/.ssh/authorized_keys
I1114 20:34:17.350780   12113 cli_runner.go:164] Run: sudo -n podman container inspect minikube --format={{.State.Status}}
I1114 20:34:17.412592   12113 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I1114 20:34:17.412605   12113 kic_runner.go:114] Args: [sudo -n podman exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I1114 20:34:17.617346   12113 cli_runner.go:164] Run: sudo -n podman container inspect minikube --format={{.State.Status}}
I1114 20:34:17.670935   12113 machine.go:88] provisioning docker machine ...
I1114 20:34:17.671158   12113 ubuntu.go:169] provisioning hostname "minikube"
I1114 20:34:17.671200   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:34:17.714409   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1114 20:34:17.776201   12113 main.go:141] libmachine: Using SSH client type: native
I1114 20:34:17.776418   12113 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 40883 <nil> <nil>}
I1114 20:34:17.776428   12113 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1114 20:34:17.776648   12113 main.go:141] libmachine: Error dialing TCP: dial tcp 127.0.0.1:40883: connect: connection refused
I1114 20:34:20.777526   12113 main.go:141] libmachine: Error dialing TCP: dial tcp 127.0.0.1:40883: connect: connection refused
I1114 20:34:24.438015   12113 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1114 20:34:24.438060   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:34:24.657234   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1114 20:34:24.901358   12113 main.go:141] libmachine: Using SSH client type: native
I1114 20:34:24.901546   12113 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 40883 <nil> <nil>}
I1114 20:34:24.901553   12113 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1114 20:34:25.211268   12113 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1114 20:34:25.215602   12113 ubuntu.go:175] set auth options {CertDir:/home/volkan/.minikube CaCertPath:/home/volkan/.minikube/certs/ca.pem CaPrivateKeyPath:/home/volkan/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/volkan/.minikube/machines/server.pem ServerKeyPath:/home/volkan/.minikube/machines/server-key.pem ClientKeyPath:/home/volkan/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/volkan/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/volkan/.minikube}
I1114 20:34:25.215616   12113 ubuntu.go:177] setting up certificates
I1114 20:34:25.215623   12113 provision.go:83] configureAuth start
I1114 20:34:25.220257   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f {{.NetworkSettings.IPAddress}} minikube
I1114 20:34:25.346262   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1114 20:34:25.445454   12113 provision.go:138] copyHostCerts
I1114 20:34:25.445484   12113 exec_runner.go:144] found /home/volkan/.minikube/ca.pem, removing ...
I1114 20:34:25.445489   12113 exec_runner.go:203] rm: /home/volkan/.minikube/ca.pem
I1114 20:34:25.445561   12113 exec_runner.go:151] cp: /home/volkan/.minikube/certs/ca.pem --> /home/volkan/.minikube/ca.pem (1078 bytes)
I1114 20:34:25.445605   12113 exec_runner.go:144] found /home/volkan/.minikube/cert.pem, removing ...
I1114 20:34:25.445607   12113 exec_runner.go:203] rm: /home/volkan/.minikube/cert.pem
I1114 20:34:25.445621   12113 exec_runner.go:151] cp: /home/volkan/.minikube/certs/cert.pem --> /home/volkan/.minikube/cert.pem (1119 bytes)
I1114 20:34:25.445643   12113 exec_runner.go:144] found /home/volkan/.minikube/key.pem, removing ...
I1114 20:34:25.445644   12113 exec_runner.go:203] rm: /home/volkan/.minikube/key.pem
I1114 20:34:25.445653   12113 exec_runner.go:151] cp: /home/volkan/.minikube/certs/key.pem --> /home/volkan/.minikube/key.pem (1675 bytes)
I1114 20:34:25.445671   12113 provision.go:112] generating server cert: /home/volkan/.minikube/machines/server.pem ca-key=/home/volkan/.minikube/certs/ca.pem private-key=/home/volkan/.minikube/certs/ca-key.pem org=volkan.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I1114 20:34:25.659716   12113 provision.go:172] copyRemoteCerts
I1114 20:34:25.659760   12113 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1114 20:34:25.659785   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:34:25.722393   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1114 20:34:25.788062   12113 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:40883 SSHKeyPath:/home/volkan/.minikube/machines/minikube/id_rsa Username:docker}
I1114 20:34:25.881004   12113 ssh_runner.go:362] scp /home/volkan/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I1114 20:34:25.911994   12113 ssh_runner.go:362] scp /home/volkan/.minikube/machines/server.pem --> /etc/docker/server.pem (1200 bytes)
I1114 20:34:25.941949   12113 ssh_runner.go:362] scp /home/volkan/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1114 20:34:25.978800   12113 provision.go:86] duration metric: configureAuth took 763.169609ms
I1114 20:34:25.978812   12113 ubuntu.go:193] setting minikube options for container-runtime
I1114 20:34:25.978955   12113 config.go:182] Loaded profile config "minikube": Driver=podman, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1114 20:34:25.979128   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:34:26.037485   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1114 20:34:26.150100   12113 main.go:141] libmachine: Using SSH client type: native
I1114 20:34:26.150297   12113 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 40883 <nil> <nil>}
I1114 20:34:26.150301   12113 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1114 20:34:26.279950   12113 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1114 20:34:26.279961   12113 ubuntu.go:71] root file system type: overlay
I1114 20:34:26.280048   12113 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1114 20:34:26.280098   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:34:26.347580   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1114 20:34:26.433282   12113 main.go:141] libmachine: Using SSH client type: native
I1114 20:34:26.433487   12113 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 40883 <nil> <nil>}
I1114 20:34:26.433525   12113 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment="HTTP_PROXY=http://webproxy.deutsche-boerse.de:8080"
Environment="HTTPS_PROXY=http://webproxy.deutsche-boerse.de:8080"
Environment="NO_PROXY=localhost,127.0.0.1,.dbgcloud.io,.deutsche-boerse.de,172.23.65.1/24,192.168.58.0/24,192.168.59.0/24,192.168.39.0/24,192.168.49.0/24,10.96.0.0/12"


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=podman --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1114 20:34:26.588923   12113 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment=HTTP_PROXY=http://webproxy.deutsche-boerse.de:8080
Environment=HTTPS_PROXY=http://webproxy.deutsche-boerse.de:8080
Environment=NO_PROXY=localhost,127.0.0.1,.dbgcloud.io,.deutsche-boerse.de,172.23.65.1/24,192.168.58.0/24,192.168.59.0/24,192.168.39.0/24,192.168.49.0/24,10.96.0.0/12


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=podman --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1114 20:34:26.589040   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:34:26.647316   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1114 20:34:26.742857   12113 main.go:141] libmachine: Using SSH client type: native
I1114 20:34:26.743055   12113 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 40883 <nil> <nil>}
I1114 20:34:26.743063   12113 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1114 20:34:27.784358   12113 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2023-10-26 09:06:22.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2023-11-14 19:34:26.581543777 +0000
@@ -1,30 +1,35 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
-
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+Environment=HTTP_PROXY=http://webproxy.deutsche-boerse.de:8080
+Environment=HTTPS_PROXY=http://webproxy.deutsche-boerse.de:8080
+Environment=NO_PROXY=localhost,127.0.0.1,.dbgcloud.io,.deutsche-boerse.de,172.23.65.1/24,192.168.58.0/24,192.168.59.0/24,192.168.39.0/24,192.168.49.0/24,10.96.0.0/12
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=podman --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
@@ -32,16 +37,16 @@
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I1114 20:34:27.784370   12113 machine.go:91] provisioned docker machine in 10.113427341s
I1114 20:34:27.784376   12113 client.go:171] LocalClient.Create took 17.966629093s
I1114 20:34:27.784381   12113 start.go:167] duration metric: libmachine.API.Create for "minikube" took 17.966680981s
I1114 20:34:27.784385   12113 start.go:300] post-start starting for "minikube" (driver="podman")
I1114 20:34:27.784389   12113 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1114 20:34:27.784419   12113 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1114 20:34:27.784437   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:34:27.845343   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1114 20:34:27.923454   12113 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:40883 SSHKeyPath:/home/volkan/.minikube/machines/minikube/id_rsa Username:docker}
I1114 20:34:28.009483   12113 ssh_runner.go:195] Run: cat /etc/os-release
I1114 20:34:28.018212   12113 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1114 20:34:28.018225   12113 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1114 20:34:28.018229   12113 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1114 20:34:28.018232   12113 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I1114 20:34:28.018239   12113 filesync.go:126] Scanning /home/volkan/.minikube/addons for local assets ...
I1114 20:34:28.018273   12113 filesync.go:126] Scanning /home/volkan/.minikube/files for local assets ...
I1114 20:34:28.018281   12113 start.go:303] post-start completed in 233.893624ms
I1114 20:34:28.018662   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f {{.NetworkSettings.IPAddress}} minikube
I1114 20:34:28.097124   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1114 20:34:28.164856   12113 profile.go:148] Saving config to /home/volkan/.minikube/profiles/minikube/config.json ...
I1114 20:34:28.165111   12113 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1114 20:34:28.165137   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:34:28.244375   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1114 20:34:28.293605   12113 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:40883 SSHKeyPath:/home/volkan/.minikube/machines/minikube/id_rsa Username:docker}
I1114 20:34:28.394735   12113 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1114 20:34:28.403410   12113 start.go:128] duration metric: createHost completed in 18.593097458s
I1114 20:34:28.403418   12113 start.go:83] releasing machines lock for "minikube", held for 18.593143785s
I1114 20:34:28.403455   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f {{.NetworkSettings.IPAddress}} minikube
I1114 20:34:28.494149   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1114 20:34:28.559632   12113 out.go:177] * Found network options:
I1114 20:34:28.563480   12113 out.go:177]   - HTTP_PROXY=http://webproxy.deutsche-boerse.de:8080
W1114 20:34:28.566984   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.566997   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.567001   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.567004   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.567008   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.567011   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.567015   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.567018   12113 proxy.go:119] fail to check proxy env: Error ip not in block
I1114 20:34:28.570509   12113 out.go:177]   - HTTPS_PROXY=http://webproxy.deutsche-boerse.de:8080
W1114 20:34:28.573297   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.573306   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.573309   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.573312   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.573316   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.573320   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.573323   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.573326   12113 proxy.go:119] fail to check proxy env: Error ip not in block
I1114 20:34:28.576197   12113 out.go:177]   - NO_PROXY=localhost,127.0.0.1,.dbgcloud.io,.deutsche-boerse.de,172.23.65.1/24,192.168.58.0/24,192.168.59.0/24,192.168.39.0/24,192.168.49.0/24,10.96.0.0/12
W1114 20:34:28.578970   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.578979   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.578983   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.578986   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.578990   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.578997   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.579000   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.579004   12113 proxy.go:119] fail to check proxy env: Error ip not in block
I1114 20:34:28.582201   12113 out.go:177]   - HTTP_PROXY=http://webproxy.deutsche-boerse.de:8080
W1114 20:34:28.586074   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.586086   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.586089   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.586096   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.586101   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.586104   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.586107   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.586110   12113 proxy.go:119] fail to check proxy env: Error ip not in block
I1114 20:34:28.588803   12113 out.go:177]   - HTTPS_PROXY=http://webproxy.deutsche-boerse.de:8080
W1114 20:34:28.592643   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.592654   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.592776   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.592781   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.592786   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.592789   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.592792   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.592795   12113 proxy.go:119] fail to check proxy env: Error ip not in block
I1114 20:34:28.595236   12113 out.go:177]   - NO_PROXY=localhost,127.0.0.1,.dbgcloud.io,.deutsche-boerse.de,172.23.65.1/24,192.168.58.0/24,192.168.59.0/24,192.168.39.0/24,192.168.49.0/24,10.96.0.0/12
W1114 20:34:28.597769   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.597778   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.597782   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.597785   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.597789   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.597793   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.597796   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.597799   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.597832   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.597836   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.597839   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.597842   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.597845   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.597849   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.597855   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:34:28.597858   12113 proxy.go:119] fail to check proxy env: Error ip not in block
I1114 20:34:28.597884   12113 ssh_runner.go:195] Run: cat /version.json
I1114 20:34:28.597920   12113 ssh_runner.go:195] Run: curl -x http://webproxy.deutsche-boerse.de:8080 -sS -m 2 https://registry.k8s.io/
I1114 20:34:28.597974   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:34:28.597996   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:34:28.809793   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1114 20:34:28.835349   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1114 20:34:28.964470   12113 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:40883 SSHKeyPath:/home/volkan/.minikube/machines/minikube/id_rsa Username:docker}
I1114 20:34:29.032856   12113 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:40883 SSHKeyPath:/home/volkan/.minikube/machines/minikube/id_rsa Username:docker}
W1114 20:35:09.574311   12113 notify.go:59] Error getting json from minikube version url: error with http GET for endpoint https://storage.googleapis.com/minikube/releases-v2.json: Get "https://storage.googleapis.com/minikube/releases-v2.json": proxyconnect tcp: dial tcp: lookup webproxy.deutsche-boerse.de: i/o timeout
I1114 20:35:17.253861   12113 ssh_runner.go:235] Completed: cat /version.json: (48.65596541s)
I1114 20:35:17.253938   12113 ssh_runner.go:195] Run: systemctl --version
I1114 20:35:17.258056   12113 ssh_runner.go:235] Completed: curl -x http://webproxy.deutsche-boerse.de:8080 -sS -m 2 https://registry.k8s.io/: (48.660077077s)
W1114 20:35:17.258067   12113 start.go:840] [curl -x http://webproxy.deutsche-boerse.de:8080 -sS -m 2 https://registry.k8s.io/] failed: curl -x http://webproxy.deutsche-boerse.de:8080 -sS -m 2 https://registry.k8s.io/: Process exited with status 28
stdout:

stderr:
curl: (28) Resolving timed out after 2003 milliseconds
W1114 20:35:17.258095   12113 out.go:239] ! This container is having trouble accessing https://registry.k8s.io
W1114 20:35:17.260759   12113 out.go:239] * To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I1114 20:35:17.284550   12113 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1114 20:35:17.292079   12113 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1114 20:35:17.335906   12113 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1114 20:35:17.335942   12113 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1114 20:35:17.373499   12113 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I1114 20:35:17.373512   12113 start.go:472] detecting cgroup driver to use...
I1114 20:35:17.373532   12113 detect.go:196] detected "cgroupfs" cgroup driver on host os
I1114 20:35:17.373586   12113 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1114 20:35:17.410439   12113 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I1114 20:35:17.425048   12113 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1114 20:35:17.440816   12113 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I1114 20:35:17.440923   12113 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1114 20:35:17.460774   12113 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1114 20:35:17.474325   12113 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1114 20:35:17.492889   12113 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1114 20:35:17.505211   12113 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1114 20:35:17.517912   12113 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1114 20:35:17.539021   12113 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1114 20:35:17.559346   12113 crio.go:148] couldn't verify netfilter by "sudo sysctl net.bridge.bridge-nf-call-iptables" which might be okay. error: sudo sysctl net.bridge.bridge-nf-call-iptables: Process exited with status 255
stdout:

stderr:
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory
I1114 20:35:17.559384   12113 ssh_runner.go:195] Run: sudo modprobe br_netfilter
I1114 20:35:17.580321   12113 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1114 20:35:17.590599   12113 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1114 20:35:17.755414   12113 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1114 20:35:17.860320   12113 start.go:472] detecting cgroup driver to use...
I1114 20:35:17.860344   12113 detect.go:196] detected "cgroupfs" cgroup driver on host os
I1114 20:35:17.860369   12113 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1114 20:35:17.894941   12113 cruntime.go:279] skipping containerd shutdown because we are bound to it
I1114 20:35:17.894971   12113 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1114 20:35:17.923587   12113 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1114 20:35:18.020472   12113 ssh_runner.go:195] Run: which cri-dockerd
I1114 20:35:18.039388   12113 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1114 20:35:18.066397   12113 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I1114 20:35:18.113278   12113 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1114 20:35:18.437217   12113 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1114 20:35:18.559765   12113 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I1114 20:35:18.559824   12113 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1114 20:35:18.579903   12113 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1114 20:35:18.687427   12113 ssh_runner.go:195] Run: sudo systemctl restart docker
I1114 20:35:19.420317   12113 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1114 20:35:19.525883   12113 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1114 20:35:19.598748   12113 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1114 20:35:19.655971   12113 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1114 20:35:19.855679   12113 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1114 20:35:19.885899   12113 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1114 20:35:19.970599   12113 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I1114 20:35:20.172517   12113 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1114 20:35:20.172547   12113 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1114 20:35:20.179574   12113 start.go:540] Will wait 60s for crictl version
I1114 20:35:20.179597   12113 ssh_runner.go:195] Run: which crictl
I1114 20:35:20.227148   12113 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1114 20:35:20.358746   12113 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I1114 20:35:20.358787   12113 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1114 20:35:20.390365   12113 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1114 20:35:20.435160   12113 out.go:204] * Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I1114 20:35:20.442570   12113 out.go:177]   - env HTTP_PROXY=http://webproxy.deutsche-boerse.de:8080
I1114 20:35:20.450245   12113 out.go:177]   - env HTTPS_PROXY=http://webproxy.deutsche-boerse.de:8080
I1114 20:35:20.454883   12113 out.go:177]   - env NO_PROXY=localhost,127.0.0.1,.dbgcloud.io,.deutsche-boerse.de,172.23.65.1/24,192.168.58.0/24,192.168.59.0/24,192.168.39.0/24,192.168.49.0/24,10.96.0.0/12
I1114 20:35:20.457343   12113 cli_runner.go:164] Run: sudo -n podman container inspect --format {{.NetworkSettings.Gateway}} minikube
I1114 20:35:20.509618   12113 cli_runner.go:164] Run: sudo -n podman container inspect --format "
{{ if index .NetworkSettings.Networks "minikube"}} 
	{{(index .NetworkSettings.Networks "minikube").Gateway}}
{{ end }}
" minikube
I1114 20:35:20.575897   12113 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I1114 20:35:20.583020   12113 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1114 20:35:20.601091   12113 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I1114 20:35:20.601122   12113 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1114 20:35:20.635238   12113 docker.go:671] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1114 20:35:20.635258   12113 docker.go:601] Images already preloaded, skipping extraction
I1114 20:35:20.635318   12113 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1114 20:35:20.667352   12113 docker.go:671] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1114 20:35:20.667362   12113 cache_images.go:84] Images are preloaded, skipping loading
I1114 20:35:20.667397   12113 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1114 20:35:20.771601   12113 cni.go:84] Creating CNI manager for ""
I1114 20:35:20.771613   12113 cni.go:136] 1 nodes found, recommending kindnet
I1114 20:35:20.771629   12113 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1114 20:35:20.771648   12113 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1114 20:35:20.771717   12113 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1114 20:35:20.771747   12113 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1114 20:35:20.771778   12113 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I1114 20:35:20.785281   12113 binaries.go:44] Found k8s binaries, skipping transfer
I1114 20:35:20.785311   12113 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1114 20:35:20.799306   12113 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I1114 20:35:20.824841   12113 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1114 20:35:20.849362   12113 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I1114 20:35:20.875349   12113 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1114 20:35:20.879630   12113 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1114 20:35:20.894586   12113 certs.go:56] Setting up /home/volkan/.minikube/profiles/minikube for IP: 192.168.49.2
I1114 20:35:20.894600   12113 certs.go:190] acquiring lock for shared ca certs: {Name:mkcf3862dec8fde9801070582903515da6d63ece Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1114 20:35:20.894669   12113 certs.go:199] skipping minikubeCA CA generation: /home/volkan/.minikube/ca.key
I1114 20:35:20.894682   12113 certs.go:199] skipping proxyClientCA CA generation: /home/volkan/.minikube/proxy-client-ca.key
I1114 20:35:20.894703   12113 certs.go:319] generating minikube-user signed cert: /home/volkan/.minikube/profiles/minikube/client.key
I1114 20:35:20.894707   12113 crypto.go:68] Generating cert /home/volkan/.minikube/profiles/minikube/client.crt with IP's: []
I1114 20:35:20.994764   12113 crypto.go:156] Writing cert to /home/volkan/.minikube/profiles/minikube/client.crt ...
I1114 20:35:20.994774   12113 lock.go:35] WriteFile acquiring /home/volkan/.minikube/profiles/minikube/client.crt: {Name:mk6df4b7fec36633b185348a956c4f7b81c5a4b1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1114 20:35:21.007455   12113 crypto.go:164] Writing key to /home/volkan/.minikube/profiles/minikube/client.key ...
I1114 20:35:21.007467   12113 lock.go:35] WriteFile acquiring /home/volkan/.minikube/profiles/minikube/client.key: {Name:mkf223494962eccf1ac1397e0eb9e72d5d79c9d4 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1114 20:35:21.013174   12113 certs.go:319] generating minikube signed cert: /home/volkan/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I1114 20:35:21.013184   12113 crypto.go:68] Generating cert /home/volkan/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 with IP's: [192.168.49.2 10.96.0.1 127.0.0.1 10.0.0.1]
I1114 20:35:21.097055   12113 crypto.go:156] Writing cert to /home/volkan/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 ...
I1114 20:35:21.097067   12113 lock.go:35] WriteFile acquiring /home/volkan/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2: {Name:mk9981c740b7e4e236a6a06ad886b6a4bbab5431 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1114 20:35:21.097241   12113 crypto.go:164] Writing key to /home/volkan/.minikube/profiles/minikube/apiserver.key.dd3b5fb2 ...
I1114 20:35:21.097245   12113 lock.go:35] WriteFile acquiring /home/volkan/.minikube/profiles/minikube/apiserver.key.dd3b5fb2: {Name:mkcff168bae2d38f81a8ce1b9919bcdbfea29671 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1114 20:35:21.097281   12113 certs.go:337] copying /home/volkan/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 -> /home/volkan/.minikube/profiles/minikube/apiserver.crt
I1114 20:35:21.097323   12113 certs.go:341] copying /home/volkan/.minikube/profiles/minikube/apiserver.key.dd3b5fb2 -> /home/volkan/.minikube/profiles/minikube/apiserver.key
I1114 20:35:21.097510   12113 certs.go:319] generating aggregator signed cert: /home/volkan/.minikube/profiles/minikube/proxy-client.key
I1114 20:35:21.097522   12113 crypto.go:68] Generating cert /home/volkan/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I1114 20:35:21.135027   12113 crypto.go:156] Writing cert to /home/volkan/.minikube/profiles/minikube/proxy-client.crt ...
I1114 20:35:21.135037   12113 lock.go:35] WriteFile acquiring /home/volkan/.minikube/profiles/minikube/proxy-client.crt: {Name:mk0083412ef2b37973ac3d184a7f09fa1b815d63 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1114 20:35:21.135189   12113 crypto.go:164] Writing key to /home/volkan/.minikube/profiles/minikube/proxy-client.key ...
I1114 20:35:21.135193   12113 lock.go:35] WriteFile acquiring /home/volkan/.minikube/profiles/minikube/proxy-client.key: {Name:mk839a09428bc4a68797d1ee8b8e5e94914c5fe7 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1114 20:35:21.135354   12113 certs.go:437] found cert: /home/volkan/.minikube/certs/home/volkan/.minikube/certs/ca-key.pem (1679 bytes)
I1114 20:35:21.135370   12113 certs.go:437] found cert: /home/volkan/.minikube/certs/home/volkan/.minikube/certs/ca.pem (1078 bytes)
I1114 20:35:21.135380   12113 certs.go:437] found cert: /home/volkan/.minikube/certs/home/volkan/.minikube/certs/cert.pem (1119 bytes)
I1114 20:35:21.135389   12113 certs.go:437] found cert: /home/volkan/.minikube/certs/home/volkan/.minikube/certs/key.pem (1675 bytes)
I1114 20:35:21.135690   12113 ssh_runner.go:362] scp /home/volkan/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I1114 20:35:21.172517   12113 ssh_runner.go:362] scp /home/volkan/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1114 20:35:21.205390   12113 ssh_runner.go:362] scp /home/volkan/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1114 20:35:21.236979   12113 ssh_runner.go:362] scp /home/volkan/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1114 20:35:21.275369   12113 ssh_runner.go:362] scp /home/volkan/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1114 20:35:21.313034   12113 ssh_runner.go:362] scp /home/volkan/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1114 20:35:21.340918   12113 ssh_runner.go:362] scp /home/volkan/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1114 20:35:21.366315   12113 ssh_runner.go:362] scp /home/volkan/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1114 20:35:21.395055   12113 ssh_runner.go:362] scp /home/volkan/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1114 20:35:21.421164   12113 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1114 20:35:21.441369   12113 ssh_runner.go:195] Run: openssl version
I1114 20:35:21.447140   12113 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1114 20:35:21.459370   12113 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1114 20:35:21.464251   12113 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Nov 14 19:26 /usr/share/ca-certificates/minikubeCA.pem
I1114 20:35:21.464277   12113 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1114 20:35:21.472560   12113 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1114 20:35:21.485871   12113 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I1114 20:35:21.490679   12113 certs.go:353] certs directory doesn't exist, likely first start: ls /var/lib/minikube/certs/etcd: Process exited with status 2
stdout:

stderr:
ls: cannot access '/var/lib/minikube/certs/etcd': No such file or directory
I1114 20:35:21.490700   12113 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:podman HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/volkan:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I1114 20:35:21.490753   12113 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1114 20:35:21.520003   12113 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1114 20:35:21.533749   12113 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1114 20:35:21.543707   12113 kubeadm.go:226] ignoring SystemVerification for kubeadm because of podman driver
I1114 20:35:21.543731   12113 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1114 20:35:21.553647   12113 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I1114 20:35:21.553666   12113 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I1114 20:35:21.622864   12113 kubeadm.go:322] [init] Using Kubernetes version: v1.28.3
I1114 20:35:21.622907   12113 kubeadm.go:322] [preflight] Running pre-flight checks
I1114 20:35:21.824427   12113 kubeadm.go:322] [preflight] Pulling images required for setting up a Kubernetes cluster
I1114 20:35:21.824482   12113 kubeadm.go:322] [preflight] This might take a minute or two, depending on the speed of your internet connection
I1114 20:35:21.824534   12113 kubeadm.go:322] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I1114 20:35:22.377310   12113 kubeadm.go:322] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I1114 20:35:22.385334   12113 out.go:204]   - Generating certificates and keys ...
I1114 20:35:22.385476   12113 kubeadm.go:322] [certs] Using existing ca certificate authority
I1114 20:35:22.385510   12113 kubeadm.go:322] [certs] Using existing apiserver certificate and key on disk
I1114 20:35:22.440700   12113 kubeadm.go:322] [certs] Generating "apiserver-kubelet-client" certificate and key
I1114 20:35:22.539859   12113 kubeadm.go:322] [certs] Generating "front-proxy-ca" certificate and key
I1114 20:35:22.607449   12113 kubeadm.go:322] [certs] Generating "front-proxy-client" certificate and key
I1114 20:35:22.824182   12113 kubeadm.go:322] [certs] Generating "etcd/ca" certificate and key
I1114 20:35:22.956395   12113 kubeadm.go:322] [certs] Generating "etcd/server" certificate and key
I1114 20:35:22.956692   12113 kubeadm.go:322] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1114 20:35:23.230666   12113 kubeadm.go:322] [certs] Generating "etcd/peer" certificate and key
I1114 20:35:23.230727   12113 kubeadm.go:322] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1114 20:35:23.311008   12113 kubeadm.go:322] [certs] Generating "etcd/healthcheck-client" certificate and key
I1114 20:35:23.496948   12113 kubeadm.go:322] [certs] Generating "apiserver-etcd-client" certificate and key
I1114 20:35:23.987448   12113 kubeadm.go:322] [certs] Generating "sa" key and public key
I1114 20:35:23.987663   12113 kubeadm.go:322] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I1114 20:35:24.074621   12113 kubeadm.go:322] [kubeconfig] Writing "admin.conf" kubeconfig file
I1114 20:35:24.199306   12113 kubeadm.go:322] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I1114 20:35:24.279640   12113 kubeadm.go:322] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I1114 20:35:24.465738   12113 kubeadm.go:322] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I1114 20:35:24.466539   12113 kubeadm.go:322] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I1114 20:35:24.469793   12113 kubeadm.go:322] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I1114 20:35:24.479387   12113 out.go:204]   - Booting up control plane ...
I1114 20:35:24.479867   12113 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-apiserver"
I1114 20:35:24.479982   12113 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I1114 20:35:24.480018   12113 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-scheduler"
I1114 20:35:24.493872   12113 kubeadm.go:322] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I1114 20:35:24.494295   12113 kubeadm.go:322] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I1114 20:35:24.494318   12113 kubeadm.go:322] [kubelet-start] Starting the kubelet
I1114 20:35:24.581764   12113 kubeadm.go:322] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
I1114 20:35:42.581936   12113 kubeadm.go:322] [apiclient] All control plane components are healthy after 18.002182 seconds
I1114 20:35:42.582045   12113 kubeadm.go:322] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I1114 20:35:42.599212   12113 kubeadm.go:322] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I1114 20:35:43.153976   12113 kubeadm.go:322] [upload-certs] Skipping phase. Please see --upload-certs
I1114 20:35:43.154067   12113 kubeadm.go:322] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I1114 20:35:43.693583   12113 kubeadm.go:322] [bootstrap-token] Using token: 2w3fxe.w1n5jrprawsbdpmr
I1114 20:35:43.702883   12113 out.go:204]   - Configuring RBAC rules ...
I1114 20:35:43.706719   12113 kubeadm.go:322] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I1114 20:35:43.738394   12113 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I1114 20:35:43.745603   12113 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I1114 20:35:43.749741   12113 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I1114 20:35:43.756338   12113 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I1114 20:35:43.760403   12113 kubeadm.go:322] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I1114 20:35:43.786454   12113 kubeadm.go:322] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I1114 20:35:45.535712   12113 kubeadm.go:322] [addons] Applied essential addon: CoreDNS
I1114 20:35:46.347298   12113 kubeadm.go:322] [addons] Applied essential addon: kube-proxy
I1114 20:35:46.347307   12113 kubeadm.go:322] 
I1114 20:35:46.347342   12113 kubeadm.go:322] Your Kubernetes control-plane has initialized successfully!
I1114 20:35:46.347344   12113 kubeadm.go:322] 
I1114 20:35:46.347381   12113 kubeadm.go:322] To start using your cluster, you need to run the following as a regular user:
I1114 20:35:46.347383   12113 kubeadm.go:322] 
I1114 20:35:46.347399   12113 kubeadm.go:322]   mkdir -p $HOME/.kube
I1114 20:35:46.347427   12113 kubeadm.go:322]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I1114 20:35:46.347451   12113 kubeadm.go:322]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I1114 20:35:46.347452   12113 kubeadm.go:322] 
I1114 20:35:46.347478   12113 kubeadm.go:322] Alternatively, if you are the root user, you can run:
I1114 20:35:46.347479   12113 kubeadm.go:322] 
I1114 20:35:46.347501   12113 kubeadm.go:322]   export KUBECONFIG=/etc/kubernetes/admin.conf
I1114 20:35:46.347503   12113 kubeadm.go:322] 
I1114 20:35:46.347530   12113 kubeadm.go:322] You should now deploy a pod network to the cluster.
I1114 20:35:46.348952   12113 kubeadm.go:322] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I1114 20:35:46.348989   12113 kubeadm.go:322]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I1114 20:35:46.348990   12113 kubeadm.go:322] 
I1114 20:35:46.350084   12113 kubeadm.go:322] You can now join any number of control-plane nodes by copying certificate authorities
I1114 20:35:46.350124   12113 kubeadm.go:322] and service account keys on each node and then running the following as root:
I1114 20:35:46.350126   12113 kubeadm.go:322] 
I1114 20:35:46.350167   12113 kubeadm.go:322]   kubeadm join control-plane.minikube.internal:8443 --token 2w3fxe.w1n5jrprawsbdpmr \
I1114 20:35:46.350216   12113 kubeadm.go:322] 	--discovery-token-ca-cert-hash sha256:a2252d43c14054fc6733fec7a145908c7919f85fb780918953be2b01b36b46af \
I1114 20:35:46.350226   12113 kubeadm.go:322] 	--control-plane 
I1114 20:35:46.350228   12113 kubeadm.go:322] 
I1114 20:35:46.350275   12113 kubeadm.go:322] Then you can join any number of worker nodes by running the following on each as root:
I1114 20:35:46.350277   12113 kubeadm.go:322] 
I1114 20:35:46.350319   12113 kubeadm.go:322] kubeadm join control-plane.minikube.internal:8443 --token 2w3fxe.w1n5jrprawsbdpmr \
I1114 20:35:46.350368   12113 kubeadm.go:322] 	--discovery-token-ca-cert-hash sha256:a2252d43c14054fc6733fec7a145908c7919f85fb780918953be2b01b36b46af 
I1114 20:35:46.453292   12113 kubeadm.go:322] 	[WARNING Swap]: swap is enabled; production deployments should disable swap unless testing the NodeSwap feature gate of the kubelet
I1114 20:35:46.453348   12113 kubeadm.go:322] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I1114 20:35:46.453408   12113 kubeadm.go:322] 	[WARNING FileContent--proc-sys-net-bridge-bridge-nf-call-iptables]: /proc/sys/net/bridge/bridge-nf-call-iptables does not exist
I1114 20:35:46.453414   12113 cni.go:84] Creating CNI manager for ""
I1114 20:35:46.453417   12113 cni.go:136] 1 nodes found, recommending kindnet
I1114 20:35:46.477064   12113 out.go:177] * Configuring CNI (Container Networking Interface) ...
I1114 20:35:46.479823   12113 ssh_runner.go:195] Run: stat /opt/cni/bin/portmap
I1114 20:35:46.624665   12113 cni.go:182] applying CNI manifest using /var/lib/minikube/binaries/v1.28.3/kubectl ...
I1114 20:35:46.624673   12113 ssh_runner.go:362] scp memory --> /var/tmp/minikube/cni.yaml (2438 bytes)
I1114 20:35:46.925534   12113 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.28.3/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml
I1114 20:35:52.683536   12113 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.28.3/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml: (5.757982898s)
I1114 20:35:52.683589   12113 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1114 20:35:52.683666   12113 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.28.3/kubectl label nodes minikube.k8s.io/version=v1.32.0 minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d minikube.k8s.io/name=minikube minikube.k8s.io/updated_at=2023_11_14T20_35_52_0700 minikube.k8s.io/primary=true --all --overwrite --kubeconfig=/var/lib/minikube/kubeconfig
I1114 20:35:52.683663   12113 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.28.3/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I1114 20:35:53.751101   12113 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.28.3/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig: (1.067422714s)
I1114 20:35:53.751113   12113 kubeadm.go:1081] duration metric: took 1.067479241s to wait for elevateKubeSystemPrivileges.
I1114 20:35:53.751137   12113 ssh_runner.go:235] Completed: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj": (1.067544112s)
I1114 20:35:53.751139   12113 ops.go:34] apiserver oom_adj: -16
I1114 20:35:53.867898   12113 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.28.3/kubectl label nodes minikube.k8s.io/version=v1.32.0 minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d minikube.k8s.io/name=minikube minikube.k8s.io/updated_at=2023_11_14T20_35_52_0700 minikube.k8s.io/primary=true --all --overwrite --kubeconfig=/var/lib/minikube/kubeconfig: (1.184157648s)
I1114 20:35:53.867914   12113 kubeadm.go:406] StartCluster complete in 32.377215353s
I1114 20:35:53.867926   12113 settings.go:142] acquiring lock: {Name:mk4b42a223058e68d313cf684feae33eedc91dee Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1114 20:35:53.867964   12113 settings.go:150] Updating kubeconfig:  /home/volkan/.kube/config
I1114 20:35:53.868554   12113 lock.go:35] WriteFile acquiring /home/volkan/.kube/config: {Name:mk8ecd43e8a2fd43e2e168fa4ecb2b3ff2977812 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1114 20:35:53.869006   12113 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1114 20:35:53.869186   12113 config.go:182] Loaded profile config "minikube": Driver=podman, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1114 20:35:53.869826   12113 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I1114 20:35:53.869856   12113 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1114 20:35:53.869864   12113 addons.go:231] Setting addon storage-provisioner=true in "minikube"
I1114 20:35:53.869887   12113 host.go:66] Checking if "minikube" exists ...
I1114 20:35:53.870088   12113 cli_runner.go:164] Run: sudo -n podman container inspect minikube --format={{.State.Status}}
I1114 20:35:53.870439   12113 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1114 20:35:53.870447   12113 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1114 20:35:53.870640   12113 cli_runner.go:164] Run: sudo -n podman container inspect minikube --format={{.State.Status}}
I1114 20:35:54.094116   12113 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I1114 20:35:54.094135   12113 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I1114 20:35:54.174603   12113 out.go:177] * Verifying Kubernetes components...
I1114 20:35:54.186602   12113 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1114 20:35:54.304929   12113 out.go:177]   - Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1114 20:35:54.308646   12113 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1114 20:35:54.308656   12113 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1114 20:35:54.308693   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:35:54.328034   12113 addons.go:231] Setting addon default-storageclass=true in "minikube"
I1114 20:35:54.328056   12113 host.go:66] Checking if "minikube" exists ...
I1114 20:35:54.328263   12113 cli_runner.go:164] Run: sudo -n podman container inspect minikube --format={{.State.Status}}
I1114 20:35:54.510238   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1114 20:35:54.593596   12113 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.49.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I1114 20:35:54.596261   12113 api_server.go:52] waiting for apiserver process to appear ...
I1114 20:35:54.596289   12113 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1114 20:35:54.637683   12113 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I1114 20:35:54.637694   12113 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1114 20:35:54.637731   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:35:54.915837   12113 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:40883 SSHKeyPath:/home/volkan/.minikube/machines/minikube/id_rsa Username:docker}
I1114 20:35:54.993875   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1114 20:35:55.124494   12113 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:40883 SSHKeyPath:/home/volkan/.minikube/machines/minikube/id_rsa Username:docker}
I1114 20:35:55.873777   12113 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1114 20:35:56.438677   12113 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1114 20:35:57.936891   12113 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.49.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -": (3.343276911s)
I1114 20:35:57.936902   12113 start.go:926] {"host.minikube.internal": 192.168.49.1} host record injected into CoreDNS's ConfigMap
I1114 20:35:57.936920   12113 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (3.340626592s)
I1114 20:35:57.936928   12113 api_server.go:72] duration metric: took 3.842781086s to wait for apiserver process to appear ...
I1114 20:35:57.936930   12113 api_server.go:88] waiting for apiserver healthz status ...
I1114 20:35:57.936937   12113 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1114 20:35:57.966613   12113 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I1114 20:35:58.027880   12113 api_server.go:141] control plane version: v1.28.3
I1114 20:35:58.027896   12113 api_server.go:131] duration metric: took 90.963093ms to wait for apiserver health ...
I1114 20:35:58.027901   12113 system_pods.go:43] waiting for kube-system pods to appear ...
I1114 20:35:58.165890   12113 system_pods.go:59] 4 kube-system pods found
I1114 20:35:58.165909   12113 system_pods.go:61] "etcd-minikube" [c6ab4208-b74d-463a-8d4f-078435bc3d4b] Running
I1114 20:35:58.165911   12113 system_pods.go:61] "kube-apiserver-minikube" [cb87c130-be61-4ded-a1b2-2b825a30f45d] Running
I1114 20:35:58.165916   12113 system_pods.go:61] "kube-controller-manager-minikube" [cbeb6e00-36da-4ebb-88f5-457ca51ad1d0] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1114 20:35:58.165919   12113 system_pods.go:61] "kube-scheduler-minikube" [6dd83337-66df-47a6-8202-37c9ca921e05] Running
I1114 20:35:58.165923   12113 system_pods.go:74] duration metric: took 138.018622ms to wait for pod list to return data ...
I1114 20:35:58.165930   12113 kubeadm.go:581] duration metric: took 4.071783863s to wait for : map[apiserver:true system_pods:true] ...
I1114 20:35:58.165938   12113 node_conditions.go:102] verifying NodePressure condition ...
I1114 20:35:58.336775   12113 node_conditions.go:122] node storage ephemeral capacity is 40302856Ki
I1114 20:35:58.336790   12113 node_conditions.go:123] node cpu capacity is 6
I1114 20:35:58.336797   12113 node_conditions.go:105] duration metric: took 170.857408ms to run NodePressure ...
I1114 20:35:58.336805   12113 start.go:228] waiting for startup goroutines ...
I1114 20:36:03.738079   12113 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (7.299363295s)
I1114 20:36:03.742151   12113 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (7.868359786s)
I1114 20:36:04.188510   12113 out.go:177] * Enabled addons: storage-provisioner, default-storageclass
I1114 20:36:04.191700   12113 addons.go:502] enable addons completed in 10.321872928s: enabled=[storage-provisioner default-storageclass]
I1114 20:36:04.191720   12113 start.go:233] waiting for cluster config update ...
I1114 20:36:04.191727   12113 start.go:242] writing updated cluster config ...
I1114 20:36:04.194511   12113 out.go:177] 
I1114 20:36:04.197671   12113 config.go:182] Loaded profile config "minikube": Driver=podman, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1114 20:36:04.197736   12113 profile.go:148] Saving config to /home/volkan/.minikube/profiles/minikube/config.json ...
I1114 20:36:04.202246   12113 out.go:177] * Starting worker node minikube-m02 in cluster minikube
I1114 20:36:04.204829   12113 cache.go:121] Beginning downloading kic base image for podman with docker
I1114 20:36:04.207763   12113 out.go:177] * Pulling base image ...
I1114 20:36:04.210158   12113 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I1114 20:36:04.210170   12113 cache.go:56] Caching tarball of preloaded images
I1114 20:36:04.210237   12113 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 to local cache
I1114 20:36:04.210220   12113 preload.go:174] Found /home/volkan/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1114 20:36:04.210225   12113 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I1114 20:36:04.210276   12113 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local cache directory
I1114 20:36:04.210282   12113 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local cache directory, skipping pull
I1114 20:36:04.210284   12113 image.go:105] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in cache, skipping pull
I1114 20:36:04.210288   12113 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 as a tarball
I1114 20:36:04.210267   12113 profile.go:148] Saving config to /home/volkan/.minikube/profiles/minikube/config.json ...
E1114 20:36:04.210498   12113 cache.go:189] Error downloading kic artifacts:  not yet implemented, see issue #8426
I1114 20:36:04.210532   12113 cache.go:194] Successfully downloaded all kic artifacts
I1114 20:36:04.210543   12113 start.go:365] acquiring machines lock for minikube-m02: {Name:mk8ee615b1dd9b052ca812dc5580b95c4f24c488 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1114 20:36:04.210576   12113 start.go:369] acquired machines lock for "minikube-m02" in 25.092µs
I1114 20:36:04.210585   12113 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:podman HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP: Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/volkan:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:} &{Name:m02 IP: Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true}
I1114 20:36:04.210613   12113 start.go:125] createHost starting for "m02" (driver="podman")
I1114 20:36:04.214269   12113 out.go:204] * Creating podman container (CPUs=2, Memory=2200MB) ...
I1114 20:36:04.214359   12113 start.go:159] libmachine.API.Create for "minikube" (driver="podman")
I1114 20:36:04.214378   12113 client.go:168] LocalClient.Create starting
I1114 20:36:04.214408   12113 main.go:141] libmachine: Reading certificate data from /home/volkan/.minikube/certs/ca.pem
I1114 20:36:04.214425   12113 main.go:141] libmachine: Decoding PEM data...
I1114 20:36:04.214433   12113 main.go:141] libmachine: Parsing certificate...
I1114 20:36:04.214459   12113 main.go:141] libmachine: Reading certificate data from /home/volkan/.minikube/certs/cert.pem
I1114 20:36:04.214479   12113 main.go:141] libmachine: Decoding PEM data...
I1114 20:36:04.214485   12113 main.go:141] libmachine: Parsing certificate...
I1114 20:36:04.214591   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:36:04.306412   12113 cli_runner.go:164] Run: sudo -n podman network inspect minikube --format "{{range .}}{{if eq .Driver "bridge"}}{{(index .Subnets 0).Subnet}},{{(index .Subnets 0).Gateway}}{{end}}{{end}}"
I1114 20:36:04.511269   12113 network_create.go:77] Found existing network {name:minikube subnet:0xc00066f920 gateway:[0 0 0 0 0 0 0 0 0 0 255 255 192 168 49 1] mtu:0}
I1114 20:36:04.511300   12113 kic.go:121] calculated static IP "192.168.49.3" for the "minikube-m02" container
I1114 20:36:04.511343   12113 cli_runner.go:164] Run: sudo -n podman ps -a --format {{.Names}}
I1114 20:36:04.615399   12113 cli_runner.go:164] Run: sudo -n podman volume create minikube-m02 --label name.minikube.sigs.k8s.io=minikube-m02 --label created_by.minikube.sigs.k8s.io=true
I1114 20:36:04.898945   12113 oci.go:103] Successfully created a podman volume minikube-m02
I1114 20:36:04.898993   12113 cli_runner.go:164] Run: sudo -n podman run --rm --name minikube-m02-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m02 --entrypoint /usr/bin/test -v minikube-m02:/var gcr.io/k8s-minikube/kicbase:v0.0.42 -d /var/lib
I1114 20:36:08.532884   12113 cli_runner.go:217] Completed: sudo -n podman run --rm --name minikube-m02-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m02 --entrypoint /usr/bin/test -v minikube-m02:/var gcr.io/k8s-minikube/kicbase:v0.0.42 -d /var/lib: (3.633866836s)
I1114 20:36:08.532897   12113 oci.go:107] Successfully prepared a podman volume minikube-m02
I1114 20:36:08.532908   12113 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I1114 20:36:08.532921   12113 kic.go:194] Starting extracting preloaded images to volume ...
I1114 20:36:08.538899   12113 cli_runner.go:164] Run: sudo -n podman run --rm --entrypoint /usr/bin/tar --security-opt label=disable -v /home/volkan/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube-m02:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.42 -I lz4 -xf /preloaded.tar -C /extractDir
I1114 20:36:19.907759   12113 cli_runner.go:217] Completed: sudo -n podman run --rm --entrypoint /usr/bin/tar --security-opt label=disable -v /home/volkan/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube-m02:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.42 -I lz4 -xf /preloaded.tar -C /extractDir: (11.368822674s)
I1114 20:36:19.907776   12113 kic.go:203] duration metric: took 11.374853 seconds to extract preloaded images to volume
W1114 20:36:19.907868   12113 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
I1114 20:36:19.907929   12113 cli_runner.go:164] Run: sudo -n podman info --format "'{{json .SecurityOptions}}'"
W1114 20:36:20.189691   12113 cli_runner.go:211] sudo -n podman info --format "'{{json .SecurityOptions}}'" returned with exit code 125
I1114 20:36:20.189777   12113 cli_runner.go:164] Run: sudo -n podman run --cgroup-manager cgroupfs -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube-m02 --name minikube-m02 --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m02 --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube-m02 --network minikube --ip 192.168.49.3 --volume minikube-m02:/var:exec --memory=2200mb --cpus=2 -e container=podman --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.42
I1114 20:36:20.975717   12113 cli_runner.go:164] Run: sudo -n podman container inspect minikube-m02 --format={{.State.Running}}
I1114 20:36:21.180528   12113 cli_runner.go:164] Run: sudo -n podman container inspect minikube-m02 --format={{.State.Status}}
I1114 20:36:21.349452   12113 cli_runner.go:164] Run: sudo -n podman exec minikube-m02 stat /var/lib/dpkg/alternatives/iptables
I1114 20:36:21.963318   12113 oci.go:144] the created container "minikube-m02" has a running status.
I1114 20:36:21.963336   12113 kic.go:225] Creating ssh key for kic: /home/volkan/.minikube/machines/minikube-m02/id_rsa...
I1114 20:36:22.339252   12113 kic_runner.go:191] podman (temp): /home/volkan/.minikube/machines/minikube-m02/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I1114 20:36:22.387676   12113 kic_runner.go:261] Run: /usr/bin/sudo -n podman cp /tmp/tmpf-memory-asset3411211655 minikube-m02:/home/docker/.ssh/authorized_keys
I1114 20:36:22.830258   12113 cli_runner.go:164] Run: sudo -n podman container inspect minikube-m02 --format={{.State.Status}}
I1114 20:36:22.897334   12113 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I1114 20:36:22.897346   12113 kic_runner.go:114] Args: [sudo -n podman exec --privileged minikube-m02 chown docker:docker /home/docker/.ssh/authorized_keys]
I1114 20:36:23.349543   12113 cli_runner.go:164] Run: sudo -n podman container inspect minikube-m02 --format={{.State.Status}}
I1114 20:36:23.793134   12113 machine.go:88] provisioning docker machine ...
I1114 20:36:23.793493   12113 ubuntu.go:169] provisioning hostname "minikube-m02"
I1114 20:36:23.793530   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:36:23.893860   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1114 20:36:24.031207   12113 main.go:141] libmachine: Using SSH client type: native
I1114 20:36:24.031414   12113 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 37223 <nil> <nil>}
I1114 20:36:24.031419   12113 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube-m02 && echo "minikube-m02" | sudo tee /etc/hostname
I1114 20:36:24.031585   12113 main.go:141] libmachine: Error dialing TCP: dial tcp 127.0.0.1:37223: connect: connection refused
I1114 20:36:27.033640   12113 main.go:141] libmachine: Error dialing TCP: dial tcp 127.0.0.1:37223: connect: connection refused
I1114 20:36:30.777116   12113 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube-m02

I1114 20:36:30.777163   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:36:31.048036   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1114 20:36:31.586317   12113 main.go:141] libmachine: Using SSH client type: native
I1114 20:36:31.586506   12113 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 37223 <nil> <nil>}
I1114 20:36:31.586512   12113 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube-m02' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube-m02/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube-m02' | sudo tee -a /etc/hosts; 
			fi
		fi
I1114 20:36:32.706849   12113 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1114 20:36:32.706861   12113 ubuntu.go:175] set auth options {CertDir:/home/volkan/.minikube CaCertPath:/home/volkan/.minikube/certs/ca.pem CaPrivateKeyPath:/home/volkan/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/volkan/.minikube/machines/server.pem ServerKeyPath:/home/volkan/.minikube/machines/server-key.pem ClientKeyPath:/home/volkan/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/volkan/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/volkan/.minikube}
I1114 20:36:32.706870   12113 ubuntu.go:177] setting up certificates
I1114 20:36:32.706875   12113 provision.go:83] configureAuth start
I1114 20:36:32.706909   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f {{.NetworkSettings.IPAddress}} minikube-m02
I1114 20:36:33.140646   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m02
I1114 20:36:33.655381   12113 provision.go:138] copyHostCerts
I1114 20:36:33.655415   12113 exec_runner.go:144] found /home/volkan/.minikube/key.pem, removing ...
I1114 20:36:33.655419   12113 exec_runner.go:203] rm: /home/volkan/.minikube/key.pem
I1114 20:36:33.657429   12113 exec_runner.go:151] cp: /home/volkan/.minikube/certs/key.pem --> /home/volkan/.minikube/key.pem (1675 bytes)
I1114 20:36:33.657482   12113 exec_runner.go:144] found /home/volkan/.minikube/ca.pem, removing ...
I1114 20:36:33.657485   12113 exec_runner.go:203] rm: /home/volkan/.minikube/ca.pem
I1114 20:36:33.657498   12113 exec_runner.go:151] cp: /home/volkan/.minikube/certs/ca.pem --> /home/volkan/.minikube/ca.pem (1078 bytes)
I1114 20:36:33.657558   12113 exec_runner.go:144] found /home/volkan/.minikube/cert.pem, removing ...
I1114 20:36:33.657560   12113 exec_runner.go:203] rm: /home/volkan/.minikube/cert.pem
I1114 20:36:33.657573   12113 exec_runner.go:151] cp: /home/volkan/.minikube/certs/cert.pem --> /home/volkan/.minikube/cert.pem (1119 bytes)
I1114 20:36:33.657596   12113 provision.go:112] generating server cert: /home/volkan/.minikube/machines/server.pem ca-key=/home/volkan/.minikube/certs/ca.pem private-key=/home/volkan/.minikube/certs/ca-key.pem org=volkan.minikube-m02 san=[192.168.49.3 127.0.0.1 localhost 127.0.0.1 minikube minikube-m02]
I1114 20:36:34.044289   12113 provision.go:172] copyRemoteCerts
I1114 20:36:34.044322   12113 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1114 20:36:34.044348   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:36:34.407615   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1114 20:36:34.915034   12113 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:37223 SSHKeyPath:/home/volkan/.minikube/machines/minikube-m02/id_rsa Username:docker}
I1114 20:36:35.114451   12113 ssh_runner.go:235] Completed: sudo mkdir -p /etc/docker /etc/docker /etc/docker: (1.070115165s)
I1114 20:36:35.114482   12113 ssh_runner.go:362] scp /home/volkan/.minikube/machines/server.pem --> /etc/docker/server.pem (1212 bytes)
I1114 20:36:35.206883   12113 ssh_runner.go:362] scp /home/volkan/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1114 20:36:35.243235   12113 ssh_runner.go:362] scp /home/volkan/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I1114 20:36:35.298358   12113 provision.go:86] duration metric: configureAuth took 2.591475887s
I1114 20:36:35.298370   12113 ubuntu.go:193] setting minikube options for container-runtime
I1114 20:36:35.298466   12113 config.go:182] Loaded profile config "minikube": Driver=podman, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1114 20:36:35.298495   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:36:35.416956   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1114 20:36:35.544096   12113 main.go:141] libmachine: Using SSH client type: native
I1114 20:36:35.544345   12113 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 37223 <nil> <nil>}
I1114 20:36:35.544351   12113 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1114 20:36:35.729117   12113 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1114 20:36:35.729126   12113 ubuntu.go:71] root file system type: overlay
I1114 20:36:35.729185   12113 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1114 20:36:35.729218   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:36:36.012160   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1114 20:36:36.139919   12113 main.go:141] libmachine: Using SSH client type: native
I1114 20:36:36.140112   12113 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 37223 <nil> <nil>}
I1114 20:36:36.140142   12113 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment="HTTP_PROXY=http://webproxy.deutsche-boerse.de:8080"
Environment="HTTPS_PROXY=http://webproxy.deutsche-boerse.de:8080"
Environment="NO_PROXY=localhost,127.0.0.1,.dbgcloud.io,.deutsche-boerse.de,172.23.65.1/24,192.168.58.0/24,192.168.59.0/24,192.168.39.0/24,192.168.49.0/24,10.96.0.0/12"


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=podman --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1114 20:36:36.595399   12113 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment=HTTP_PROXY=http://webproxy.deutsche-boerse.de:8080
Environment=HTTPS_PROXY=http://webproxy.deutsche-boerse.de:8080
Environment=NO_PROXY=localhost,127.0.0.1,.dbgcloud.io,.deutsche-boerse.de,172.23.65.1/24,192.168.58.0/24,192.168.59.0/24,192.168.39.0/24,192.168.49.0/24,10.96.0.0/12


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=podman --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1114 20:36:36.595462   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:36:36.721063   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1114 20:36:36.908936   12113 main.go:141] libmachine: Using SSH client type: native
I1114 20:36:36.909192   12113 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 37223 <nil> <nil>}
I1114 20:36:36.909200   12113 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1114 20:36:38.058665   12113 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2023-10-26 09:06:22.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2023-11-14 19:36:36.568166585 +0000
@@ -1,30 +1,35 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
-
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+Environment=HTTP_PROXY=http://webproxy.deutsche-boerse.de:8080
+Environment=HTTPS_PROXY=http://webproxy.deutsche-boerse.de:8080
+Environment=NO_PROXY=localhost,127.0.0.1,.dbgcloud.io,.deutsche-boerse.de,172.23.65.1/24,192.168.58.0/24,192.168.59.0/24,192.168.39.0/24,192.168.49.0/24,10.96.0.0/12
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=podman --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
@@ -32,16 +37,16 @@
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I1114 20:36:38.058677   12113 machine.go:91] provisioned docker machine in 14.265197592s
I1114 20:36:38.058683   12113 client.go:171] LocalClient.Create took 33.844301869s
I1114 20:36:38.058786   12113 start.go:167] duration metric: libmachine.API.Create for "minikube" took 33.844426854s
I1114 20:36:38.058790   12113 start.go:300] post-start starting for "minikube-m02" (driver="podman")
I1114 20:36:38.058795   12113 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1114 20:36:38.058826   12113 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1114 20:36:38.058846   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:36:38.104162   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1114 20:36:38.189241   12113 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:37223 SSHKeyPath:/home/volkan/.minikube/machines/minikube-m02/id_rsa Username:docker}
I1114 20:36:38.276254   12113 ssh_runner.go:195] Run: cat /etc/os-release
I1114 20:36:38.280657   12113 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1114 20:36:38.280670   12113 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1114 20:36:38.280674   12113 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1114 20:36:38.280677   12113 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I1114 20:36:38.280682   12113 filesync.go:126] Scanning /home/volkan/.minikube/addons for local assets ...
I1114 20:36:38.280708   12113 filesync.go:126] Scanning /home/volkan/.minikube/files for local assets ...
I1114 20:36:38.280786   12113 start.go:303] post-start completed in 221.992556ms
I1114 20:36:38.281047   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f {{.NetworkSettings.IPAddress}} minikube-m02
I1114 20:36:38.351323   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m02
I1114 20:36:38.421270   12113 profile.go:148] Saving config to /home/volkan/.minikube/profiles/minikube/config.json ...
I1114 20:36:38.421771   12113 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1114 20:36:38.421874   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:36:38.513274   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1114 20:36:38.756984   12113 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:37223 SSHKeyPath:/home/volkan/.minikube/machines/minikube-m02/id_rsa Username:docker}
I1114 20:36:38.846368   12113 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1114 20:36:38.916568   12113 start.go:128] duration metric: createHost completed in 34.705945175s
I1114 20:36:38.916577   12113 start.go:83] releasing machines lock for "minikube-m02", held for 34.705998057s
I1114 20:36:38.916622   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f {{.NetworkSettings.IPAddress}} minikube-m02
I1114 20:36:39.028741   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m02
I1114 20:36:39.104440   12113 out.go:177] * Found network options:
I1114 20:36:39.107996   12113 out.go:177]   - HTTP_PROXY=http://webproxy.deutsche-boerse.de:8080
W1114 20:36:39.110897   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.110907   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.110911   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.110914   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.110919   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.110922   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.110925   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.110928   12113 proxy.go:119] fail to check proxy env: Error ip not in block
I1114 20:36:39.113177   12113 out.go:177]   - HTTPS_PROXY=http://webproxy.deutsche-boerse.de:8080
W1114 20:36:39.119534   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.119545   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.119551   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.119554   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.119614   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.119619   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.119622   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.119625   12113 proxy.go:119] fail to check proxy env: Error ip not in block
I1114 20:36:39.122159   12113 out.go:177]   - NO_PROXY=localhost,127.0.0.1,.dbgcloud.io,.deutsche-boerse.de,172.23.65.1/24,192.168.58.0/24,192.168.59.0/24,192.168.39.0/24,192.168.49.0/24,10.96.0.0/12
W1114 20:36:39.134725   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.134737   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.134746   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.134750   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.134755   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.134828   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.134832   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.134836   12113 proxy.go:119] fail to check proxy env: Error ip not in block
I1114 20:36:39.137384   12113 out.go:177]   - HTTP_PROXY=http://webproxy.deutsche-boerse.de:8080
W1114 20:36:39.140087   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.140175   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.140178   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.140181   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.140186   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.140189   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.140192   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.140195   12113 proxy.go:119] fail to check proxy env: Error ip not in block
I1114 20:36:39.142278   12113 out.go:177]   - HTTPS_PROXY=http://webproxy.deutsche-boerse.de:8080
W1114 20:36:39.144947   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.144955   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.144959   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.144962   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.144966   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.145020   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.145024   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.145027   12113 proxy.go:119] fail to check proxy env: Error ip not in block
I1114 20:36:39.147315   12113 out.go:177]   - NO_PROXY=localhost,127.0.0.1,.dbgcloud.io,.deutsche-boerse.de,172.23.65.1/24,192.168.58.0/24,192.168.59.0/24,192.168.39.0/24,192.168.49.0/24,10.96.0.0/12
W1114 20:36:39.149726   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.149738   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.149742   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.149745   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.149749   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.149752   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.149755   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.149758   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.149770   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.149773   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.149776   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.149782   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.149786   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.149789   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.149792   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:36:39.149796   12113 proxy.go:119] fail to check proxy env: Error ip not in block
I1114 20:36:39.149828   12113 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1114 20:36:39.149850   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:36:39.150180   12113 ssh_runner.go:195] Run: curl -x http://webproxy.deutsche-boerse.de:8080 -sS -m 2 https://registry.k8s.io/
I1114 20:36:39.150210   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:36:39.239772   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1114 20:36:39.240199   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1114 20:36:39.430275   12113 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:37223 SSHKeyPath:/home/volkan/.minikube/machines/minikube-m02/id_rsa Username:docker}
I1114 20:36:39.482238   12113 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:37223 SSHKeyPath:/home/volkan/.minikube/machines/minikube-m02/id_rsa Username:docker}
I1114 20:36:39.508925   12113 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1114 20:36:39.558814   12113 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1114 20:36:39.560553   12113 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1114 20:37:27.716548   12113 ssh_runner.go:235] Completed: curl -x http://webproxy.deutsche-boerse.de:8080 -sS -m 2 https://registry.k8s.io/: (48.566350584s)
W1114 20:37:27.716565   12113 start.go:840] [curl -x http://webproxy.deutsche-boerse.de:8080 -sS -m 2 https://registry.k8s.io/] failed: curl -x http://webproxy.deutsche-boerse.de:8080 -sS -m 2 https://registry.k8s.io/: Process exited with status 28
stdout:

stderr:
curl: (28) Resolving timed out after 2002 milliseconds
W1114 20:37:27.716602   12113 out.go:239] ! This container is having trouble accessing https://registry.k8s.io
W1114 20:37:27.716613   12113 out.go:239] * To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I1114 20:37:27.716761   12113 ssh_runner.go:235] Completed: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;: (48.156189031s)
I1114 20:37:27.716770   12113 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I1114 20:37:27.716778   12113 start.go:472] detecting cgroup driver to use...
I1114 20:37:27.716920   12113 detect.go:196] detected "cgroupfs" cgroup driver on host os
I1114 20:37:27.716979   12113 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1114 20:37:27.772088   12113 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I1114 20:37:27.790166   12113 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1114 20:37:27.823198   12113 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I1114 20:37:27.823229   12113 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1114 20:37:27.845648   12113 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1114 20:37:27.919722   12113 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1114 20:37:27.935596   12113 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1114 20:37:27.965724   12113 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1114 20:37:28.089658   12113 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1114 20:37:28.137338   12113 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1114 20:37:28.176969   12113 crio.go:148] couldn't verify netfilter by "sudo sysctl net.bridge.bridge-nf-call-iptables" which might be okay. error: sudo sysctl net.bridge.bridge-nf-call-iptables: Process exited with status 255
stdout:

stderr:
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory
I1114 20:37:28.176998   12113 ssh_runner.go:195] Run: sudo modprobe br_netfilter
I1114 20:37:28.193088   12113 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1114 20:37:28.203768   12113 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1114 20:37:28.418258   12113 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1114 20:37:28.615939   12113 start.go:472] detecting cgroup driver to use...
I1114 20:37:28.615964   12113 detect.go:196] detected "cgroupfs" cgroup driver on host os
I1114 20:37:28.615992   12113 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1114 20:37:28.661723   12113 cruntime.go:279] skipping containerd shutdown because we are bound to it
I1114 20:37:28.661761   12113 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1114 20:37:28.696380   12113 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1114 20:37:28.812428   12113 ssh_runner.go:195] Run: which cri-dockerd
I1114 20:37:28.882279   12113 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1114 20:37:28.920442   12113 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I1114 20:37:29.005214   12113 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1114 20:37:29.220853   12113 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1114 20:37:29.778838   12113 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I1114 20:37:29.778860   12113 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1114 20:37:30.025824   12113 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1114 20:37:30.633655   12113 ssh_runner.go:195] Run: sudo systemctl restart docker
I1114 20:37:32.927342   12113 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.293667757s)
I1114 20:37:32.927378   12113 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1114 20:37:32.992405   12113 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1114 20:37:33.076275   12113 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1114 20:37:33.262726   12113 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1114 20:37:33.359574   12113 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1114 20:37:33.382776   12113 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1114 20:37:33.549728   12113 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I1114 20:37:33.748185   12113 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1114 20:37:33.748217   12113 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1114 20:37:33.802420   12113 start.go:540] Will wait 60s for crictl version
I1114 20:37:33.802450   12113 ssh_runner.go:195] Run: which crictl
I1114 20:37:33.863290   12113 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1114 20:37:34.112837   12113 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I1114 20:37:34.112875   12113 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1114 20:37:34.216818   12113 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1114 20:37:34.392885   12113 out.go:204] * Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I1114 20:37:34.397351   12113 out.go:177]   - env HTTP_PROXY=http://webproxy.deutsche-boerse.de:8080
I1114 20:37:34.427365   12113 out.go:177]   - env HTTPS_PROXY=http://webproxy.deutsche-boerse.de:8080
I1114 20:37:34.444373   12113 out.go:177]   - env NO_PROXY=localhost,127.0.0.1,.dbgcloud.io,.deutsche-boerse.de,172.23.65.1/24,192.168.58.0/24,192.168.59.0/24,192.168.39.0/24,192.168.49.0/24,10.96.0.0/12
I1114 20:37:34.449931   12113 cli_runner.go:164] Run: sudo -n podman container inspect --format {{.NetworkSettings.Gateway}} minikube-m02
I1114 20:37:34.651824   12113 cli_runner.go:164] Run: sudo -n podman container inspect --format "
{{ if index .NetworkSettings.Networks "minikube-m02"}} 
	{{(index .NetworkSettings.Networks "minikube-m02").Gateway}}
{{ end }}
" minikube-m02
I1114 20:37:34.896815   12113 cli_runner.go:164] Run: sudo -n podman container inspect --format "
{{ if index .NetworkSettings.Networks "podman"}} 
	{{(index .NetworkSettings.Networks "podman").Gateway}}
{{ end }}
" minikube-m02
I1114 20:37:35.018375   12113 network.go:124] Couldn't find gateway for container minikube-m02
I1114 20:37:35.018416   12113 ssh_runner.go:195] Run: grep <nil>	host.minikube.internal$ /etc/hosts
I1114 20:37:35.075506   12113 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "<nil>	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1114 20:37:35.093357   12113 certs.go:56] Setting up /home/volkan/.minikube/profiles/minikube for IP: 192.168.49.3
I1114 20:37:35.093373   12113 certs.go:190] acquiring lock for shared ca certs: {Name:mkcf3862dec8fde9801070582903515da6d63ece Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1114 20:37:35.093502   12113 certs.go:199] skipping minikubeCA CA generation: /home/volkan/.minikube/ca.key
I1114 20:37:35.093522   12113 certs.go:199] skipping proxyClientCA CA generation: /home/volkan/.minikube/proxy-client-ca.key
I1114 20:37:35.093563   12113 certs.go:437] found cert: /home/volkan/.minikube/certs/home/volkan/.minikube/certs/ca-key.pem (1679 bytes)
I1114 20:37:35.093575   12113 certs.go:437] found cert: /home/volkan/.minikube/certs/home/volkan/.minikube/certs/ca.pem (1078 bytes)
I1114 20:37:35.093595   12113 certs.go:437] found cert: /home/volkan/.minikube/certs/home/volkan/.minikube/certs/cert.pem (1119 bytes)
I1114 20:37:35.093606   12113 certs.go:437] found cert: /home/volkan/.minikube/certs/home/volkan/.minikube/certs/key.pem (1675 bytes)
I1114 20:37:35.093820   12113 ssh_runner.go:362] scp /home/volkan/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1114 20:37:35.144479   12113 ssh_runner.go:362] scp /home/volkan/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1114 20:37:35.216779   12113 ssh_runner.go:362] scp /home/volkan/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1114 20:37:35.293138   12113 ssh_runner.go:362] scp /home/volkan/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1114 20:37:35.326447   12113 ssh_runner.go:362] scp /home/volkan/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1114 20:37:35.401086   12113 ssh_runner.go:195] Run: openssl version
I1114 20:37:35.441474   12113 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1114 20:37:35.465546   12113 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1114 20:37:35.474556   12113 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Nov 14 19:26 /usr/share/ca-certificates/minikubeCA.pem
I1114 20:37:35.474587   12113 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1114 20:37:35.484742   12113 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1114 20:37:35.519895   12113 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I1114 20:37:35.529818   12113 certs.go:353] certs directory doesn't exist, likely first start: ls /var/lib/minikube/certs/etcd: Process exited with status 2
stdout:

stderr:
ls: cannot access '/var/lib/minikube/certs/etcd': No such file or directory
I1114 20:37:35.529876   12113 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1114 20:37:35.761469   12113 cni.go:84] Creating CNI manager for ""
I1114 20:37:35.761475   12113 cni.go:136] 2 nodes found, recommending kindnet
I1114 20:37:35.761481   12113 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1114 20:37:35.761491   12113 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.3 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube-m02 DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.3 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1114 20:37:35.761573   12113 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.3
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube-m02"
  kubeletExtraArgs:
    node-ip: 192.168.49.3
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1114 20:37:35.761600   12113 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube-m02 --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.3

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1114 20:37:35.761632   12113 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I1114 20:37:35.793866   12113 binaries.go:44] Found k8s binaries, skipping transfer
I1114 20:37:35.793901   12113 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system
I1114 20:37:35.805516   12113 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (373 bytes)
I1114 20:37:35.873777   12113 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1114 20:37:35.953906   12113 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1114 20:37:35.959029   12113 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1114 20:37:36.013883   12113 host.go:66] Checking if "minikube" exists ...
I1114 20:37:36.013998   12113 config.go:182] Loaded profile config "minikube": Driver=podman, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1114 20:37:36.014014   12113 start.go:304] JoinCluster: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:podman HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/volkan:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I1114 20:37:36.014068   12113 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm token create --print-join-command --ttl=0"
I1114 20:37:36.014096   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:37:36.112853   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1114 20:37:36.322296   12113 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:40883 SSHKeyPath:/home/volkan/.minikube/machines/minikube/id_rsa Username:docker}
I1114 20:37:36.857256   12113 start.go:325] trying to join worker node "m02" to cluster: &{Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true}
I1114 20:37:36.857275   12113 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm join control-plane.minikube.internal:8443 --token 7pj3j6.x9co49r1fubdl0g9 --discovery-token-ca-cert-hash sha256:a2252d43c14054fc6733fec7a145908c7919f85fb780918953be2b01b36b46af --ignore-preflight-errors=all --cri-socket /var/run/cri-dockerd.sock --node-name=minikube-m02"
I1114 20:37:40.915228   12113 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm join control-plane.minikube.internal:8443 --token 7pj3j6.x9co49r1fubdl0g9 --discovery-token-ca-cert-hash sha256:a2252d43c14054fc6733fec7a145908c7919f85fb780918953be2b01b36b46af --ignore-preflight-errors=all --cri-socket /var/run/cri-dockerd.sock --node-name=minikube-m02": (4.057935764s)
I1114 20:37:40.915247   12113 ssh_runner.go:195] Run: /bin/bash -c "sudo systemctl daemon-reload && sudo systemctl enable kubelet && sudo systemctl start kubelet"
I1114 20:37:41.534808   12113 start.go:306] JoinCluster complete in 5.520789087s
I1114 20:37:41.534818   12113 cni.go:84] Creating CNI manager for ""
I1114 20:37:41.534821   12113 cni.go:136] 2 nodes found, recommending kindnet
I1114 20:37:41.534849   12113 ssh_runner.go:195] Run: stat /opt/cni/bin/portmap
I1114 20:37:41.586877   12113 cni.go:182] applying CNI manifest using /var/lib/minikube/binaries/v1.28.3/kubectl ...
I1114 20:37:41.586889   12113 ssh_runner.go:362] scp memory --> /var/tmp/minikube/cni.yaml (2438 bytes)
I1114 20:37:41.961029   12113 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.28.3/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml
I1114 20:37:44.855459   12113 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.28.3/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml: (2.894406585s)
I1114 20:37:44.870116   12113 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I1114 20:37:44.870136   12113 start.go:223] Will wait 6m0s for node &{Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true}
I1114 20:37:44.913776   12113 out.go:177] * Verifying Kubernetes components...
I1114 20:37:44.981476   12113 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1114 20:37:45.126201   12113 kubeadm.go:581] duration metric: took 256.04795ms to wait for : map[apiserver:true system_pods:true] ...
I1114 20:37:45.126216   12113 node_conditions.go:102] verifying NodePressure condition ...
I1114 20:37:45.135915   12113 node_conditions.go:122] node storage ephemeral capacity is 40302856Ki
I1114 20:37:45.135926   12113 node_conditions.go:123] node cpu capacity is 6
I1114 20:37:45.135933   12113 node_conditions.go:122] node storage ephemeral capacity is 40302856Ki
I1114 20:37:45.135935   12113 node_conditions.go:123] node cpu capacity is 6
I1114 20:37:45.135937   12113 node_conditions.go:105] duration metric: took 9.718749ms to run NodePressure ...
I1114 20:37:45.135944   12113 start.go:228] waiting for startup goroutines ...
I1114 20:37:45.135981   12113 start.go:242] writing updated cluster config ...
I1114 20:37:45.193999   12113 out.go:177] 
I1114 20:37:45.250725   12113 config.go:182] Loaded profile config "minikube": Driver=podman, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1114 20:37:45.250794   12113 profile.go:148] Saving config to /home/volkan/.minikube/profiles/minikube/config.json ...
I1114 20:37:45.357464   12113 out.go:177] * Starting worker node minikube-m03 in cluster minikube
I1114 20:37:45.441974   12113 cache.go:121] Beginning downloading kic base image for podman with docker
I1114 20:37:45.513750   12113 out.go:177] * Pulling base image ...
I1114 20:37:45.586973   12113 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I1114 20:37:45.586994   12113 cache.go:56] Caching tarball of preloaded images
I1114 20:37:45.588162   12113 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 to local cache
I1114 20:37:45.588229   12113 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local cache directory
I1114 20:37:45.588239   12113 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local cache directory, skipping pull
I1114 20:37:45.588240   12113 image.go:105] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in cache, skipping pull
I1114 20:37:45.588246   12113 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 as a tarball
I1114 20:37:45.594735   12113 preload.go:174] Found /home/volkan/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1114 20:37:45.594752   12113 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I1114 20:37:45.594829   12113 profile.go:148] Saving config to /home/volkan/.minikube/profiles/minikube/config.json ...
E1114 20:37:45.601632   12113 cache.go:189] Error downloading kic artifacts:  not yet implemented, see issue #8426
I1114 20:37:45.601679   12113 cache.go:194] Successfully downloaded all kic artifacts
I1114 20:37:45.601776   12113 start.go:365] acquiring machines lock for minikube-m03: {Name:mk14bad58ae0ea254fed28cfa47b68ce31a13320 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1114 20:37:45.601857   12113 start.go:369] acquired machines lock for "minikube-m03" in 71.238µs
I1114 20:37:45.601869   12113 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:podman HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m03 IP: Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:false efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:false storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/volkan:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:} &{Name:m03 IP: Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true}
I1114 20:37:45.601994   12113 start.go:125] createHost starting for "m03" (driver="podman")
I1114 20:37:45.618896   12113 out.go:204] * Creating podman container (CPUs=2, Memory=2200MB) ...
I1114 20:37:45.625847   12113 start.go:159] libmachine.API.Create for "minikube" (driver="podman")
I1114 20:37:45.635563   12113 client.go:168] LocalClient.Create starting
I1114 20:37:45.635622   12113 main.go:141] libmachine: Reading certificate data from /home/volkan/.minikube/certs/ca.pem
I1114 20:37:45.635645   12113 main.go:141] libmachine: Decoding PEM data...
I1114 20:37:45.635656   12113 main.go:141] libmachine: Parsing certificate...
I1114 20:37:45.635692   12113 main.go:141] libmachine: Reading certificate data from /home/volkan/.minikube/certs/cert.pem
I1114 20:37:45.635699   12113 main.go:141] libmachine: Decoding PEM data...
I1114 20:37:45.635703   12113 main.go:141] libmachine: Parsing certificate...
I1114 20:37:45.635857   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:37:46.067884   12113 cli_runner.go:164] Run: sudo -n podman network inspect minikube --format "{{range .}}{{if eq .Driver "bridge"}}{{(index .Subnets 0).Subnet}},{{(index .Subnets 0).Gateway}}{{end}}{{end}}"
I1114 20:37:46.451003   12113 network_create.go:77] Found existing network {name:minikube subnet:0xc003018d50 gateway:[0 0 0 0 0 0 0 0 0 0 255 255 192 168 49 1] mtu:0}
I1114 20:37:46.451021   12113 kic.go:121] calculated static IP "192.168.49.4" for the "minikube-m03" container
I1114 20:37:46.451059   12113 cli_runner.go:164] Run: sudo -n podman ps -a --format {{.Names}}
I1114 20:37:46.784510   12113 cli_runner.go:164] Run: sudo -n podman volume create minikube-m03 --label name.minikube.sigs.k8s.io=minikube-m03 --label created_by.minikube.sigs.k8s.io=true
I1114 20:37:47.194494   12113 oci.go:103] Successfully created a podman volume minikube-m03
I1114 20:37:47.194539   12113 cli_runner.go:164] Run: sudo -n podman run --rm --name minikube-m03-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m03 --entrypoint /usr/bin/test -v minikube-m03:/var gcr.io/k8s-minikube/kicbase:v0.0.42 -d /var/lib
I1114 20:37:53.604757   12113 cli_runner.go:217] Completed: sudo -n podman run --rm --name minikube-m03-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m03 --entrypoint /usr/bin/test -v minikube-m03:/var gcr.io/k8s-minikube/kicbase:v0.0.42 -d /var/lib: (6.410194472s)
I1114 20:37:53.604769   12113 oci.go:107] Successfully prepared a podman volume minikube-m03
I1114 20:37:53.604814   12113 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I1114 20:37:53.604827   12113 kic.go:194] Starting extracting preloaded images to volume ...
I1114 20:37:53.604862   12113 cli_runner.go:164] Run: sudo -n podman run --rm --entrypoint /usr/bin/tar --security-opt label=disable -v /home/volkan/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube-m03:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.42 -I lz4 -xf /preloaded.tar -C /extractDir
I1114 20:38:05.004639   12113 cli_runner.go:217] Completed: sudo -n podman run --rm --entrypoint /usr/bin/tar --security-opt label=disable -v /home/volkan/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube-m03:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.42 -I lz4 -xf /preloaded.tar -C /extractDir: (11.399743102s)
I1114 20:38:05.004658   12113 kic.go:203] duration metric: took 11.399829 seconds to extract preloaded images to volume
W1114 20:38:05.004767   12113 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
I1114 20:38:05.004822   12113 cli_runner.go:164] Run: sudo -n podman info --format "'{{json .SecurityOptions}}'"
W1114 20:38:05.557175   12113 cli_runner.go:211] sudo -n podman info --format "'{{json .SecurityOptions}}'" returned with exit code 125
I1114 20:38:05.557239   12113 cli_runner.go:164] Run: sudo -n podman run --cgroup-manager cgroupfs -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube-m03 --name minikube-m03 --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m03 --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube-m03 --network minikube --ip 192.168.49.4 --volume minikube-m03:/var:exec --memory=2200mb --cpus=2 -e container=podman --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.42
I1114 20:38:09.194653   12113 cli_runner.go:217] Completed: sudo -n podman run --cgroup-manager cgroupfs -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube-m03 --name minikube-m03 --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m03 --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube-m03 --network minikube --ip 192.168.49.4 --volume minikube-m03:/var:exec --memory=2200mb --cpus=2 -e container=podman --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.42: (3.637383763s)
I1114 20:38:09.194711   12113 cli_runner.go:164] Run: sudo -n podman container inspect minikube-m03 --format={{.State.Running}}
I1114 20:38:09.660023   12113 cli_runner.go:164] Run: sudo -n podman container inspect minikube-m03 --format={{.State.Status}}
I1114 20:38:10.222476   12113 cli_runner.go:164] Run: sudo -n podman exec minikube-m03 stat /var/lib/dpkg/alternatives/iptables
I1114 20:38:11.253562   12113 cli_runner.go:217] Completed: sudo -n podman exec minikube-m03 stat /var/lib/dpkg/alternatives/iptables: (1.031065218s)
I1114 20:38:11.253573   12113 oci.go:144] the created container "minikube-m03" has a running status.
I1114 20:38:11.253589   12113 kic.go:225] Creating ssh key for kic: /home/volkan/.minikube/machines/minikube-m03/id_rsa...
I1114 20:38:12.047366   12113 kic_runner.go:191] podman (temp): /home/volkan/.minikube/machines/minikube-m03/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I1114 20:38:12.070681   12113 kic_runner.go:261] Run: /usr/bin/sudo -n podman cp /tmp/tmpf-memory-asset4068340197 minikube-m03:/home/docker/.ssh/authorized_keys
I1114 20:38:13.984575   12113 cli_runner.go:164] Run: sudo -n podman container inspect minikube-m03 --format={{.State.Status}}
I1114 20:38:14.352836   12113 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I1114 20:38:14.352849   12113 kic_runner.go:114] Args: [sudo -n podman exec --privileged minikube-m03 chown docker:docker /home/docker/.ssh/authorized_keys]
I1114 20:38:15.026965   12113 cli_runner.go:164] Run: sudo -n podman container inspect minikube-m03 --format={{.State.Status}}
I1114 20:38:15.361439   12113 machine.go:88] provisioning docker machine ...
I1114 20:38:15.361457   12113 ubuntu.go:169] provisioning hostname "minikube-m03"
I1114 20:38:15.361494   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:38:15.551129   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I1114 20:38:15.736606   12113 main.go:141] libmachine: Using SSH client type: native
I1114 20:38:15.736803   12113 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 40509 <nil> <nil>}
I1114 20:38:15.736808   12113 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube-m03 && echo "minikube-m03" | sudo tee /etc/hostname
I1114 20:38:15.737147   12113 main.go:141] libmachine: Error dialing TCP: dial tcp 127.0.0.1:40509: connect: connection refused
I1114 20:38:20.206378   12113 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube-m03

I1114 20:38:20.206423   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:38:20.817434   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I1114 20:38:21.358520   12113 main.go:141] libmachine: Using SSH client type: native
I1114 20:38:21.358714   12113 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 40509 <nil> <nil>}
I1114 20:38:21.358721   12113 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube-m03' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube-m03/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube-m03' | sudo tee -a /etc/hosts; 
			fi
		fi
I1114 20:38:21.897566   12113 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1114 20:38:21.897581   12113 ubuntu.go:175] set auth options {CertDir:/home/volkan/.minikube CaCertPath:/home/volkan/.minikube/certs/ca.pem CaPrivateKeyPath:/home/volkan/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/volkan/.minikube/machines/server.pem ServerKeyPath:/home/volkan/.minikube/machines/server-key.pem ClientKeyPath:/home/volkan/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/volkan/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/volkan/.minikube}
I1114 20:38:21.897588   12113 ubuntu.go:177] setting up certificates
I1114 20:38:21.897593   12113 provision.go:83] configureAuth start
I1114 20:38:21.897627   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f {{.NetworkSettings.IPAddress}} minikube-m03
I1114 20:38:22.184482   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m03
I1114 20:38:22.963230   12113 provision.go:138] copyHostCerts
I1114 20:38:22.963262   12113 exec_runner.go:144] found /home/volkan/.minikube/ca.pem, removing ...
I1114 20:38:22.963266   12113 exec_runner.go:203] rm: /home/volkan/.minikube/ca.pem
I1114 20:38:22.963310   12113 exec_runner.go:151] cp: /home/volkan/.minikube/certs/ca.pem --> /home/volkan/.minikube/ca.pem (1078 bytes)
I1114 20:38:22.964556   12113 exec_runner.go:144] found /home/volkan/.minikube/cert.pem, removing ...
I1114 20:38:22.964561   12113 exec_runner.go:203] rm: /home/volkan/.minikube/cert.pem
I1114 20:38:22.964579   12113 exec_runner.go:151] cp: /home/volkan/.minikube/certs/cert.pem --> /home/volkan/.minikube/cert.pem (1119 bytes)
I1114 20:38:22.964601   12113 exec_runner.go:144] found /home/volkan/.minikube/key.pem, removing ...
I1114 20:38:22.964603   12113 exec_runner.go:203] rm: /home/volkan/.minikube/key.pem
I1114 20:38:22.964613   12113 exec_runner.go:151] cp: /home/volkan/.minikube/certs/key.pem --> /home/volkan/.minikube/key.pem (1675 bytes)
I1114 20:38:22.966815   12113 provision.go:112] generating server cert: /home/volkan/.minikube/machines/server.pem ca-key=/home/volkan/.minikube/certs/ca.pem private-key=/home/volkan/.minikube/certs/ca-key.pem org=volkan.minikube-m03 san=[192.168.49.4 127.0.0.1 localhost 127.0.0.1 minikube minikube-m03]
I1114 20:38:23.386207   12113 provision.go:172] copyRemoteCerts
I1114 20:38:23.386238   12113 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1114 20:38:23.386259   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:38:23.735615   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I1114 20:38:24.108119   12113 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:40509 SSHKeyPath:/home/volkan/.minikube/machines/minikube-m03/id_rsa Username:docker}
I1114 20:38:24.471070   12113 ssh_runner.go:235] Completed: sudo mkdir -p /etc/docker /etc/docker /etc/docker: (1.084814083s)
I1114 20:38:24.471104   12113 ssh_runner.go:362] scp /home/volkan/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I1114 20:38:24.530531   12113 ssh_runner.go:362] scp /home/volkan/.minikube/machines/server.pem --> /etc/docker/server.pem (1212 bytes)
I1114 20:38:24.705568   12113 ssh_runner.go:362] scp /home/volkan/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1114 20:38:24.785008   12113 provision.go:86] duration metric: configureAuth took 2.887405196s
I1114 20:38:24.785021   12113 ubuntu.go:193] setting minikube options for container-runtime
I1114 20:38:24.785194   12113 config.go:182] Loaded profile config "minikube": Driver=podman, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1114 20:38:24.785233   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:38:24.917033   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I1114 20:38:25.254643   12113 main.go:141] libmachine: Using SSH client type: native
I1114 20:38:25.254835   12113 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 40509 <nil> <nil>}
I1114 20:38:25.254839   12113 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1114 20:38:25.517478   12113 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1114 20:38:25.517487   12113 ubuntu.go:71] root file system type: overlay
I1114 20:38:25.517630   12113 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1114 20:38:25.517715   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:38:25.648690   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I1114 20:38:25.786128   12113 main.go:141] libmachine: Using SSH client type: native
I1114 20:38:25.786331   12113 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 40509 <nil> <nil>}
I1114 20:38:25.786365   12113 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment="HTTP_PROXY=http://webproxy.deutsche-boerse.de:8080"
Environment="HTTPS_PROXY=http://webproxy.deutsche-boerse.de:8080"
Environment="NO_PROXY=localhost,127.0.0.1,.dbgcloud.io,.deutsche-boerse.de,172.23.65.1/24,192.168.58.0/24,192.168.59.0/24,192.168.39.0/24,192.168.49.0/24,10.96.0.0/12"


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=podman --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1114 20:38:26.045347   12113 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment=HTTP_PROXY=http://webproxy.deutsche-boerse.de:8080
Environment=HTTPS_PROXY=http://webproxy.deutsche-boerse.de:8080
Environment=NO_PROXY=localhost,127.0.0.1,.dbgcloud.io,.deutsche-boerse.de,172.23.65.1/24,192.168.58.0/24,192.168.59.0/24,192.168.39.0/24,192.168.49.0/24,10.96.0.0/12


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=podman --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1114 20:38:26.045477   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:38:26.399852   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I1114 20:38:26.552798   12113 main.go:141] libmachine: Using SSH client type: native
I1114 20:38:26.553042   12113 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 40509 <nil> <nil>}
I1114 20:38:26.553051   12113 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1114 20:38:28.817503   12113 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2023-10-26 09:06:22.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2023-11-14 19:38:26.039533249 +0000
@@ -1,30 +1,35 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
-
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+Environment=HTTP_PROXY=http://webproxy.deutsche-boerse.de:8080
+Environment=HTTPS_PROXY=http://webproxy.deutsche-boerse.de:8080
+Environment=NO_PROXY=localhost,127.0.0.1,.dbgcloud.io,.deutsche-boerse.de,172.23.65.1/24,192.168.58.0/24,192.168.59.0/24,192.168.39.0/24,192.168.49.0/24,10.96.0.0/12
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=podman --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
@@ -32,16 +37,16 @@
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I1114 20:38:28.817517   12113 machine.go:91] provisioned docker machine in 13.456069872s
I1114 20:38:28.817521   12113 client.go:171] LocalClient.Create took 43.181950633s
I1114 20:38:28.817528   12113 start.go:167] duration metric: libmachine.API.Create for "minikube" took 43.191683598s
I1114 20:38:28.817582   12113 start.go:300] post-start starting for "minikube-m03" (driver="podman")
I1114 20:38:28.817588   12113 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1114 20:38:28.817618   12113 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1114 20:38:28.817641   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:38:29.105426   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I1114 20:38:29.264083   12113 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:40509 SSHKeyPath:/home/volkan/.minikube/machines/minikube-m03/id_rsa Username:docker}
I1114 20:38:29.399288   12113 ssh_runner.go:195] Run: cat /etc/os-release
I1114 20:38:29.405262   12113 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1114 20:38:29.405274   12113 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1114 20:38:29.405279   12113 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1114 20:38:29.405281   12113 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I1114 20:38:29.405287   12113 filesync.go:126] Scanning /home/volkan/.minikube/addons for local assets ...
I1114 20:38:29.405315   12113 filesync.go:126] Scanning /home/volkan/.minikube/files for local assets ...
I1114 20:38:29.405323   12113 start.go:303] post-start completed in 587.738356ms
I1114 20:38:29.405535   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f {{.NetworkSettings.IPAddress}} minikube-m03
I1114 20:38:29.589481   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m03
I1114 20:38:29.878903   12113 profile.go:148] Saving config to /home/volkan/.minikube/profiles/minikube/config.json ...
I1114 20:38:29.880676   12113 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1114 20:38:29.880699   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:38:30.023975   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I1114 20:38:30.277626   12113 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:40509 SSHKeyPath:/home/volkan/.minikube/machines/minikube-m03/id_rsa Username:docker}
I1114 20:38:30.436283   12113 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1114 20:38:30.470202   12113 start.go:128] duration metric: createHost completed in 44.868197388s
I1114 20:38:30.470212   12113 start.go:83] releasing machines lock for "minikube-m03", held for 44.868350975s
I1114 20:38:30.470261   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f {{.NetworkSettings.IPAddress}} minikube-m03
I1114 20:38:30.738754   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m03
I1114 20:38:31.066952   12113 out.go:177] * Found network options:
I1114 20:38:31.082130   12113 out.go:177]   - HTTP_PROXY=http://webproxy.deutsche-boerse.de:8080
W1114 20:38:31.084839   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.084849   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.084853   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.084856   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.084860   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.084864   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.084867   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.084870   12113 proxy.go:119] fail to check proxy env: Error ip not in block
I1114 20:38:31.087075   12113 out.go:177]   - HTTPS_PROXY=http://webproxy.deutsche-boerse.de:8080
W1114 20:38:31.089546   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.089557   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.089560   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.089563   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.089568   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.089571   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.089575   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.089578   12113 proxy.go:119] fail to check proxy env: Error ip not in block
I1114 20:38:31.091875   12113 out.go:177]   - NO_PROXY=localhost,127.0.0.1,.dbgcloud.io,.deutsche-boerse.de,172.23.65.1/24,192.168.58.0/24,192.168.59.0/24,192.168.39.0/24,192.168.49.0/24,10.96.0.0/12
W1114 20:38:31.093997   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.094004   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.094008   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.094011   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.094016   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.094019   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.094022   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.094025   12113 proxy.go:119] fail to check proxy env: Error ip not in block
I1114 20:38:31.096399   12113 out.go:177]   - HTTP_PROXY=http://webproxy.deutsche-boerse.de:8080
W1114 20:38:31.098609   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.098616   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.098620   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.098623   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.098627   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.098630   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.098633   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.098637   12113 proxy.go:119] fail to check proxy env: Error ip not in block
I1114 20:38:31.100605   12113 out.go:177]   - HTTPS_PROXY=http://webproxy.deutsche-boerse.de:8080
W1114 20:38:31.102817   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.102824   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.102827   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.102833   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.102837   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.102840   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.102843   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.102846   12113 proxy.go:119] fail to check proxy env: Error ip not in block
I1114 20:38:31.104930   12113 out.go:177]   - NO_PROXY=localhost,127.0.0.1,.dbgcloud.io,.deutsche-boerse.de,172.23.65.1/24,192.168.58.0/24,192.168.59.0/24,192.168.39.0/24,192.168.49.0/24,10.96.0.0/12
W1114 20:38:31.107205   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.107212   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.107215   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.107219   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.107223   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.107226   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.107229   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.107233   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.107269   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.107273   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.107276   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.107279   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.107282   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.107286   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.107289   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:38:31.107292   12113 proxy.go:119] fail to check proxy env: Error ip not in block
I1114 20:38:31.107328   12113 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1114 20:38:31.107348   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:38:31.107695   12113 ssh_runner.go:195] Run: curl -x http://webproxy.deutsche-boerse.de:8080 -sS -m 2 https://registry.k8s.io/
I1114 20:38:31.107723   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:38:31.469289   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I1114 20:38:31.508965   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I1114 20:38:31.862747   12113 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:40509 SSHKeyPath:/home/volkan/.minikube/machines/minikube-m03/id_rsa Username:docker}
I1114 20:38:31.975521   12113 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:40509 SSHKeyPath:/home/volkan/.minikube/machines/minikube-m03/id_rsa Username:docker}
I1114 20:38:32.081694   12113 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1114 20:39:20.483318   12113 ssh_runner.go:235] Completed: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: (48.401603942s)
I1114 20:39:20.483331   12113 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1114 20:39:20.483361   12113 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1114 20:39:20.483518   12113 ssh_runner.go:235] Completed: curl -x http://webproxy.deutsche-boerse.de:8080 -sS -m 2 https://registry.k8s.io/: (49.375809644s)
W1114 20:39:20.483533   12113 start.go:840] [curl -x http://webproxy.deutsche-boerse.de:8080 -sS -m 2 https://registry.k8s.io/] failed: curl -x http://webproxy.deutsche-boerse.de:8080 -sS -m 2 https://registry.k8s.io/: Process exited with status 28
stdout:

stderr:
curl: (28) Resolving timed out after 2000 milliseconds
W1114 20:39:20.483695   12113 out.go:239] ! This container is having trouble accessing https://registry.k8s.io
W1114 20:39:20.483816   12113 out.go:239] * To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I1114 20:39:20.538327   12113 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I1114 20:39:20.538339   12113 start.go:472] detecting cgroup driver to use...
I1114 20:39:20.538358   12113 detect.go:196] detected "cgroupfs" cgroup driver on host os
I1114 20:39:20.538426   12113 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1114 20:39:20.562702   12113 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I1114 20:39:20.628220   12113 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1114 20:39:20.642187   12113 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I1114 20:39:20.642219   12113 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1114 20:39:20.704635   12113 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1114 20:39:20.718818   12113 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1114 20:39:20.753501   12113 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1114 20:39:20.797652   12113 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1114 20:39:20.813179   12113 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1114 20:39:20.833447   12113 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1114 20:39:20.843781   12113 crio.go:148] couldn't verify netfilter by "sudo sysctl net.bridge.bridge-nf-call-iptables" which might be okay. error: sudo sysctl net.bridge.bridge-nf-call-iptables: Process exited with status 255
stdout:

stderr:
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory
I1114 20:39:20.843810   12113 ssh_runner.go:195] Run: sudo modprobe br_netfilter
I1114 20:39:20.902270   12113 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1114 20:39:20.912200   12113 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1114 20:39:21.046261   12113 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1114 20:39:21.359351   12113 start.go:472] detecting cgroup driver to use...
I1114 20:39:21.359375   12113 detect.go:196] detected "cgroupfs" cgroup driver on host os
I1114 20:39:21.359405   12113 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1114 20:39:21.498555   12113 cruntime.go:279] skipping containerd shutdown because we are bound to it
I1114 20:39:21.498586   12113 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1114 20:39:21.542441   12113 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1114 20:39:21.650934   12113 ssh_runner.go:195] Run: which cri-dockerd
I1114 20:39:21.728506   12113 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1114 20:39:21.774385   12113 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I1114 20:39:21.845652   12113 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1114 20:39:22.156552   12113 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1114 20:39:22.381724   12113 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I1114 20:39:22.381811   12113 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1114 20:39:22.443102   12113 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1114 20:39:22.901492   12113 ssh_runner.go:195] Run: sudo systemctl restart docker
I1114 20:39:26.304560   12113 ssh_runner.go:235] Completed: sudo systemctl restart docker: (3.403047596s)
I1114 20:39:26.304593   12113 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1114 20:39:26.541291   12113 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1114 20:39:26.890481   12113 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1114 20:39:27.024921   12113 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1114 20:39:27.214441   12113 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1114 20:39:27.287303   12113 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1114 20:39:27.444683   12113 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I1114 20:39:28.080112   12113 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1114 20:39:28.081940   12113 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1114 20:39:28.110789   12113 start.go:540] Will wait 60s for crictl version
I1114 20:39:28.110825   12113 ssh_runner.go:195] Run: which crictl
I1114 20:39:28.116123   12113 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1114 20:39:28.313834   12113 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I1114 20:39:28.313879   12113 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1114 20:39:28.443106   12113 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1114 20:39:28.625379   12113 out.go:204] * Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I1114 20:39:28.628802   12113 out.go:177]   - env HTTP_PROXY=http://webproxy.deutsche-boerse.de:8080
I1114 20:39:28.653169   12113 out.go:177]   - env HTTPS_PROXY=http://webproxy.deutsche-boerse.de:8080
I1114 20:39:28.657276   12113 out.go:177]   - env NO_PROXY=localhost,127.0.0.1,.dbgcloud.io,.deutsche-boerse.de,172.23.65.1/24,192.168.58.0/24,192.168.59.0/24,192.168.39.0/24,192.168.49.0/24,10.96.0.0/12
I1114 20:39:28.659998   12113 cli_runner.go:164] Run: sudo -n podman container inspect --format {{.NetworkSettings.Gateway}} minikube-m03
I1114 20:39:28.712344   12113 cli_runner.go:164] Run: sudo -n podman container inspect --format "
{{ if index .NetworkSettings.Networks "minikube-m03"}} 
	{{(index .NetworkSettings.Networks "minikube-m03").Gateway}}
{{ end }}
" minikube-m03
I1114 20:39:28.780287   12113 cli_runner.go:164] Run: sudo -n podman container inspect --format "
{{ if index .NetworkSettings.Networks "podman"}} 
	{{(index .NetworkSettings.Networks "podman").Gateway}}
{{ end }}
" minikube-m03
I1114 20:39:28.907583   12113 network.go:124] Couldn't find gateway for container minikube-m03
I1114 20:39:28.907619   12113 ssh_runner.go:195] Run: grep <nil>	host.minikube.internal$ /etc/hosts
I1114 20:39:28.917550   12113 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "<nil>	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1114 20:39:28.930465   12113 certs.go:56] Setting up /home/volkan/.minikube/profiles/minikube for IP: 192.168.49.4
I1114 20:39:28.930478   12113 certs.go:190] acquiring lock for shared ca certs: {Name:mkcf3862dec8fde9801070582903515da6d63ece Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1114 20:39:28.930576   12113 certs.go:199] skipping minikubeCA CA generation: /home/volkan/.minikube/ca.key
I1114 20:39:28.930591   12113 certs.go:199] skipping proxyClientCA CA generation: /home/volkan/.minikube/proxy-client-ca.key
I1114 20:39:28.930627   12113 certs.go:437] found cert: /home/volkan/.minikube/certs/home/volkan/.minikube/certs/ca-key.pem (1679 bytes)
I1114 20:39:28.930639   12113 certs.go:437] found cert: /home/volkan/.minikube/certs/home/volkan/.minikube/certs/ca.pem (1078 bytes)
I1114 20:39:28.930653   12113 certs.go:437] found cert: /home/volkan/.minikube/certs/home/volkan/.minikube/certs/cert.pem (1119 bytes)
I1114 20:39:28.930661   12113 certs.go:437] found cert: /home/volkan/.minikube/certs/home/volkan/.minikube/certs/key.pem (1675 bytes)
I1114 20:39:28.930859   12113 ssh_runner.go:362] scp /home/volkan/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1114 20:39:29.023532   12113 ssh_runner.go:362] scp /home/volkan/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1114 20:39:29.082082   12113 ssh_runner.go:362] scp /home/volkan/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1114 20:39:29.116511   12113 ssh_runner.go:362] scp /home/volkan/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1114 20:39:29.205805   12113 ssh_runner.go:362] scp /home/volkan/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1114 20:39:29.244470   12113 ssh_runner.go:195] Run: openssl version
I1114 20:39:29.249664   12113 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1114 20:39:29.260272   12113 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1114 20:39:29.266455   12113 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Nov 14 19:26 /usr/share/ca-certificates/minikubeCA.pem
I1114 20:39:29.266477   12113 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1114 20:39:29.273899   12113 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1114 20:39:29.367811   12113 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I1114 20:39:29.397672   12113 certs.go:353] certs directory doesn't exist, likely first start: ls /var/lib/minikube/certs/etcd: Process exited with status 2
stdout:

stderr:
ls: cannot access '/var/lib/minikube/certs/etcd': No such file or directory
I1114 20:39:29.397719   12113 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1114 20:39:29.573380   12113 cni.go:84] Creating CNI manager for ""
I1114 20:39:29.618883   12113 cni.go:136] 3 nodes found, recommending kindnet
I1114 20:39:29.618894   12113 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1114 20:39:29.618910   12113 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.4 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube-m03 DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.4 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1114 20:39:29.619040   12113 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.4
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube-m03"
  kubeletExtraArgs:
    node-ip: 192.168.49.4
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1114 20:39:29.619069   12113 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube-m03 --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.4

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1114 20:39:29.619212   12113 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I1114 20:39:29.714029   12113 binaries.go:44] Found k8s binaries, skipping transfer
I1114 20:39:29.714062   12113 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system
I1114 20:39:29.743009   12113 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (373 bytes)
I1114 20:39:29.812080   12113 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1114 20:39:29.840243   12113 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1114 20:39:29.847757   12113 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1114 20:39:29.865467   12113 host.go:66] Checking if "minikube" exists ...
I1114 20:39:29.865585   12113 start.go:304] JoinCluster: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:podman HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m03 IP:192.168.49.4 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:false efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:false storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/volkan:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I1114 20:39:29.865728   12113 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm token create --print-join-command --ttl=0"
I1114 20:39:29.865756   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:39:29.866095   12113 config.go:182] Loaded profile config "minikube": Driver=podman, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1114 20:39:30.177483   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1114 20:39:30.398259   12113 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:40883 SSHKeyPath:/home/volkan/.minikube/machines/minikube/id_rsa Username:docker}
I1114 20:39:33.952807   12113 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm token create --print-join-command --ttl=0": (4.087063195s)
I1114 20:39:33.952831   12113 start.go:325] trying to join worker node "m03" to cluster: &{Name:m03 IP:192.168.49.4 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true}
I1114 20:39:33.952844   12113 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm join control-plane.minikube.internal:8443 --token aaxzvj.vacahk5n4yb16hgu --discovery-token-ca-cert-hash sha256:a2252d43c14054fc6733fec7a145908c7919f85fb780918953be2b01b36b46af --ignore-preflight-errors=all --cri-socket /var/run/cri-dockerd.sock --node-name=minikube-m03"
I1114 20:39:37.509210   12113 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm join control-plane.minikube.internal:8443 --token aaxzvj.vacahk5n4yb16hgu --discovery-token-ca-cert-hash sha256:a2252d43c14054fc6733fec7a145908c7919f85fb780918953be2b01b36b46af --ignore-preflight-errors=all --cri-socket /var/run/cri-dockerd.sock --node-name=minikube-m03": (3.556353036s)
I1114 20:39:37.509221   12113 ssh_runner.go:195] Run: /bin/bash -c "sudo systemctl daemon-reload && sudo systemctl enable kubelet && sudo systemctl start kubelet"
I1114 20:39:39.136061   12113 ssh_runner.go:235] Completed: /bin/bash -c "sudo systemctl daemon-reload && sudo systemctl enable kubelet && sudo systemctl start kubelet": (1.626821562s)
I1114 20:39:39.136074   12113 start.go:306] JoinCluster complete in 9.27048831s
I1114 20:39:39.136079   12113 cni.go:84] Creating CNI manager for ""
I1114 20:39:39.136081   12113 cni.go:136] 3 nodes found, recommending kindnet
I1114 20:39:39.136106   12113 ssh_runner.go:195] Run: stat /opt/cni/bin/portmap
I1114 20:39:39.322142   12113 cni.go:182] applying CNI manifest using /var/lib/minikube/binaries/v1.28.3/kubectl ...
I1114 20:39:39.322149   12113 ssh_runner.go:362] scp memory --> /var/tmp/minikube/cni.yaml (2438 bytes)
I1114 20:39:39.980583   12113 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.28.3/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml
I1114 20:39:43.900416   12113 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.28.3/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml: (3.919815398s)
I1114 20:39:43.952656   12113 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I1114 20:39:43.952674   12113 start.go:223] Will wait 6m0s for node &{Name:m03 IP:192.168.49.4 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true}
I1114 20:39:44.016202   12113 out.go:177] * Verifying Kubernetes components...
I1114 20:39:44.071542   12113 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1114 20:39:44.205414   12113 kubeadm.go:581] duration metric: took 252.722349ms to wait for : map[apiserver:true system_pods:true] ...
I1114 20:39:44.205426   12113 node_conditions.go:102] verifying NodePressure condition ...
I1114 20:39:44.248050   12113 node_conditions.go:122] node storage ephemeral capacity is 40302856Ki
I1114 20:39:44.248060   12113 node_conditions.go:123] node cpu capacity is 6
I1114 20:39:44.248066   12113 node_conditions.go:122] node storage ephemeral capacity is 40302856Ki
I1114 20:39:44.248067   12113 node_conditions.go:123] node cpu capacity is 6
I1114 20:39:44.248069   12113 node_conditions.go:122] node storage ephemeral capacity is 40302856Ki
I1114 20:39:44.248070   12113 node_conditions.go:123] node cpu capacity is 6
I1114 20:39:44.248072   12113 node_conditions.go:105] duration metric: took 42.643875ms to run NodePressure ...
I1114 20:39:44.248079   12113 start.go:228] waiting for startup goroutines ...
I1114 20:39:44.248089   12113 start.go:242] writing updated cluster config ...
I1114 20:39:44.267621   12113 out.go:177] 
I1114 20:39:44.270576   12113 config.go:182] Loaded profile config "minikube": Driver=podman, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1114 20:39:44.270631   12113 profile.go:148] Saving config to /home/volkan/.minikube/profiles/minikube/config.json ...
I1114 20:39:44.276508   12113 out.go:177] * Starting worker node minikube-m04 in cluster minikube
I1114 20:39:44.279518   12113 cache.go:121] Beginning downloading kic base image for podman with docker
I1114 20:39:44.281881   12113 out.go:177] * Pulling base image ...
I1114 20:39:44.284305   12113 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I1114 20:39:44.284315   12113 cache.go:56] Caching tarball of preloaded images
I1114 20:39:44.284367   12113 preload.go:174] Found /home/volkan/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1114 20:39:44.284385   12113 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 to local cache
I1114 20:39:44.284371   12113 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I1114 20:39:44.284425   12113 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local cache directory
I1114 20:39:44.284431   12113 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local cache directory, skipping pull
I1114 20:39:44.284433   12113 image.go:105] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in cache, skipping pull
I1114 20:39:44.284437   12113 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 as a tarball
I1114 20:39:44.284431   12113 profile.go:148] Saving config to /home/volkan/.minikube/profiles/minikube/config.json ...
E1114 20:39:44.284588   12113 cache.go:189] Error downloading kic artifacts:  not yet implemented, see issue #8426
I1114 20:39:44.284630   12113 cache.go:194] Successfully downloaded all kic artifacts
I1114 20:39:44.284641   12113 start.go:365] acquiring machines lock for minikube-m04: {Name:mk7ffc363af273559311a3d6a4d04faacaa2aabb Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1114 20:39:44.284700   12113 start.go:369] acquired machines lock for "minikube-m04" in 51.707µs
I1114 20:39:44.284709   12113 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:podman HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m03 IP:192.168.49.4 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m04 IP: Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:false efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:false storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/volkan:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:} &{Name:m04 IP: Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true}
I1114 20:39:44.284810   12113 start.go:125] createHost starting for "m04" (driver="podman")
I1114 20:39:44.288140   12113 out.go:204] * Creating podman container (CPUs=2, Memory=2200MB) ...
I1114 20:39:44.288253   12113 start.go:159] libmachine.API.Create for "minikube" (driver="podman")
I1114 20:39:44.288263   12113 client.go:168] LocalClient.Create starting
I1114 20:39:44.288290   12113 main.go:141] libmachine: Reading certificate data from /home/volkan/.minikube/certs/ca.pem
I1114 20:39:44.288305   12113 main.go:141] libmachine: Decoding PEM data...
I1114 20:39:44.288312   12113 main.go:141] libmachine: Parsing certificate...
I1114 20:39:44.288339   12113 main.go:141] libmachine: Reading certificate data from /home/volkan/.minikube/certs/cert.pem
I1114 20:39:44.288346   12113 main.go:141] libmachine: Decoding PEM data...
I1114 20:39:44.288351   12113 main.go:141] libmachine: Parsing certificate...
I1114 20:39:44.288456   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:39:44.542763   12113 cli_runner.go:164] Run: sudo -n podman network inspect minikube --format "{{range .}}{{if eq .Driver "bridge"}}{{(index .Subnets 0).Subnet}},{{(index .Subnets 0).Gateway}}{{end}}{{end}}"
I1114 20:39:44.812506   12113 network_create.go:77] Found existing network {name:minikube subnet:0xc00221dd70 gateway:[0 0 0 0 0 0 0 0 0 0 255 255 192 168 49 1] mtu:0}
I1114 20:39:44.812523   12113 kic.go:121] calculated static IP "192.168.49.5" for the "minikube-m04" container
I1114 20:39:44.812568   12113 cli_runner.go:164] Run: sudo -n podman ps -a --format {{.Names}}
I1114 20:39:45.165185   12113 cli_runner.go:164] Run: sudo -n podman volume create minikube-m04 --label name.minikube.sigs.k8s.io=minikube-m04 --label created_by.minikube.sigs.k8s.io=true
I1114 20:39:45.551810   12113 oci.go:103] Successfully created a podman volume minikube-m04
I1114 20:39:45.551854   12113 cli_runner.go:164] Run: sudo -n podman run --rm --name minikube-m04-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m04 --entrypoint /usr/bin/test -v minikube-m04:/var gcr.io/k8s-minikube/kicbase:v0.0.42 -d /var/lib
I1114 20:39:51.879557   12113 cli_runner.go:217] Completed: sudo -n podman run --rm --name minikube-m04-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m04 --entrypoint /usr/bin/test -v minikube-m04:/var gcr.io/k8s-minikube/kicbase:v0.0.42 -d /var/lib: (6.327676417s)
I1114 20:39:51.879571   12113 oci.go:107] Successfully prepared a podman volume minikube-m04
I1114 20:39:51.879592   12113 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I1114 20:39:51.879606   12113 kic.go:194] Starting extracting preloaded images to volume ...
I1114 20:39:51.879679   12113 cli_runner.go:164] Run: sudo -n podman run --rm --entrypoint /usr/bin/tar --security-opt label=disable -v /home/volkan/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube-m04:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.42 -I lz4 -xf /preloaded.tar -C /extractDir
I1114 20:40:07.422540   12113 cli_runner.go:217] Completed: sudo -n podman run --rm --entrypoint /usr/bin/tar --security-opt label=disable -v /home/volkan/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube-m04:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.42 -I lz4 -xf /preloaded.tar -C /extractDir: (15.542829581s)
I1114 20:40:07.422557   12113 kic.go:203] duration metric: took 15.542948 seconds to extract preloaded images to volume
W1114 20:40:07.422681   12113 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
I1114 20:40:07.422747   12113 cli_runner.go:164] Run: sudo -n podman info --format "'{{json .SecurityOptions}}'"
W1114 20:40:08.001286   12113 cli_runner.go:211] sudo -n podman info --format "'{{json .SecurityOptions}}'" returned with exit code 125
I1114 20:40:08.001349   12113 cli_runner.go:164] Run: sudo -n podman run --cgroup-manager cgroupfs -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube-m04 --name minikube-m04 --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m04 --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube-m04 --network minikube --ip 192.168.49.5 --volume minikube-m04:/var:exec --memory=2200mb --cpus=2 -e container=podman --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.42
I1114 20:40:11.207163   12113 cli_runner.go:217] Completed: sudo -n podman run --cgroup-manager cgroupfs -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube-m04 --name minikube-m04 --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m04 --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube-m04 --network minikube --ip 192.168.49.5 --volume minikube-m04:/var:exec --memory=2200mb --cpus=2 -e container=podman --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.42: (3.205789504s)
I1114 20:40:11.207206   12113 cli_runner.go:164] Run: sudo -n podman container inspect minikube-m04 --format={{.State.Running}}
I1114 20:40:11.845141   12113 cli_runner.go:164] Run: sudo -n podman container inspect minikube-m04 --format={{.State.Status}}
I1114 20:40:12.315404   12113 cli_runner.go:164] Run: sudo -n podman exec minikube-m04 stat /var/lib/dpkg/alternatives/iptables
I1114 20:40:13.694686   12113 cli_runner.go:217] Completed: sudo -n podman exec minikube-m04 stat /var/lib/dpkg/alternatives/iptables: (1.379259814s)
I1114 20:40:13.694700   12113 oci.go:144] the created container "minikube-m04" has a running status.
I1114 20:40:13.694708   12113 kic.go:225] Creating ssh key for kic: /home/volkan/.minikube/machines/minikube-m04/id_rsa...
I1114 20:40:13.862959   12113 kic_runner.go:191] podman (temp): /home/volkan/.minikube/machines/minikube-m04/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I1114 20:40:13.897721   12113 kic_runner.go:261] Run: /usr/bin/sudo -n podman cp /tmp/tmpf-memory-asset473135209 minikube-m04:/home/docker/.ssh/authorized_keys
I1114 20:40:16.252019   12113 cli_runner.go:164] Run: sudo -n podman container inspect minikube-m04 --format={{.State.Status}}
I1114 20:40:16.370751   12113 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I1114 20:40:16.370764   12113 kic_runner.go:114] Args: [sudo -n podman exec --privileged minikube-m04 chown docker:docker /home/docker/.ssh/authorized_keys]
I1114 20:40:17.622046   12113 kic_runner.go:123] Done: [sudo -n podman exec --privileged minikube-m04 chown docker:docker /home/docker/.ssh/authorized_keys]: (1.25126457s)
I1114 20:40:17.622106   12113 cli_runner.go:164] Run: sudo -n podman container inspect minikube-m04 --format={{.State.Status}}
I1114 20:40:17.847237   12113 machine.go:88] provisioning docker machine ...
I1114 20:40:17.847255   12113 ubuntu.go:169] provisioning hostname "minikube-m04"
I1114 20:40:17.847292   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:40:17.901347   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m04
I1114 20:40:18.028961   12113 main.go:141] libmachine: Using SSH client type: native
I1114 20:40:18.029256   12113 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 35591 <nil> <nil>}
I1114 20:40:18.029263   12113 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube-m04 && echo "minikube-m04" | sudo tee /etc/hostname
I1114 20:40:18.029474   12113 main.go:141] libmachine: Error dialing TCP: dial tcp 127.0.0.1:35591: connect: connection refused
I1114 20:40:21.029709   12113 main.go:141] libmachine: Error dialing TCP: dial tcp 127.0.0.1:35591: connect: connection refused
I1114 20:40:25.381138   12113 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube-m04

I1114 20:40:25.381180   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:40:25.950591   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m04
I1114 20:40:26.746920   12113 main.go:141] libmachine: Using SSH client type: native
I1114 20:40:26.747114   12113 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 35591 <nil> <nil>}
I1114 20:40:26.749342   12113 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube-m04' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube-m04/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube-m04' | sudo tee -a /etc/hosts; 
			fi
		fi
I1114 20:40:27.590778   12113 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1114 20:40:27.590791   12113 ubuntu.go:175] set auth options {CertDir:/home/volkan/.minikube CaCertPath:/home/volkan/.minikube/certs/ca.pem CaPrivateKeyPath:/home/volkan/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/volkan/.minikube/machines/server.pem ServerKeyPath:/home/volkan/.minikube/machines/server-key.pem ClientKeyPath:/home/volkan/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/volkan/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/volkan/.minikube}
I1114 20:40:27.590797   12113 ubuntu.go:177] setting up certificates
I1114 20:40:27.590801   12113 provision.go:83] configureAuth start
I1114 20:40:27.590837   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f {{.NetworkSettings.IPAddress}} minikube-m04
I1114 20:40:28.265596   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m04
I1114 20:40:28.917766   12113 provision.go:138] copyHostCerts
I1114 20:40:28.917798   12113 exec_runner.go:144] found /home/volkan/.minikube/ca.pem, removing ...
I1114 20:40:28.917802   12113 exec_runner.go:203] rm: /home/volkan/.minikube/ca.pem
I1114 20:40:28.917844   12113 exec_runner.go:151] cp: /home/volkan/.minikube/certs/ca.pem --> /home/volkan/.minikube/ca.pem (1078 bytes)
I1114 20:40:28.917887   12113 exec_runner.go:144] found /home/volkan/.minikube/cert.pem, removing ...
I1114 20:40:28.917889   12113 exec_runner.go:203] rm: /home/volkan/.minikube/cert.pem
I1114 20:40:28.917900   12113 exec_runner.go:151] cp: /home/volkan/.minikube/certs/cert.pem --> /home/volkan/.minikube/cert.pem (1119 bytes)
I1114 20:40:28.917925   12113 exec_runner.go:144] found /home/volkan/.minikube/key.pem, removing ...
I1114 20:40:28.917926   12113 exec_runner.go:203] rm: /home/volkan/.minikube/key.pem
I1114 20:40:28.917936   12113 exec_runner.go:151] cp: /home/volkan/.minikube/certs/key.pem --> /home/volkan/.minikube/key.pem (1675 bytes)
I1114 20:40:28.917953   12113 provision.go:112] generating server cert: /home/volkan/.minikube/machines/server.pem ca-key=/home/volkan/.minikube/certs/ca.pem private-key=/home/volkan/.minikube/certs/ca-key.pem org=volkan.minikube-m04 san=[192.168.49.5 127.0.0.1 localhost 127.0.0.1 minikube minikube-m04]
I1114 20:40:29.439349   12113 provision.go:172] copyRemoteCerts
I1114 20:40:29.439384   12113 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1114 20:40:29.439408   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:40:30.125888   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m04
I1114 20:40:30.807658   12113 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:35591 SSHKeyPath:/home/volkan/.minikube/machines/minikube-m04/id_rsa Username:docker}
I1114 20:40:31.411071   12113 ssh_runner.go:235] Completed: sudo mkdir -p /etc/docker /etc/docker /etc/docker: (1.971672884s)
I1114 20:40:31.411107   12113 ssh_runner.go:362] scp /home/volkan/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I1114 20:40:31.695732   12113 ssh_runner.go:362] scp /home/volkan/.minikube/machines/server.pem --> /etc/docker/server.pem (1212 bytes)
I1114 20:40:32.010953   12113 ssh_runner.go:362] scp /home/volkan/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1114 20:40:32.102568   12113 provision.go:86] duration metric: configureAuth took 4.511758821s
I1114 20:40:32.102581   12113 ubuntu.go:193] setting minikube options for container-runtime
I1114 20:40:32.102698   12113 config.go:182] Loaded profile config "minikube": Driver=podman, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1114 20:40:32.102729   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:40:32.431870   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m04
I1114 20:40:32.710729   12113 main.go:141] libmachine: Using SSH client type: native
I1114 20:40:32.710928   12113 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 35591 <nil> <nil>}
I1114 20:40:32.710931   12113 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1114 20:40:33.062514   12113 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1114 20:40:33.062524   12113 ubuntu.go:71] root file system type: overlay
I1114 20:40:33.062582   12113 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1114 20:40:33.062624   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:40:33.176265   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m04
I1114 20:40:33.359635   12113 main.go:141] libmachine: Using SSH client type: native
I1114 20:40:33.359817   12113 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 35591 <nil> <nil>}
I1114 20:40:33.359850   12113 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment="HTTP_PROXY=http://webproxy.deutsche-boerse.de:8080"
Environment="HTTPS_PROXY=http://webproxy.deutsche-boerse.de:8080"
Environment="NO_PROXY=localhost,127.0.0.1,.dbgcloud.io,.deutsche-boerse.de,172.23.65.1/24,192.168.58.0/24,192.168.59.0/24,192.168.39.0/24,192.168.49.0/24,10.96.0.0/12"


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=podman --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1114 20:40:33.591825   12113 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment=HTTP_PROXY=http://webproxy.deutsche-boerse.de:8080
Environment=HTTPS_PROXY=http://webproxy.deutsche-boerse.de:8080
Environment=NO_PROXY=localhost,127.0.0.1,.dbgcloud.io,.deutsche-boerse.de,172.23.65.1/24,192.168.58.0/24,192.168.59.0/24,192.168.39.0/24,192.168.49.0/24,10.96.0.0/12


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=podman --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1114 20:40:33.591870   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:40:33.705236   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m04
I1114 20:40:33.881608   12113 main.go:141] libmachine: Using SSH client type: native
I1114 20:40:33.881793   12113 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 35591 <nil> <nil>}
I1114 20:40:33.881800   12113 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1114 20:40:36.310336   12113 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2023-10-26 09:06:22.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2023-11-14 19:40:33.589125615 +0000
@@ -1,30 +1,35 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
-
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+Environment=HTTP_PROXY=http://webproxy.deutsche-boerse.de:8080
+Environment=HTTPS_PROXY=http://webproxy.deutsche-boerse.de:8080
+Environment=NO_PROXY=localhost,127.0.0.1,.dbgcloud.io,.deutsche-boerse.de,172.23.65.1/24,192.168.58.0/24,192.168.59.0/24,192.168.39.0/24,192.168.49.0/24,10.96.0.0/12
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=podman --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
@@ -32,16 +37,16 @@
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I1114 20:40:36.310349   12113 machine.go:91] provisioned docker machine in 18.463102717s
I1114 20:40:36.310353   12113 client.go:171] LocalClient.Create took 52.022088588s
I1114 20:40:36.310361   12113 start.go:167] duration metric: libmachine.API.Create for "minikube" took 52.02210872s
I1114 20:40:36.310364   12113 start.go:300] post-start starting for "minikube-m04" (driver="podman")
I1114 20:40:36.310369   12113 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1114 20:40:36.310403   12113 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1114 20:40:36.310423   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:40:36.626630   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m04
I1114 20:40:37.035829   12113 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:35591 SSHKeyPath:/home/volkan/.minikube/machines/minikube-m04/id_rsa Username:docker}
I1114 20:40:37.226117   12113 ssh_runner.go:195] Run: cat /etc/os-release
I1114 20:40:37.243750   12113 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1114 20:40:37.243766   12113 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1114 20:40:37.243770   12113 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1114 20:40:37.243774   12113 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I1114 20:40:37.243781   12113 filesync.go:126] Scanning /home/volkan/.minikube/addons for local assets ...
I1114 20:40:37.243813   12113 filesync.go:126] Scanning /home/volkan/.minikube/files for local assets ...
I1114 20:40:37.243821   12113 start.go:303] post-start completed in 933.4547ms
I1114 20:40:37.244541   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f {{.NetworkSettings.IPAddress}} minikube-m04
I1114 20:40:37.424773   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m04
I1114 20:40:37.564572   12113 profile.go:148] Saving config to /home/volkan/.minikube/profiles/minikube/config.json ...
I1114 20:40:37.564878   12113 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1114 20:40:37.564904   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:40:38.140627   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m04
I1114 20:40:38.630384   12113 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:35591 SSHKeyPath:/home/volkan/.minikube/machines/minikube-m04/id_rsa Username:docker}
I1114 20:40:38.914969   12113 ssh_runner.go:235] Completed: sh -c "df -h /var | awk 'NR==2{print $5}'": (1.350075722s)
I1114 20:40:38.915003   12113 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1114 20:40:38.981465   12113 start.go:128] duration metric: createHost completed in 54.696645968s
I1114 20:40:38.981475   12113 start.go:83] releasing machines lock for "minikube-m04", held for 54.696771289s
I1114 20:40:38.981518   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f {{.NetworkSettings.IPAddress}} minikube-m04
I1114 20:40:39.121142   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m04
I1114 20:40:39.304939   12113 out.go:177] * Found network options:
I1114 20:40:39.308184   12113 out.go:177]   - HTTP_PROXY=http://webproxy.deutsche-boerse.de:8080
W1114 20:40:39.311178   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.311187   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.311190   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.311194   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.311198   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.311201   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.311205   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.311208   12113 proxy.go:119] fail to check proxy env: Error ip not in block
I1114 20:40:39.314328   12113 out.go:177]   - HTTPS_PROXY=http://webproxy.deutsche-boerse.de:8080
W1114 20:40:39.316881   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.316889   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.316892   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.316895   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.316900   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.316903   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.316906   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.316909   12113 proxy.go:119] fail to check proxy env: Error ip not in block
I1114 20:40:39.319555   12113 out.go:177]   - NO_PROXY=localhost,127.0.0.1,.dbgcloud.io,.deutsche-boerse.de,172.23.65.1/24,192.168.58.0/24,192.168.59.0/24,192.168.39.0/24,192.168.49.0/24,10.96.0.0/12
W1114 20:40:39.373682   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.373700   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.373761   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.373765   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.373769   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.373773   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.373777   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.373780   12113 proxy.go:119] fail to check proxy env: Error ip not in block
I1114 20:40:39.376128   12113 out.go:177]   - HTTP_PROXY=http://webproxy.deutsche-boerse.de:8080
W1114 20:40:39.378773   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.378782   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.378785   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.378788   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.378793   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.378796   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.378799   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.378802   12113 proxy.go:119] fail to check proxy env: Error ip not in block
I1114 20:40:39.381224   12113 out.go:177]   - HTTPS_PROXY=http://webproxy.deutsche-boerse.de:8080
W1114 20:40:39.383940   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.383948   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.383952   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.383955   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.383959   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.383969   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.383972   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.383976   12113 proxy.go:119] fail to check proxy env: Error ip not in block
I1114 20:40:39.386774   12113 out.go:177]   - NO_PROXY=localhost,127.0.0.1,.dbgcloud.io,.deutsche-boerse.de,172.23.65.1/24,192.168.58.0/24,192.168.59.0/24,192.168.39.0/24,192.168.49.0/24,10.96.0.0/12
W1114 20:40:39.389482   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.389493   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.389496   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.389500   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.389504   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.389507   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.389510   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.389513   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.389548   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.389552   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.389555   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.389570   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.389573   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.389577   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.389580   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:40:39.389583   12113 proxy.go:119] fail to check proxy env: Error ip not in block
I1114 20:40:39.389617   12113 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1114 20:40:39.389638   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:40:39.390264   12113 ssh_runner.go:195] Run: curl -x http://webproxy.deutsche-boerse.de:8080 -sS -m 2 https://registry.k8s.io/
I1114 20:40:39.390291   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:40:39.673624   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m04
I1114 20:40:39.819883   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m04
I1114 20:40:40.144355   12113 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:35591 SSHKeyPath:/home/volkan/.minikube/machines/minikube-m04/id_rsa Username:docker}
I1114 20:40:40.182083   12113 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:35591 SSHKeyPath:/home/volkan/.minikube/machines/minikube-m04/id_rsa Username:docker}
I1114 20:40:40.285684   12113 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1114 20:41:28.502779   12113 ssh_runner.go:235] Completed: curl -x http://webproxy.deutsche-boerse.de:8080 -sS -m 2 https://registry.k8s.io/: (49.112501204s)
W1114 20:41:28.502794   12113 start.go:840] [curl -x http://webproxy.deutsche-boerse.de:8080 -sS -m 2 https://registry.k8s.io/] failed: curl -x http://webproxy.deutsche-boerse.de:8080 -sS -m 2 https://registry.k8s.io/: Process exited with status 28
stdout:

stderr:
curl: (28) Resolving timed out after 2000 milliseconds
W1114 20:41:28.502827   12113 out.go:239] ! This container is having trouble accessing https://registry.k8s.io
I1114 20:41:28.502851   12113 ssh_runner.go:235] Completed: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: (48.217150876s)
W1114 20:41:28.502893   12113 out.go:239] * To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I1114 20:41:28.502859   12113 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1114 20:41:28.502890   12113 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1114 20:41:28.593028   12113 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I1114 20:41:28.593040   12113 start.go:472] detecting cgroup driver to use...
I1114 20:41:28.593156   12113 detect.go:196] detected "cgroupfs" cgroup driver on host os
I1114 20:41:28.593207   12113 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1114 20:41:28.618465   12113 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I1114 20:41:28.634571   12113 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1114 20:41:28.836035   12113 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I1114 20:41:28.836069   12113 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1114 20:41:28.914194   12113 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1114 20:41:29.020735   12113 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1114 20:41:29.082756   12113 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1114 20:41:29.093725   12113 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1114 20:41:29.114508   12113 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1114 20:41:29.195586   12113 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1114 20:41:29.251580   12113 crio.go:148] couldn't verify netfilter by "sudo sysctl net.bridge.bridge-nf-call-iptables" which might be okay. error: sudo sysctl net.bridge.bridge-nf-call-iptables: Process exited with status 255
stdout:

stderr:
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory
I1114 20:41:29.251612   12113 ssh_runner.go:195] Run: sudo modprobe br_netfilter
I1114 20:41:29.320277   12113 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1114 20:41:29.368407   12113 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1114 20:41:29.780237   12113 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1114 20:41:30.590794   12113 start.go:472] detecting cgroup driver to use...
I1114 20:41:30.590821   12113 detect.go:196] detected "cgroupfs" cgroup driver on host os
I1114 20:41:30.590849   12113 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1114 20:41:30.682565   12113 cruntime.go:279] skipping containerd shutdown because we are bound to it
I1114 20:41:30.682684   12113 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1114 20:41:30.841549   12113 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1114 20:41:31.137332   12113 ssh_runner.go:195] Run: which cri-dockerd
I1114 20:41:31.208466   12113 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1114 20:41:31.317855   12113 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I1114 20:41:31.456392   12113 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1114 20:41:32.026291   12113 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1114 20:41:32.514793   12113 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I1114 20:41:32.514811   12113 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1114 20:41:32.606601   12113 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1114 20:41:33.196117   12113 ssh_runner.go:195] Run: sudo systemctl restart docker
I1114 20:41:35.898691   12113 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.702557687s)
I1114 20:41:35.898723   12113 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1114 20:41:36.102528   12113 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1114 20:41:36.309349   12113 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1114 20:41:36.579613   12113 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1114 20:41:36.881536   12113 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1114 20:41:36.898325   12113 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1114 20:41:37.215603   12113 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I1114 20:41:37.712833   12113 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1114 20:41:37.712864   12113 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1114 20:41:37.799104   12113 start.go:540] Will wait 60s for crictl version
I1114 20:41:37.799133   12113 ssh_runner.go:195] Run: which crictl
I1114 20:41:37.812353   12113 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1114 20:41:38.194539   12113 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I1114 20:41:38.194587   12113 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1114 20:41:38.318787   12113 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1114 20:41:38.605995   12113 out.go:204] * Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I1114 20:41:38.622212   12113 out.go:177]   - env HTTP_PROXY=http://webproxy.deutsche-boerse.de:8080
I1114 20:41:38.678763   12113 out.go:177]   - env HTTPS_PROXY=http://webproxy.deutsche-boerse.de:8080
I1114 20:41:38.701974   12113 out.go:177]   - env NO_PROXY=localhost,127.0.0.1,.dbgcloud.io,.deutsche-boerse.de,172.23.65.1/24,192.168.58.0/24,192.168.59.0/24,192.168.39.0/24,192.168.49.0/24,10.96.0.0/12
I1114 20:41:38.704693   12113 cli_runner.go:164] Run: sudo -n podman container inspect --format {{.NetworkSettings.Gateway}} minikube-m04
I1114 20:41:39.125471   12113 cli_runner.go:164] Run: sudo -n podman container inspect --format "
{{ if index .NetworkSettings.Networks "minikube-m04"}} 
	{{(index .NetworkSettings.Networks "minikube-m04").Gateway}}
{{ end }}
" minikube-m04
I1114 20:41:39.518950   12113 cli_runner.go:164] Run: sudo -n podman container inspect --format "
{{ if index .NetworkSettings.Networks "podman"}} 
	{{(index .NetworkSettings.Networks "podman").Gateway}}
{{ end }}
" minikube-m04
I1114 20:41:39.840830   12113 network.go:124] Couldn't find gateway for container minikube-m04
I1114 20:41:39.840976   12113 ssh_runner.go:195] Run: grep <nil>	host.minikube.internal$ /etc/hosts
I1114 20:41:39.847823   12113 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "<nil>	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1114 20:41:39.969579   12113 certs.go:56] Setting up /home/volkan/.minikube/profiles/minikube for IP: 192.168.49.5
I1114 20:41:39.969594   12113 certs.go:190] acquiring lock for shared ca certs: {Name:mkcf3862dec8fde9801070582903515da6d63ece Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1114 20:41:39.969695   12113 certs.go:199] skipping minikubeCA CA generation: /home/volkan/.minikube/ca.key
I1114 20:41:39.969724   12113 certs.go:199] skipping proxyClientCA CA generation: /home/volkan/.minikube/proxy-client-ca.key
I1114 20:41:39.969830   12113 certs.go:437] found cert: /home/volkan/.minikube/certs/home/volkan/.minikube/certs/ca-key.pem (1679 bytes)
I1114 20:41:39.969854   12113 certs.go:437] found cert: /home/volkan/.minikube/certs/home/volkan/.minikube/certs/ca.pem (1078 bytes)
I1114 20:41:39.969873   12113 certs.go:437] found cert: /home/volkan/.minikube/certs/home/volkan/.minikube/certs/cert.pem (1119 bytes)
I1114 20:41:39.969889   12113 certs.go:437] found cert: /home/volkan/.minikube/certs/home/volkan/.minikube/certs/key.pem (1675 bytes)
I1114 20:41:39.970162   12113 ssh_runner.go:362] scp /home/volkan/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1114 20:41:40.115839   12113 ssh_runner.go:362] scp /home/volkan/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1114 20:41:40.172313   12113 ssh_runner.go:362] scp /home/volkan/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1114 20:41:40.217987   12113 ssh_runner.go:362] scp /home/volkan/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1114 20:41:40.342352   12113 ssh_runner.go:362] scp /home/volkan/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1114 20:41:40.564607   12113 ssh_runner.go:195] Run: openssl version
I1114 20:41:40.596811   12113 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1114 20:41:40.708251   12113 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1114 20:41:40.712627   12113 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Nov 14 19:26 /usr/share/ca-certificates/minikubeCA.pem
I1114 20:41:40.712651   12113 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1114 20:41:40.721459   12113 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1114 20:41:40.788351   12113 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I1114 20:41:40.792671   12113 certs.go:353] certs directory doesn't exist, likely first start: ls /var/lib/minikube/certs/etcd: Process exited with status 2
stdout:

stderr:
ls: cannot access '/var/lib/minikube/certs/etcd': No such file or directory
I1114 20:41:40.792709   12113 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1114 20:41:41.113576   12113 cni.go:84] Creating CNI manager for ""
I1114 20:41:41.113582   12113 cni.go:136] 4 nodes found, recommending kindnet
I1114 20:41:41.113588   12113 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1114 20:41:41.113600   12113 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.5 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube-m04 DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.5 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1114 20:41:41.113661   12113 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.5
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube-m04"
  kubeletExtraArgs:
    node-ip: 192.168.49.5
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1114 20:41:41.113752   12113 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube-m04 --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.5

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1114 20:41:41.113784   12113 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I1114 20:41:41.147851   12113 binaries.go:44] Found k8s binaries, skipping transfer
I1114 20:41:41.147948   12113 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system
I1114 20:41:41.157718   12113 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (373 bytes)
I1114 20:41:41.177965   12113 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1114 20:41:41.194705   12113 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1114 20:41:41.206441   12113 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1114 20:41:41.219241   12113 host.go:66] Checking if "minikube" exists ...
I1114 20:41:41.219353   12113 start.go:304] JoinCluster: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:podman HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m03 IP:192.168.49.4 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m04 IP:192.168.49.5 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:false efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:false storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/volkan:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I1114 20:41:41.219402   12113 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm token create --print-join-command --ttl=0"
I1114 20:41:41.219426   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:41:41.219670   12113 config.go:182] Loaded profile config "minikube": Driver=podman, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1114 20:41:41.287087   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1114 20:41:41.352284   12113 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:40883 SSHKeyPath:/home/volkan/.minikube/machines/minikube/id_rsa Username:docker}
I1114 20:41:41.540599   12113 start.go:325] trying to join worker node "m04" to cluster: &{Name:m04 IP:192.168.49.5 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true}
I1114 20:41:41.540615   12113 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm join control-plane.minikube.internal:8443 --token 2778n0.sj4dvy1371f689rk --discovery-token-ca-cert-hash sha256:a2252d43c14054fc6733fec7a145908c7919f85fb780918953be2b01b36b46af --ignore-preflight-errors=all --cri-socket /var/run/cri-dockerd.sock --node-name=minikube-m04"
I1114 20:41:47.236580   12113 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm join control-plane.minikube.internal:8443 --token 2778n0.sj4dvy1371f689rk --discovery-token-ca-cert-hash sha256:a2252d43c14054fc6733fec7a145908c7919f85fb780918953be2b01b36b46af --ignore-preflight-errors=all --cri-socket /var/run/cri-dockerd.sock --node-name=minikube-m04": (5.695940576s)
I1114 20:41:47.236593   12113 ssh_runner.go:195] Run: /bin/bash -c "sudo systemctl daemon-reload && sudo systemctl enable kubelet && sudo systemctl start kubelet"
I1114 20:41:48.189012   12113 start.go:306] JoinCluster complete in 6.969654253s
I1114 20:41:48.189023   12113 cni.go:84] Creating CNI manager for ""
I1114 20:41:48.189025   12113 cni.go:136] 4 nodes found, recommending kindnet
I1114 20:41:48.189049   12113 ssh_runner.go:195] Run: stat /opt/cni/bin/portmap
I1114 20:41:48.341413   12113 cni.go:182] applying CNI manifest using /var/lib/minikube/binaries/v1.28.3/kubectl ...
I1114 20:41:48.341420   12113 ssh_runner.go:362] scp memory --> /var/tmp/minikube/cni.yaml (2438 bytes)
I1114 20:41:49.221319   12113 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.28.3/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml
I1114 20:41:51.769440   12113 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.28.3/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml: (2.548102299s)
I1114 20:41:51.850028   12113 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I1114 20:41:51.850046   12113 start.go:223] Will wait 6m0s for node &{Name:m04 IP:192.168.49.5 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true}
I1114 20:41:51.904407   12113 out.go:177] * Verifying Kubernetes components...
I1114 20:41:51.922780   12113 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1114 20:41:52.002651   12113 kubeadm.go:581] duration metric: took 152.587205ms to wait for : map[apiserver:true system_pods:true] ...
I1114 20:41:52.002662   12113 node_conditions.go:102] verifying NodePressure condition ...
I1114 20:41:52.007773   12113 node_conditions.go:122] node storage ephemeral capacity is 40302856Ki
I1114 20:41:52.007783   12113 node_conditions.go:123] node cpu capacity is 6
I1114 20:41:52.007788   12113 node_conditions.go:122] node storage ephemeral capacity is 40302856Ki
I1114 20:41:52.007789   12113 node_conditions.go:123] node cpu capacity is 6
I1114 20:41:52.007791   12113 node_conditions.go:122] node storage ephemeral capacity is 40302856Ki
I1114 20:41:52.007793   12113 node_conditions.go:123] node cpu capacity is 6
I1114 20:41:52.007794   12113 node_conditions.go:122] node storage ephemeral capacity is 40302856Ki
I1114 20:41:52.007795   12113 node_conditions.go:123] node cpu capacity is 6
I1114 20:41:52.007797   12113 node_conditions.go:105] duration metric: took 5.132834ms to run NodePressure ...
I1114 20:41:52.007803   12113 start.go:228] waiting for startup goroutines ...
I1114 20:41:52.007814   12113 start.go:242] writing updated cluster config ...
I1114 20:41:52.018148   12113 out.go:177] 
I1114 20:41:52.025031   12113 config.go:182] Loaded profile config "minikube": Driver=podman, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1114 20:41:52.025088   12113 profile.go:148] Saving config to /home/volkan/.minikube/profiles/minikube/config.json ...
I1114 20:41:52.085924   12113 out.go:177] * Starting worker node minikube-m05 in cluster minikube
I1114 20:41:52.112467   12113 cache.go:121] Beginning downloading kic base image for podman with docker
I1114 20:41:52.162482   12113 out.go:177] * Pulling base image ...
I1114 20:41:52.236615   12113 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I1114 20:41:52.236632   12113 cache.go:56] Caching tarball of preloaded images
I1114 20:41:52.236694   12113 preload.go:174] Found /home/volkan/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1114 20:41:52.236698   12113 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I1114 20:41:52.236764   12113 profile.go:148] Saving config to /home/volkan/.minikube/profiles/minikube/config.json ...
I1114 20:41:52.238439   12113 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 to local cache
I1114 20:41:52.238485   12113 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local cache directory
I1114 20:41:52.238491   12113 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local cache directory, skipping pull
I1114 20:41:52.238493   12113 image.go:105] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in cache, skipping pull
I1114 20:41:52.238497   12113 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 as a tarball
E1114 20:41:52.247677   12113 cache.go:189] Error downloading kic artifacts:  not yet implemented, see issue #8426
I1114 20:41:52.250867   12113 cache.go:194] Successfully downloaded all kic artifacts
I1114 20:41:52.250884   12113 start.go:365] acquiring machines lock for minikube-m05: {Name:mkfff1bbe8a5fc77ef3a5805edd0791af19cdd96 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1114 20:41:52.250931   12113 start.go:369] acquired machines lock for "minikube-m05" in 37.229µs
I1114 20:41:52.250941   12113 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:podman HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m03 IP:192.168.49.4 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m04 IP:192.168.49.5 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m05 IP: Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:false efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:false storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/volkan:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:} &{Name:m05 IP: Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true}
I1114 20:41:52.254173   12113 start.go:125] createHost starting for "m05" (driver="podman")
I1114 20:41:52.335846   12113 out.go:204] * Creating podman container (CPUs=2, Memory=2200MB) ...
I1114 20:41:52.342762   12113 start.go:159] libmachine.API.Create for "minikube" (driver="podman")
I1114 20:41:52.342778   12113 client.go:168] LocalClient.Create starting
I1114 20:41:52.348972   12113 main.go:141] libmachine: Reading certificate data from /home/volkan/.minikube/certs/ca.pem
I1114 20:41:52.348994   12113 main.go:141] libmachine: Decoding PEM data...
I1114 20:41:52.349001   12113 main.go:141] libmachine: Parsing certificate...
I1114 20:41:52.349033   12113 main.go:141] libmachine: Reading certificate data from /home/volkan/.minikube/certs/cert.pem
I1114 20:41:52.349039   12113 main.go:141] libmachine: Decoding PEM data...
I1114 20:41:52.349047   12113 main.go:141] libmachine: Parsing certificate...
I1114 20:41:52.349182   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:41:53.026857   12113 cli_runner.go:164] Run: sudo -n podman network inspect minikube --format "{{range .}}{{if eq .Driver "bridge"}}{{(index .Subnets 0).Subnet}},{{(index .Subnets 0).Gateway}}{{end}}{{end}}"
I1114 20:41:54.158614   12113 cli_runner.go:217] Completed: sudo -n podman network inspect minikube --format "{{range .}}{{if eq .Driver "bridge"}}{{(index .Subnets 0).Subnet}},{{(index .Subnets 0).Gateway}}{{end}}{{end}}": (1.131735286s)
I1114 20:41:54.158628   12113 network_create.go:77] Found existing network {name:minikube subnet:0xc003761770 gateway:[0 0 0 0 0 0 0 0 0 0 255 255 192 168 49 1] mtu:0}
I1114 20:41:54.158639   12113 kic.go:121] calculated static IP "192.168.49.6" for the "minikube-m05" container
I1114 20:41:54.158678   12113 cli_runner.go:164] Run: sudo -n podman ps -a --format {{.Names}}
I1114 20:41:54.968446   12113 cli_runner.go:164] Run: sudo -n podman volume create minikube-m05 --label name.minikube.sigs.k8s.io=minikube-m05 --label created_by.minikube.sigs.k8s.io=true
I1114 20:41:55.549253   12113 oci.go:103] Successfully created a podman volume minikube-m05
I1114 20:41:55.549308   12113 cli_runner.go:164] Run: sudo -n podman run --rm --name minikube-m05-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m05 --entrypoint /usr/bin/test -v minikube-m05:/var gcr.io/k8s-minikube/kicbase:v0.0.42 -d /var/lib
I1114 20:42:02.196952   12113 cli_runner.go:217] Completed: sudo -n podman run --rm --name minikube-m05-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m05 --entrypoint /usr/bin/test -v minikube-m05:/var gcr.io/k8s-minikube/kicbase:v0.0.42 -d /var/lib: (6.647614449s)
I1114 20:42:02.196966   12113 oci.go:107] Successfully prepared a podman volume minikube-m05
I1114 20:42:02.196980   12113 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I1114 20:42:02.196991   12113 kic.go:194] Starting extracting preloaded images to volume ...
I1114 20:42:02.197035   12113 cli_runner.go:164] Run: sudo -n podman run --rm --entrypoint /usr/bin/tar --security-opt label=disable -v /home/volkan/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube-m05:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.42 -I lz4 -xf /preloaded.tar -C /extractDir
I1114 20:42:22.314552   12113 cli_runner.go:217] Completed: sudo -n podman run --rm --entrypoint /usr/bin/tar --security-opt label=disable -v /home/volkan/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube-m05:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.42 -I lz4 -xf /preloaded.tar -C /extractDir: (20.117492585s)
I1114 20:42:22.314568   12113 kic.go:203] duration metric: took 20.117574 seconds to extract preloaded images to volume
W1114 20:42:22.314689   12113 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
I1114 20:42:22.314859   12113 cli_runner.go:164] Run: sudo -n podman info --format "'{{json .SecurityOptions}}'"
W1114 20:42:23.385955   12113 cli_runner.go:211] sudo -n podman info --format "'{{json .SecurityOptions}}'" returned with exit code 125
I1114 20:42:23.385966   12113 cli_runner.go:217] Completed: sudo -n podman info --format "'{{json .SecurityOptions}}'": (1.071074611s)
I1114 20:42:23.386028   12113 cli_runner.go:164] Run: sudo -n podman run --cgroup-manager cgroupfs -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube-m05 --name minikube-m05 --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m05 --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube-m05 --network minikube --ip 192.168.49.6 --volume minikube-m05:/var:exec --memory=2200mb --cpus=2 -e container=podman --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.42
I1114 20:42:27.601241   12113 cli_runner.go:217] Completed: sudo -n podman run --cgroup-manager cgroupfs -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube-m05 --name minikube-m05 --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m05 --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube-m05 --network minikube --ip 192.168.49.6 --volume minikube-m05:/var:exec --memory=2200mb --cpus=2 -e container=podman --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.42: (4.215183479s)
I1114 20:42:27.601286   12113 cli_runner.go:164] Run: sudo -n podman container inspect minikube-m05 --format={{.State.Running}}
I1114 20:42:28.271458   12113 cli_runner.go:164] Run: sudo -n podman container inspect minikube-m05 --format={{.State.Status}}
I1114 20:42:29.142317   12113 cli_runner.go:164] Run: sudo -n podman exec minikube-m05 stat /var/lib/dpkg/alternatives/iptables
I1114 20:42:30.617030   12113 cli_runner.go:217] Completed: sudo -n podman exec minikube-m05 stat /var/lib/dpkg/alternatives/iptables: (1.474690846s)
I1114 20:42:30.617041   12113 oci.go:144] the created container "minikube-m05" has a running status.
I1114 20:42:30.617050   12113 kic.go:225] Creating ssh key for kic: /home/volkan/.minikube/machines/minikube-m05/id_rsa...
I1114 20:42:31.137389   12113 kic_runner.go:191] podman (temp): /home/volkan/.minikube/machines/minikube-m05/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I1114 20:42:31.161525   12113 kic_runner.go:261] Run: /usr/bin/sudo -n podman cp /tmp/tmpf-memory-asset2481781394 minikube-m05:/home/docker/.ssh/authorized_keys
I1114 20:42:32.417576   12113 cli_runner.go:164] Run: sudo -n podman container inspect minikube-m05 --format={{.State.Status}}
I1114 20:42:32.569296   12113 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I1114 20:42:32.569309   12113 kic_runner.go:114] Args: [sudo -n podman exec --privileged minikube-m05 chown docker:docker /home/docker/.ssh/authorized_keys]
I1114 20:42:33.342104   12113 cli_runner.go:164] Run: sudo -n podman container inspect minikube-m05 --format={{.State.Status}}
I1114 20:42:33.966822   12113 machine.go:88] provisioning docker machine ...
I1114 20:42:33.966839   12113 ubuntu.go:169] provisioning hostname "minikube-m05"
I1114 20:42:33.966875   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:42:34.583836   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m05
I1114 20:42:35.112348   12113 main.go:141] libmachine: Using SSH client type: native
I1114 20:42:35.112567   12113 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 34313 <nil> <nil>}
I1114 20:42:35.112573   12113 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube-m05 && echo "minikube-m05" | sudo tee /etc/hostname
I1114 20:42:35.112998   12113 main.go:141] libmachine: Error dialing TCP: dial tcp 127.0.0.1:34313: connect: connection refused
I1114 20:42:38.117940   12113 main.go:141] libmachine: Error dialing TCP: dial tcp 127.0.0.1:34313: connect: connection refused
I1114 20:42:42.768724   12113 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube-m05

I1114 20:42:42.768772   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:42:43.379617   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m05
I1114 20:42:43.942887   12113 main.go:141] libmachine: Using SSH client type: native
I1114 20:42:43.943088   12113 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 34313 <nil> <nil>}
I1114 20:42:43.943096   12113 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube-m05' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube-m05/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube-m05' | sudo tee -a /etc/hosts; 
			fi
		fi
I1114 20:42:44.700448   12113 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1114 20:42:44.700462   12113 ubuntu.go:175] set auth options {CertDir:/home/volkan/.minikube CaCertPath:/home/volkan/.minikube/certs/ca.pem CaPrivateKeyPath:/home/volkan/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/volkan/.minikube/machines/server.pem ServerKeyPath:/home/volkan/.minikube/machines/server-key.pem ClientKeyPath:/home/volkan/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/volkan/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/volkan/.minikube}
I1114 20:42:44.700471   12113 ubuntu.go:177] setting up certificates
I1114 20:42:44.700476   12113 provision.go:83] configureAuth start
I1114 20:42:44.700516   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f {{.NetworkSettings.IPAddress}} minikube-m05
I1114 20:42:45.515866   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m05
I1114 20:42:46.250545   12113 provision.go:138] copyHostCerts
I1114 20:42:46.250574   12113 exec_runner.go:144] found /home/volkan/.minikube/ca.pem, removing ...
I1114 20:42:46.250578   12113 exec_runner.go:203] rm: /home/volkan/.minikube/ca.pem
I1114 20:42:46.250619   12113 exec_runner.go:151] cp: /home/volkan/.minikube/certs/ca.pem --> /home/volkan/.minikube/ca.pem (1078 bytes)
I1114 20:42:46.250661   12113 exec_runner.go:144] found /home/volkan/.minikube/cert.pem, removing ...
I1114 20:42:46.250662   12113 exec_runner.go:203] rm: /home/volkan/.minikube/cert.pem
I1114 20:42:46.250673   12113 exec_runner.go:151] cp: /home/volkan/.minikube/certs/cert.pem --> /home/volkan/.minikube/cert.pem (1119 bytes)
I1114 20:42:46.250692   12113 exec_runner.go:144] found /home/volkan/.minikube/key.pem, removing ...
I1114 20:42:46.250694   12113 exec_runner.go:203] rm: /home/volkan/.minikube/key.pem
I1114 20:42:46.250702   12113 exec_runner.go:151] cp: /home/volkan/.minikube/certs/key.pem --> /home/volkan/.minikube/key.pem (1675 bytes)
I1114 20:42:46.250719   12113 provision.go:112] generating server cert: /home/volkan/.minikube/machines/server.pem ca-key=/home/volkan/.minikube/certs/ca.pem private-key=/home/volkan/.minikube/certs/ca-key.pem org=volkan.minikube-m05 san=[192.168.49.6 127.0.0.1 localhost 127.0.0.1 minikube minikube-m05]
I1114 20:42:46.559733   12113 provision.go:172] copyRemoteCerts
I1114 20:42:46.559767   12113 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1114 20:42:46.559789   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:42:47.356095   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m05
I1114 20:42:47.927635   12113 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:34313 SSHKeyPath:/home/volkan/.minikube/machines/minikube-m05/id_rsa Username:docker}
I1114 20:42:48.278548   12113 ssh_runner.go:235] Completed: sudo mkdir -p /etc/docker /etc/docker /etc/docker: (1.716107701s)
I1114 20:42:48.278600   12113 ssh_runner.go:362] scp /home/volkan/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1114 20:42:48.716835   12113 ssh_runner.go:362] scp /home/volkan/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I1114 20:42:48.834626   12113 ssh_runner.go:362] scp /home/volkan/.minikube/machines/server.pem --> /etc/docker/server.pem (1212 bytes)
I1114 20:42:49.110525   12113 provision.go:86] duration metric: configureAuth took 4.410040012s
I1114 20:42:49.110537   12113 ubuntu.go:193] setting minikube options for container-runtime
I1114 20:42:49.110678   12113 config.go:182] Loaded profile config "minikube": Driver=podman, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1114 20:42:49.110710   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:42:49.768681   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m05
I1114 20:42:50.533765   12113 main.go:141] libmachine: Using SSH client type: native
I1114 20:42:50.533965   12113 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 34313 <nil> <nil>}
I1114 20:42:50.533969   12113 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1114 20:42:50.960423   12113 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1114 20:42:50.960432   12113 ubuntu.go:71] root file system type: overlay
I1114 20:42:50.960497   12113 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1114 20:42:50.960534   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:42:51.408725   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m05
I1114 20:42:51.633018   12113 main.go:141] libmachine: Using SSH client type: native
I1114 20:42:51.633223   12113 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 34313 <nil> <nil>}
I1114 20:42:51.633255   12113 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment="HTTP_PROXY=http://webproxy.deutsche-boerse.de:8080"
Environment="HTTPS_PROXY=http://webproxy.deutsche-boerse.de:8080"
Environment="NO_PROXY=localhost,127.0.0.1,.dbgcloud.io,.deutsche-boerse.de,172.23.65.1/24,192.168.58.0/24,192.168.59.0/24,192.168.39.0/24,192.168.49.0/24,10.96.0.0/12"


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=podman --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1114 20:42:52.134697   12113 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment=HTTP_PROXY=http://webproxy.deutsche-boerse.de:8080
Environment=HTTPS_PROXY=http://webproxy.deutsche-boerse.de:8080
Environment=NO_PROXY=localhost,127.0.0.1,.dbgcloud.io,.deutsche-boerse.de,172.23.65.1/24,192.168.58.0/24,192.168.59.0/24,192.168.39.0/24,192.168.49.0/24,10.96.0.0/12


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=podman --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1114 20:42:52.134746   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:42:52.705933   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m05
I1114 20:42:53.261807   12113 main.go:141] libmachine: Using SSH client type: native
I1114 20:42:53.264351   12113 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 34313 <nil> <nil>}
I1114 20:42:53.264360   12113 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1114 20:42:57.121830   12113 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2023-10-26 09:06:22.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2023-11-14 19:42:52.071854454 +0000
@@ -1,30 +1,35 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
-
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+Environment=HTTP_PROXY=http://webproxy.deutsche-boerse.de:8080
+Environment=HTTPS_PROXY=http://webproxy.deutsche-boerse.de:8080
+Environment=NO_PROXY=localhost,127.0.0.1,.dbgcloud.io,.deutsche-boerse.de,172.23.65.1/24,192.168.58.0/24,192.168.59.0/24,192.168.39.0/24,192.168.49.0/24,10.96.0.0/12
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=podman --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
@@ -32,16 +37,16 @@
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I1114 20:42:57.121906   12113 machine.go:91] provisioned docker machine in 23.155073843s
I1114 20:42:57.121911   12113 client.go:171] LocalClient.Create took 1m4.779130232s
I1114 20:42:57.121918   12113 start.go:167] duration metric: libmachine.API.Create for "minikube" took 1m4.779160259s
I1114 20:42:57.122298   12113 start.go:300] post-start starting for "minikube-m05" (driver="podman")
I1114 20:42:57.122305   12113 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1114 20:42:57.122338   12113 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1114 20:42:57.122359   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:42:57.513762   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m05
I1114 20:42:57.920493   12113 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:34313 SSHKeyPath:/home/volkan/.minikube/machines/minikube-m05/id_rsa Username:docker}
I1114 20:42:58.279670   12113 ssh_runner.go:235] Completed: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs: (1.157312789s)
I1114 20:42:58.279702   12113 ssh_runner.go:195] Run: cat /etc/os-release
I1114 20:42:58.288526   12113 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1114 20:42:58.288540   12113 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1114 20:42:58.288544   12113 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1114 20:42:58.288547   12113 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I1114 20:42:58.288554   12113 filesync.go:126] Scanning /home/volkan/.minikube/addons for local assets ...
I1114 20:42:58.288584   12113 filesync.go:126] Scanning /home/volkan/.minikube/files for local assets ...
I1114 20:42:58.288594   12113 start.go:303] post-start completed in 1.166290576s
I1114 20:42:58.295685   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f {{.NetworkSettings.IPAddress}} minikube-m05
I1114 20:42:59.058982   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m05
I1114 20:42:59.462815   12113 profile.go:148] Saving config to /home/volkan/.minikube/profiles/minikube/config.json ...
I1114 20:42:59.466116   12113 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1114 20:42:59.466140   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:42:59.832145   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m05
I1114 20:43:00.421875   12113 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:34313 SSHKeyPath:/home/volkan/.minikube/machines/minikube-m05/id_rsa Username:docker}
I1114 20:43:00.897012   12113 ssh_runner.go:235] Completed: sh -c "df -h /var | awk 'NR==2{print $5}'": (1.430879679s)
I1114 20:43:00.897045   12113 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1114 20:43:01.088130   12113 start.go:128] duration metric: createHost completed in 1m8.832359929s
I1114 20:43:01.088149   12113 start.go:83] releasing machines lock for "minikube-m05", held for 1m8.837210392s
I1114 20:43:01.088214   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f {{.NetworkSettings.IPAddress}} minikube-m05
I1114 20:43:01.781266   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m05
I1114 20:43:01.939203   12113 out.go:177] * Found network options:
I1114 20:43:01.957771   12113 out.go:177]   - HTTP_PROXY=http://webproxy.deutsche-boerse.de:8080
W1114 20:43:01.960854   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:01.960868   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:01.960874   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:01.960880   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:01.960888   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:01.960894   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:01.960900   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:01.960907   12113 proxy.go:119] fail to check proxy env: Error ip not in block
I1114 20:43:01.963811   12113 out.go:177]   - HTTPS_PROXY=http://webproxy.deutsche-boerse.de:8080
W1114 20:43:02.000719   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.000732   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.000736   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.000740   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.000745   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.000748   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.000753   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.000757   12113 proxy.go:119] fail to check proxy env: Error ip not in block
I1114 20:43:02.018539   12113 out.go:177]   - NO_PROXY=localhost,127.0.0.1,.dbgcloud.io,.deutsche-boerse.de,172.23.65.1/24,192.168.58.0/24,192.168.59.0/24,192.168.39.0/24,192.168.49.0/24,10.96.0.0/12
W1114 20:43:02.022019   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.022032   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.022036   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.022039   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.022044   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.022047   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.022051   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.022054   12113 proxy.go:119] fail to check proxy env: Error ip not in block
I1114 20:43:02.025657   12113 out.go:177]   - HTTP_PROXY=http://webproxy.deutsche-boerse.de:8080
W1114 20:43:02.029495   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.029508   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.029512   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.029519   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.029524   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.029527   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.029531   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.029534   12113 proxy.go:119] fail to check proxy env: Error ip not in block
I1114 20:43:02.031970   12113 out.go:177]   - HTTPS_PROXY=http://webproxy.deutsche-boerse.de:8080
W1114 20:43:02.042778   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.042790   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.042794   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.042797   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.042802   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.042805   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.042809   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.042812   12113 proxy.go:119] fail to check proxy env: Error ip not in block
I1114 20:43:02.084557   12113 out.go:177]   - NO_PROXY=localhost,127.0.0.1,.dbgcloud.io,.deutsche-boerse.de,172.23.65.1/24,192.168.58.0/24,192.168.59.0/24,192.168.39.0/24,192.168.49.0/24,10.96.0.0/12
W1114 20:43:02.105497   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.105510   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.105513   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.105517   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.105522   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.105525   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.105531   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.105534   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.105661   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.105665   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.105671   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.105675   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.105678   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.105682   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.105685   12113 proxy.go:119] fail to check proxy env: Error ip not in block
W1114 20:43:02.105689   12113 proxy.go:119] fail to check proxy env: Error ip not in block
I1114 20:43:02.105727   12113 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1114 20:43:02.105752   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:43:02.106034   12113 ssh_runner.go:195] Run: curl -x http://webproxy.deutsche-boerse.de:8080 -sS -m 2 https://registry.k8s.io/
I1114 20:43:02.106167   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:43:02.515534   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m05
I1114 20:43:02.630195   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m05
I1114 20:43:03.206378   12113 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:34313 SSHKeyPath:/home/volkan/.minikube/machines/minikube-m05/id_rsa Username:docker}
I1114 20:43:03.385980   12113 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:34313 SSHKeyPath:/home/volkan/.minikube/machines/minikube-m05/id_rsa Username:docker}
I1114 20:43:51.800220   12113 ssh_runner.go:235] Completed: curl -x http://webproxy.deutsche-boerse.de:8080 -sS -m 2 https://registry.k8s.io/: (49.694170553s)
W1114 20:43:51.800237   12113 start.go:840] [curl -x http://webproxy.deutsche-boerse.de:8080 -sS -m 2 https://registry.k8s.io/] failed: curl -x http://webproxy.deutsche-boerse.de:8080 -sS -m 2 https://registry.k8s.io/: Process exited with status 28
stdout:

stderr:
curl: (28) Resolving timed out after 2000 milliseconds
W1114 20:43:51.800284   12113 out.go:239] ! This container is having trouble accessing https://registry.k8s.io
W1114 20:43:51.800325   12113 out.go:239] * To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I1114 20:43:51.800542   12113 ssh_runner.go:235] Completed: sh -c "stat /etc/cni/net.d/*loopback.conf*": (49.694808123s)
I1114 20:43:51.800573   12113 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1114 20:43:52.053164   12113 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1114 20:43:52.054556   12113 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1114 20:43:52.284341   12113 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I1114 20:43:52.284353   12113 start.go:472] detecting cgroup driver to use...
I1114 20:43:52.284374   12113 detect.go:196] detected "cgroupfs" cgroup driver on host os
I1114 20:43:52.284430   12113 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1114 20:43:52.395987   12113 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I1114 20:43:52.461525   12113 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1114 20:43:52.539616   12113 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I1114 20:43:52.539651   12113 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1114 20:43:52.720804   12113 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1114 20:43:52.780464   12113 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1114 20:43:52.874189   12113 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1114 20:43:52.892525   12113 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1114 20:43:52.942613   12113 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1114 20:43:53.099681   12113 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1114 20:43:53.233508   12113 crio.go:148] couldn't verify netfilter by "sudo sysctl net.bridge.bridge-nf-call-iptables" which might be okay. error: sudo sysctl net.bridge.bridge-nf-call-iptables: Process exited with status 255
stdout:

stderr:
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory
I1114 20:43:53.233537   12113 ssh_runner.go:195] Run: sudo modprobe br_netfilter
I1114 20:43:53.349376   12113 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1114 20:43:53.392677   12113 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1114 20:43:53.905820   12113 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1114 20:43:54.536905   12113 start.go:472] detecting cgroup driver to use...
I1114 20:43:54.536928   12113 detect.go:196] detected "cgroupfs" cgroup driver on host os
I1114 20:43:54.536952   12113 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1114 20:43:54.697355   12113 cruntime.go:279] skipping containerd shutdown because we are bound to it
I1114 20:43:54.697389   12113 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1114 20:43:54.772153   12113 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1114 20:43:54.903257   12113 ssh_runner.go:195] Run: which cri-dockerd
I1114 20:43:54.954870   12113 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1114 20:43:55.146351   12113 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I1114 20:43:55.552612   12113 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1114 20:43:56.083446   12113 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1114 20:43:56.611671   12113 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I1114 20:43:56.611698   12113 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1114 20:43:56.855258   12113 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1114 20:43:57.555873   12113 ssh_runner.go:195] Run: sudo systemctl restart docker
I1114 20:44:03.911506   12113 ssh_runner.go:235] Completed: sudo systemctl restart docker: (6.355612698s)
I1114 20:44:03.911541   12113 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1114 20:44:04.306084   12113 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1114 20:44:04.718495   12113 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1114 20:44:04.798035   12113 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1114 20:44:05.001402   12113 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1114 20:44:05.111590   12113 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1114 20:44:05.565590   12113 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I1114 20:44:06.676993   12113 ssh_runner.go:235] Completed: sudo systemctl restart cri-docker: (1.11138711s)
I1114 20:44:06.677004   12113 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1114 20:44:06.679502   12113 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1114 20:44:06.730667   12113 start.go:540] Will wait 60s for crictl version
I1114 20:44:06.730697   12113 ssh_runner.go:195] Run: which crictl
I1114 20:44:06.787236   12113 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1114 20:44:07.314855   12113 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I1114 20:44:07.314973   12113 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1114 20:44:07.452614   12113 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1114 20:44:07.621366   12113 out.go:204] * Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I1114 20:44:07.684134   12113 out.go:177]   - env HTTP_PROXY=http://webproxy.deutsche-boerse.de:8080
I1114 20:44:07.776197   12113 out.go:177]   - env HTTPS_PROXY=http://webproxy.deutsche-boerse.de:8080
I1114 20:44:07.787651   12113 out.go:177]   - env NO_PROXY=localhost,127.0.0.1,.dbgcloud.io,.deutsche-boerse.de,172.23.65.1/24,192.168.58.0/24,192.168.59.0/24,192.168.39.0/24,192.168.49.0/24,10.96.0.0/12
I1114 20:44:07.790423   12113 cli_runner.go:164] Run: sudo -n podman container inspect --format {{.NetworkSettings.Gateway}} minikube-m05
I1114 20:44:08.142833   12113 cli_runner.go:164] Run: sudo -n podman container inspect --format "
{{ if index .NetworkSettings.Networks "minikube-m05"}} 
	{{(index .NetworkSettings.Networks "minikube-m05").Gateway}}
{{ end }}
" minikube-m05
I1114 20:44:08.908292   12113 cli_runner.go:164] Run: sudo -n podman container inspect --format "
{{ if index .NetworkSettings.Networks "podman"}} 
	{{(index .NetworkSettings.Networks "podman").Gateway}}
{{ end }}
" minikube-m05
I1114 20:44:09.395565   12113 network.go:124] Couldn't find gateway for container minikube-m05
I1114 20:44:09.395606   12113 ssh_runner.go:195] Run: grep <nil>	host.minikube.internal$ /etc/hosts
I1114 20:44:09.422597   12113 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "<nil>	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1114 20:44:09.565624   12113 certs.go:56] Setting up /home/volkan/.minikube/profiles/minikube for IP: 192.168.49.6
I1114 20:44:09.566294   12113 certs.go:190] acquiring lock for shared ca certs: {Name:mkcf3862dec8fde9801070582903515da6d63ece Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1114 20:44:09.566529   12113 certs.go:199] skipping minikubeCA CA generation: /home/volkan/.minikube/ca.key
I1114 20:44:09.566544   12113 certs.go:199] skipping proxyClientCA CA generation: /home/volkan/.minikube/proxy-client-ca.key
I1114 20:44:09.566581   12113 certs.go:437] found cert: /home/volkan/.minikube/certs/home/volkan/.minikube/certs/ca-key.pem (1679 bytes)
I1114 20:44:09.566593   12113 certs.go:437] found cert: /home/volkan/.minikube/certs/home/volkan/.minikube/certs/ca.pem (1078 bytes)
I1114 20:44:09.566603   12113 certs.go:437] found cert: /home/volkan/.minikube/certs/home/volkan/.minikube/certs/cert.pem (1119 bytes)
I1114 20:44:09.566611   12113 certs.go:437] found cert: /home/volkan/.minikube/certs/home/volkan/.minikube/certs/key.pem (1675 bytes)
I1114 20:44:09.566814   12113 ssh_runner.go:362] scp /home/volkan/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1114 20:44:09.679337   12113 ssh_runner.go:362] scp /home/volkan/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1114 20:44:09.872720   12113 ssh_runner.go:362] scp /home/volkan/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1114 20:44:09.961112   12113 ssh_runner.go:362] scp /home/volkan/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1114 20:44:10.005380   12113 ssh_runner.go:362] scp /home/volkan/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1114 20:44:10.037617   12113 ssh_runner.go:195] Run: openssl version
I1114 20:44:10.043731   12113 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1114 20:44:10.076431   12113 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1114 20:44:10.081181   12113 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Nov 14 19:26 /usr/share/ca-certificates/minikubeCA.pem
I1114 20:44:10.081205   12113 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1114 20:44:10.087503   12113 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1114 20:44:10.124339   12113 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I1114 20:44:10.129403   12113 certs.go:353] certs directory doesn't exist, likely first start: ls /var/lib/minikube/certs/etcd: Process exited with status 2
stdout:

stderr:
ls: cannot access '/var/lib/minikube/certs/etcd': No such file or directory
I1114 20:44:10.129450   12113 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1114 20:44:10.326510   12113 cni.go:84] Creating CNI manager for ""
I1114 20:44:10.326522   12113 cni.go:136] 5 nodes found, recommending kindnet
I1114 20:44:10.326528   12113 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1114 20:44:10.326542   12113 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.6 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube-m05 DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.6 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1114 20:44:10.326610   12113 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.6
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube-m05"
  kubeletExtraArgs:
    node-ip: 192.168.49.6
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1114 20:44:10.326639   12113 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube-m05 --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.6

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1114 20:44:10.326671   12113 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I1114 20:44:10.373406   12113 binaries.go:44] Found k8s binaries, skipping transfer
I1114 20:44:10.373438   12113 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system
I1114 20:44:10.444074   12113 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (373 bytes)
I1114 20:44:10.481070   12113 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1114 20:44:10.505879   12113 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1114 20:44:10.513251   12113 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1114 20:44:10.525531   12113 host.go:66] Checking if "minikube" exists ...
I1114 20:44:10.525659   12113 start.go:304] JoinCluster: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:podman HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m03 IP:192.168.49.4 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m04 IP:192.168.49.5 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m05 IP:192.168.49.6 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:false efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:false storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/volkan:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I1114 20:44:10.525717   12113 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm token create --print-join-command --ttl=0"
I1114 20:44:10.525741   12113 cli_runner.go:164] Run: sudo -n podman version --format {{.Version}}
I1114 20:44:10.526212   12113 config.go:182] Loaded profile config "minikube": Driver=podman, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1114 20:44:11.116313   12113 cli_runner.go:164] Run: sudo -n podman container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1114 20:44:11.460750   12113 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:40883 SSHKeyPath:/home/volkan/.minikube/machines/minikube/id_rsa Username:docker}
I1114 20:44:13.130337   12113 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm token create --print-join-command --ttl=0": (2.604607164s)
I1114 20:44:13.130362   12113 start.go:325] trying to join worker node "m05" to cluster: &{Name:m05 IP:192.168.49.6 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true}
I1114 20:44:13.130377   12113 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm join control-plane.minikube.internal:8443 --token svsjzv.1ynme4yi98w6uqcb --discovery-token-ca-cert-hash sha256:a2252d43c14054fc6733fec7a145908c7919f85fb780918953be2b01b36b46af --ignore-preflight-errors=all --cri-socket /var/run/cri-dockerd.sock --node-name=minikube-m05"
I1114 20:44:19.925944   12113 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm join control-plane.minikube.internal:8443 --token svsjzv.1ynme4yi98w6uqcb --discovery-token-ca-cert-hash sha256:a2252d43c14054fc6733fec7a145908c7919f85fb780918953be2b01b36b46af --ignore-preflight-errors=all --cri-socket /var/run/cri-dockerd.sock --node-name=minikube-m05": (6.795550328s)
I1114 20:44:19.925959   12113 ssh_runner.go:195] Run: /bin/bash -c "sudo systemctl daemon-reload && sudo systemctl enable kubelet && sudo systemctl start kubelet"
I1114 20:44:22.084684   12113 ssh_runner.go:235] Completed: /bin/bash -c "sudo systemctl daemon-reload && sudo systemctl enable kubelet && sudo systemctl start kubelet": (2.15870916s)
I1114 20:44:22.084697   12113 start.go:306] JoinCluster complete in 11.5590361s
I1114 20:44:22.084713   12113 cni.go:84] Creating CNI manager for ""
I1114 20:44:22.084723   12113 cni.go:136] 5 nodes found, recommending kindnet
I1114 20:44:22.084759   12113 ssh_runner.go:195] Run: stat /opt/cni/bin/portmap
I1114 20:44:22.176012   12113 cni.go:182] applying CNI manifest using /var/lib/minikube/binaries/v1.28.3/kubectl ...
I1114 20:44:22.176021   12113 ssh_runner.go:362] scp memory --> /var/tmp/minikube/cni.yaml (2438 bytes)
I1114 20:44:22.542030   12113 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.28.3/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml
I1114 20:44:26.255779   12113 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.28.3/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml: (3.713730988s)
I1114 20:44:26.350657   12113 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I1114 20:44:26.350675   12113 start.go:223] Will wait 6m0s for node &{Name:m05 IP:192.168.49.6 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true}
I1114 20:44:26.364236   12113 out.go:177] * Verifying Kubernetes components...
I1114 20:44:26.367642   12113 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1114 20:44:26.659959   12113 kubeadm.go:581] duration metric: took 309.268158ms to wait for : map[apiserver:true system_pods:true] ...
I1114 20:44:26.659971   12113 node_conditions.go:102] verifying NodePressure condition ...
I1114 20:44:26.729793   12113 node_conditions.go:122] node storage ephemeral capacity is 40302856Ki
I1114 20:44:26.729804   12113 node_conditions.go:123] node cpu capacity is 6
I1114 20:44:26.729810   12113 node_conditions.go:122] node storage ephemeral capacity is 40302856Ki
I1114 20:44:26.729812   12113 node_conditions.go:123] node cpu capacity is 6
I1114 20:44:26.729813   12113 node_conditions.go:122] node storage ephemeral capacity is 40302856Ki
I1114 20:44:26.729814   12113 node_conditions.go:123] node cpu capacity is 6
I1114 20:44:26.729816   12113 node_conditions.go:122] node storage ephemeral capacity is 40302856Ki
I1114 20:44:26.729818   12113 node_conditions.go:123] node cpu capacity is 6
I1114 20:44:26.729819   12113 node_conditions.go:122] node storage ephemeral capacity is 40302856Ki
I1114 20:44:26.729820   12113 node_conditions.go:123] node cpu capacity is 6
I1114 20:44:26.729822   12113 node_conditions.go:105] duration metric: took 69.848642ms to run NodePressure ...
I1114 20:44:26.729829   12113 start.go:228] waiting for startup goroutines ...
I1114 20:44:26.729840   12113 start.go:242] writing updated cluster config ...
I1114 20:44:26.730008   12113 ssh_runner.go:195] Run: rm -f paused
I1114 20:44:27.572658   12113 start.go:600] kubectl: 1.28.2, cluster: 1.28.3 (minor skew: 0)
I1114 20:44:27.631797   12113 out.go:177] * Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Nov 14 20:09:18 minikube cri-dockerd[1339]: time="2023-11-14T20:09:18Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-p4tj4_kube-system\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Nov 14 20:09:20 minikube dockerd[1125]: time="2023-11-14T20:09:20.744171729Z" level=info msg="ignoring event" container=5c37d2a14d205b52b33e35e8ab35cfa3bf83804fb1b19a992989efe29cbf1230 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 14 20:09:21 minikube dockerd[1125]: time="2023-11-14T20:09:21.238528919Z" level=info msg="ignoring event" container=11e92391b04b41ced8dc987ea8ddd88e1fda826419bfcb86b3e3d09981545efd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 14 20:09:26 minikube cri-dockerd[1339]: time="2023-11-14T20:09:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/45b8242c69f8e88fcfbd2b30404b1b209006d6826630ea94b7a8c11f4c948016/resolv.conf as [nameserver 172.23.253.222 nameserver 172.23.201.222 nameserver 10.0.2.3 search deutsche-boerse.de rdev.deutsche-boerse.de dbgcloud.io localdomain myguest.virtualbox.org]"
Nov 14 20:09:26 minikube cri-dockerd[1339]: time="2023-11-14T20:09:26Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/21565554146b35a15ddad9c6e6027636d9f995a23bcde98a5e296384d672b298/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local deutsche-boerse.de rdev.deutsche-boerse.de dbgcloud.io localdomain myguest.virtualbox.org options ndots:5]"
Nov 14 20:09:26 minikube cri-dockerd[1339]: time="2023-11-14T20:09:26Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-p4tj4_kube-system\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Nov 14 20:09:27 minikube cri-dockerd[1339]: time="2023-11-14T20:09:27Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"ingress-nginx-admission-create-429zk_ingress-nginx\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Nov 14 20:09:27 minikube cri-dockerd[1339]: time="2023-11-14T20:09:27Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-p4tj4_kube-system\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Nov 14 20:09:29 minikube cri-dockerd[1339]: time="2023-11-14T20:09:29Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"ingress-nginx-admission-create-429zk_ingress-nginx\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Nov 14 20:09:31 minikube dockerd[1125]: time="2023-11-14T20:09:31.052017398Z" level=info msg="ignoring event" container=45b8242c69f8e88fcfbd2b30404b1b209006d6826630ea94b7a8c11f4c948016 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 14 20:09:33 minikube dockerd[1125]: time="2023-11-14T20:09:33.522450643Z" level=info msg="ignoring event" container=21565554146b35a15ddad9c6e6027636d9f995a23bcde98a5e296384d672b298 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 14 20:09:37 minikube cri-dockerd[1339]: time="2023-11-14T20:09:37Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c9bf67437178bfbfd72d0d0e4ca1c677674a3a0e6a5fc655bd30c2c14e9d731a/resolv.conf as [nameserver 172.23.253.222 nameserver 172.23.201.222 nameserver 10.0.2.3 search deutsche-boerse.de rdev.deutsche-boerse.de dbgcloud.io localdomain myguest.virtualbox.org]"
Nov 14 20:09:38 minikube cri-dockerd[1339]: time="2023-11-14T20:09:38Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-p4tj4_kube-system\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Nov 14 20:09:38 minikube cri-dockerd[1339]: time="2023-11-14T20:09:38Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b94b2665dd8c837e6befa6a4d18e529ac2724ab1e897274974545359f3985010/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local deutsche-boerse.de rdev.deutsche-boerse.de dbgcloud.io localdomain myguest.virtualbox.org options ndots:5]"
Nov 14 20:09:39 minikube cri-dockerd[1339]: time="2023-11-14T20:09:39Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"ingress-nginx-admission-create-429zk_ingress-nginx\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Nov 14 20:09:40 minikube cri-dockerd[1339]: time="2023-11-14T20:09:40Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-p4tj4_kube-system\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Nov 14 20:09:41 minikube cri-dockerd[1339]: time="2023-11-14T20:09:41Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"ingress-nginx-admission-create-429zk_ingress-nginx\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Nov 14 20:09:42 minikube dockerd[1125]: time="2023-11-14T20:09:42.322548819Z" level=info msg="ignoring event" container=c9bf67437178bfbfd72d0d0e4ca1c677674a3a0e6a5fc655bd30c2c14e9d731a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 14 20:09:43 minikube dockerd[1125]: time="2023-11-14T20:09:43.341650175Z" level=info msg="ignoring event" container=b94b2665dd8c837e6befa6a4d18e529ac2724ab1e897274974545359f3985010 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 14 20:09:50 minikube cri-dockerd[1339]: time="2023-11-14T20:09:50Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/032456491fa1342f61e9e7c33f3ef1caa5f4c070a96801ab0ac8a0b2a67ae98d/resolv.conf as [nameserver 172.23.253.222 nameserver 172.23.201.222 nameserver 10.0.2.3 search deutsche-boerse.de rdev.deutsche-boerse.de dbgcloud.io localdomain myguest.virtualbox.org]"
Nov 14 20:09:50 minikube cri-dockerd[1339]: time="2023-11-14T20:09:50Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/959cde08f68045201a43a76de907b026fa71202508a967eee7273f9336bfd40a/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local deutsche-boerse.de rdev.deutsche-boerse.de dbgcloud.io localdomain myguest.virtualbox.org options ndots:5]"
Nov 14 20:09:50 minikube cri-dockerd[1339]: time="2023-11-14T20:09:50Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-p4tj4_kube-system\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Nov 14 20:09:50 minikube cri-dockerd[1339]: time="2023-11-14T20:09:50Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"ingress-nginx-admission-create-429zk_ingress-nginx\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Nov 14 20:09:51 minikube cri-dockerd[1339]: time="2023-11-14T20:09:51Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"ingress-nginx-admission-create-429zk_ingress-nginx\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Nov 14 20:09:51 minikube cri-dockerd[1339]: time="2023-11-14T20:09:51Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-p4tj4_kube-system\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Nov 14 20:09:53 minikube dockerd[1125]: time="2023-11-14T20:09:53.122952667Z" level=info msg="ignoring event" container=959cde08f68045201a43a76de907b026fa71202508a967eee7273f9336bfd40a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 14 20:09:54 minikube dockerd[1125]: time="2023-11-14T20:09:54.325821719Z" level=info msg="ignoring event" container=032456491fa1342f61e9e7c33f3ef1caa5f4c070a96801ab0ac8a0b2a67ae98d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 14 20:10:01 minikube cri-dockerd[1339]: time="2023-11-14T20:10:01Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/54daeb27dc11358c203e93add5228e4ada9addee2615a23fcd103e718ab79a63/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local deutsche-boerse.de rdev.deutsche-boerse.de dbgcloud.io localdomain myguest.virtualbox.org options ndots:5]"
Nov 14 20:10:01 minikube cri-dockerd[1339]: time="2023-11-14T20:10:01Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"ingress-nginx-admission-create-429zk_ingress-nginx\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Nov 14 20:10:01 minikube cri-dockerd[1339]: time="2023-11-14T20:10:01Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7f9cb0cc4005698d7cfb2b477d5d7b73be5703c7950342e1819ed892b0e8baa5/resolv.conf as [nameserver 172.23.253.222 nameserver 172.23.201.222 nameserver 10.0.2.3 search deutsche-boerse.de rdev.deutsche-boerse.de dbgcloud.io localdomain myguest.virtualbox.org]"
Nov 14 20:10:02 minikube cri-dockerd[1339]: time="2023-11-14T20:10:02Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-p4tj4_kube-system\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Nov 14 20:10:02 minikube dockerd[1125]: time="2023-11-14T20:10:02.443689339Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Nov 14 20:10:02 minikube dockerd[1125]: time="2023-11-14T20:10:02.443844414Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Nov 14 20:10:02 minikube dockerd[1125]: time="2023-11-14T20:10:02.447797749Z" level=error msg="Handler for POST /v1.42/images/create returned error: Get \"https://registry.k8s.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Nov 14 20:10:02 minikube cri-dockerd[1339]: time="2023-11-14T20:10:02Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-p4tj4_kube-system\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Nov 14 20:10:03 minikube cri-dockerd[1339]: time="2023-11-14T20:10:03Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"ingress-nginx-admission-create-429zk_ingress-nginx\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Nov 14 20:10:04 minikube dockerd[1125]: time="2023-11-14T20:10:04.530486921Z" level=info msg="ignoring event" container=7f9cb0cc4005698d7cfb2b477d5d7b73be5703c7950342e1819ed892b0e8baa5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 14 20:10:04 minikube dockerd[1125]: time="2023-11-14T20:10:04.799706554Z" level=info msg="ignoring event" container=e8dc1b11bf95634964b8094edd2226359d3a5529ddae5c276629e0fdb11e55a9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 14 20:10:09 minikube cri-dockerd[1339]: time="2023-11-14T20:10:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/26323674ee7aa4ae56a3bfd6874c7fd8fe4f7b204fd564f393eb9dbf6030a6f7/resolv.conf as [nameserver 172.23.253.222 nameserver 172.23.201.222 nameserver 10.0.2.3 search deutsche-boerse.de rdev.deutsche-boerse.de dbgcloud.io localdomain myguest.virtualbox.org]"
Nov 14 20:10:10 minikube cri-dockerd[1339]: time="2023-11-14T20:10:10Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-p4tj4_kube-system\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Nov 14 20:10:11 minikube cri-dockerd[1339]: time="2023-11-14T20:10:11Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ec170200f355b117e154090a7a17f965f2621447f698f38b5daa677c84d98dd1/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local deutsche-boerse.de rdev.deutsche-boerse.de dbgcloud.io localdomain myguest.virtualbox.org options ndots:5]"
Nov 14 20:10:13 minikube cri-dockerd[1339]: time="2023-11-14T20:10:13Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-p4tj4_kube-system\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Nov 14 20:10:13 minikube cri-dockerd[1339]: time="2023-11-14T20:10:13Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"ingress-nginx-admission-patch-zb8cs_ingress-nginx\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Nov 14 20:10:15 minikube cri-dockerd[1339]: time="2023-11-14T20:10:15Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"ingress-nginx-admission-create-429zk_ingress-nginx\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Nov 14 20:10:15 minikube dockerd[1125]: time="2023-11-14T20:10:15.135109163Z" level=info msg="ignoring event" container=26323674ee7aa4ae56a3bfd6874c7fd8fe4f7b204fd564f393eb9dbf6030a6f7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 14 20:10:15 minikube cri-dockerd[1339]: time="2023-11-14T20:10:15Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"ingress-nginx-admission-patch-zb8cs_ingress-nginx\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Nov 14 20:10:17 minikube dockerd[1125]: time="2023-11-14T20:10:17.221808779Z" level=info msg="ignoring event" container=ec170200f355b117e154090a7a17f965f2621447f698f38b5daa677c84d98dd1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 14 20:10:20 minikube cri-dockerd[1339]: time="2023-11-14T20:10:20Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/88c3405e2e61f6abb8c1b399a70865b2be4f8c780dc02d1bfec96bcc475a46ae/resolv.conf as [nameserver 172.23.253.222 nameserver 172.23.201.222 nameserver 10.0.2.3 search deutsche-boerse.de rdev.deutsche-boerse.de dbgcloud.io localdomain myguest.virtualbox.org]"
Nov 14 20:10:21 minikube cri-dockerd[1339]: time="2023-11-14T20:10:21Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-p4tj4_kube-system\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Nov 14 20:10:21 minikube cri-dockerd[1339]: time="2023-11-14T20:10:21Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e5c81900f8170a8455ed080e43ec3a95832c2780734f9adbee098bd8825d1f32/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local deutsche-boerse.de rdev.deutsche-boerse.de dbgcloud.io localdomain myguest.virtualbox.org options ndots:5]"
Nov 14 20:10:21 minikube cri-dockerd[1339]: time="2023-11-14T20:10:21Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"ingress-nginx-admission-patch-zb8cs_ingress-nginx\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Nov 14 20:10:21 minikube cri-dockerd[1339]: time="2023-11-14T20:10:21Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-p4tj4_kube-system\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Nov 14 20:10:22 minikube cri-dockerd[1339]: time="2023-11-14T20:10:22Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"ingress-nginx-admission-patch-zb8cs_ingress-nginx\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Nov 14 20:10:23 minikube dockerd[1125]: time="2023-11-14T20:10:23.381283388Z" level=info msg="ignoring event" container=88c3405e2e61f6abb8c1b399a70865b2be4f8c780dc02d1bfec96bcc475a46ae module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 14 20:10:24 minikube dockerd[1125]: time="2023-11-14T20:10:24.522540453Z" level=info msg="ignoring event" container=e5c81900f8170a8455ed080e43ec3a95832c2780734f9adbee098bd8825d1f32 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 14 20:10:32 minikube cri-dockerd[1339]: time="2023-11-14T20:10:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a6c247c153f69a97f5fa412bf609dbfe69cef66afb9e2683d3ef8408603ee182/resolv.conf as [nameserver 172.23.253.222 nameserver 172.23.201.222 nameserver 10.0.2.3 search deutsche-boerse.de rdev.deutsche-boerse.de dbgcloud.io localdomain myguest.virtualbox.org]"
Nov 14 20:10:32 minikube cri-dockerd[1339]: time="2023-11-14T20:10:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4ec39c7102cffa94076bed21c676baaa5999787d90d912fc1d147407538bb441/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local deutsche-boerse.de rdev.deutsche-boerse.de dbgcloud.io localdomain myguest.virtualbox.org options ndots:5]"
Nov 14 20:10:34 minikube cri-dockerd[1339]: time="2023-11-14T20:10:34Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-p4tj4_kube-system\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Nov 14 20:10:34 minikube cri-dockerd[1339]: time="2023-11-14T20:10:34Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"ingress-nginx-admission-patch-zb8cs_ingress-nginx\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Nov 14 20:10:34 minikube cri-dockerd[1339]: time="2023-11-14T20:10:34Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-p4tj4_kube-system\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"

* 
* ==> container status <==
* CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
6c98ebf6a341d       ead0a4a53df89       4 minutes ago       Exited              coredns                   17                  d021cdde9ea60       coredns-5dd5756b68-p4tj4
67d464747ebb7       6e38f40d628db       33 minutes ago      Running             storage-provisioner       1                   2ccf006b16bc2       storage-provisioner
4c9e00a0a8441       bfc896cf80fba       34 minutes ago      Running             kube-proxy                0                   6869b405d7c4a       kube-proxy-v4hdg
97788a3c9550e       6d1b4fd1b182d       35 minutes ago      Running             kube-scheduler            0                   049942520ac7a       kube-scheduler-minikube
d2fa38c41030c       73deb9a3f7025       35 minutes ago      Running             etcd                      0                   1ac35e69b39f8       etcd-minikube
a0da0bda8a53b       10baa1ca17068       35 minutes ago      Running             kube-controller-manager   0                   ed36e181a8e73       kube-controller-manager-minikube
b4a4c13823a8e       5374347291230       35 minutes ago      Running             kube-apiserver            0                   e77b2a6638498       kube-apiserver-minikube

* 
* ==> coredns [6c98ebf6a341] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] plugin/health: Going into lameduck mode for 5s
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": dial tcp 10.96.0.1:443: connect: network is unreachable
[INFO] 127.0.0.1:59267 - 47581 "HINFO IN 2425367077594553680.9111814707189624802. udp 57 false 512" - - 0 5.000050014s
[ERROR] plugin/errors: 2 2425367077594553680.9111814707189624802. HINFO: dial udp 172.23.253.222:53: connect: network is unreachable
[INFO] 127.0.0.1:53985 - 22464 "HINFO IN 2425367077594553680.9111814707189624802. udp 57 false 512" - - 0 5.014145323s
[ERROR] plugin/errors: 2 2425367077594553680.9111814707189624802. HINFO: dial udp 172.23.201.222:53: connect: network is unreachable

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2023_11_14T20_35_52_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 14 Nov 2023 19:35:40 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 14 Nov 2023 20:10:40 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 14 Nov 2023 20:06:45 +0000   Tue, 14 Nov 2023 19:35:40 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 14 Nov 2023 20:06:45 +0000   Tue, 14 Nov 2023 19:35:40 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 14 Nov 2023 20:06:45 +0000   Tue, 14 Nov 2023 19:35:40 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 14 Nov 2023 20:06:45 +0000   Tue, 14 Nov 2023 19:35:50 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                6
  ephemeral-storage:  40302856Ki
  hugepages-2Mi:      0
  memory:             9846432Ki
  pods:               110
Allocatable:
  cpu:                6
  ephemeral-storage:  40302856Ki
  hugepages-2Mi:      0
  memory:             9846432Ki
  pods:               110
System Info:
  Machine ID:                 0f821b8c832c4a549a5fc7d7fd457ff4
  System UUID:                6429d8e3-5589-4e5a-a97a-e8145e778fe2
  Boot ID:                    6ad8ab6e-6d88-45f8-a0e1-0aa7e1c6ccd3
  Kernel Version:             4.18.0-477.27.1.el8_8.x86_64
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (11 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  ingress-nginx               ingress-nginx-admission-create-429zk         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6m36s
  ingress-nginx               ingress-nginx-admission-patch-zb8cs          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6m36s
  ingress-nginx               ingress-nginx-controller-7c6974c4d8-f969f    100m (1%!)(MISSING)     0 (0%!)(MISSING)      90Mi (0%!)(MISSING)        0 (0%!)(MISSING)         6m37s
  kube-system                 coredns-5dd5756b68-p4tj4                     100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (1%!)(MISSING)     34m
  kube-system                 etcd-minikube                                100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         34m
  kube-system                 kindnet-bblwt                                100m (1%!)(MISSING)     100m (1%!)(MISSING)   50Mi (0%!)(MISSING)        50Mi (0%!)(MISSING)      34m
  kube-system                 kube-apiserver-minikube                      250m (4%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         34m
  kube-system                 kube-controller-manager-minikube             200m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         34m
  kube-system                 kube-proxy-v4hdg                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         34m
  kube-system                 kube-scheduler-minikube                      100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         34m
  kube-system                 storage-provisioner                          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         34m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                950m (15%!)(MISSING)  100m (1%!)(MISSING)
  memory             310Mi (3%!)(MISSING)  220Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 34m                kube-proxy       
  Normal  NodeHasSufficientMemory  35m (x8 over 35m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    35m (x8 over 35m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     35m (x7 over 35m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  35m                kubelet          Updated Node Allocatable limit across pods
  Normal  Starting                 34m                kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  34m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    34m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     34m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeNotReady             34m                kubelet          Node minikube status is now: NodeNotReady
  Normal  NodeAllocatableEnforced  34m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeReady                34m                kubelet          Node minikube status is now: NodeReady
  Normal  RegisteredNode           34m                node-controller  Node minikube event: Registered Node minikube in Controller


Name:               minikube-m02
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube-m02
                    kubernetes.io/os=linux
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 14 Nov 2023 19:37:40 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube-m02
  AcquireTime:     <unset>
  RenewTime:       Tue, 14 Nov 2023 20:10:40 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 14 Nov 2023 20:08:03 +0000   Tue, 14 Nov 2023 19:37:39 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 14 Nov 2023 20:08:03 +0000   Tue, 14 Nov 2023 19:37:39 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 14 Nov 2023 20:08:03 +0000   Tue, 14 Nov 2023 19:37:39 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 14 Nov 2023 20:08:03 +0000   Tue, 14 Nov 2023 19:37:41 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.3
  Hostname:    minikube-m02
Capacity:
  cpu:                6
  ephemeral-storage:  40302856Ki
  hugepages-2Mi:      0
  memory:             9846432Ki
  pods:               110
Allocatable:
  cpu:                6
  ephemeral-storage:  40302856Ki
  hugepages-2Mi:      0
  memory:             9846432Ki
  pods:               110
System Info:
  Machine ID:                 0c4bda6d840a4b1897ad6e85d9ef8700
  System UUID:                c43973c6-8f6b-49e6-af9b-192ae460e2b8
  Boot ID:                    6ad8ab6e-6d88-45f8-a0e1-0aa7e1c6ccd3
  Kernel Version:             4.18.0-477.27.1.el8_8.x86_64
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.1.0/24
PodCIDRs:                     10.244.1.0/24
Non-terminated Pods:          (2 in total)
  Namespace                   Name                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                ------------  ----------  ---------------  -------------  ---
  kube-system                 kindnet-xvqqw       100m (1%!)(MISSING)     100m (1%!)(MISSING)   50Mi (0%!)(MISSING)        50Mi (0%!)(MISSING)      33m
  kube-system                 kube-proxy-nxvf2    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         33m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests   Limits
  --------           --------   ------
  cpu                100m (1%!)(MISSING)  100m (1%!)(MISSING)
  memory             50Mi (0%!)(MISSING)  50Mi (0%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)     0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)     0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 32m                kube-proxy       
  Normal  NodeHasSufficientMemory  33m (x3 over 33m)  kubelet          Node minikube-m02 status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    33m (x3 over 33m)  kubelet          Node minikube-m02 status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     33m (x3 over 33m)  kubelet          Node minikube-m02 status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  33m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeReady                33m                kubelet          Node minikube-m02 status is now: NodeReady
  Normal  RegisteredNode           33m                node-controller  Node minikube-m02 event: Registered Node minikube-m02 in Controller


Name:               minikube-m03
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube-m03
                    kubernetes.io/os=linux
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 14 Nov 2023 19:39:36 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube-m03
  AcquireTime:     <unset>
  RenewTime:       Tue, 14 Nov 2023 20:10:41 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 14 Nov 2023 20:09:53 +0000   Tue, 14 Nov 2023 19:39:36 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 14 Nov 2023 20:09:53 +0000   Tue, 14 Nov 2023 19:39:36 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 14 Nov 2023 20:09:53 +0000   Tue, 14 Nov 2023 19:39:36 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 14 Nov 2023 20:09:53 +0000   Tue, 14 Nov 2023 19:39:39 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.4
  Hostname:    minikube-m03
Capacity:
  cpu:                6
  ephemeral-storage:  40302856Ki
  hugepages-2Mi:      0
  memory:             9846432Ki
  pods:               110
Allocatable:
  cpu:                6
  ephemeral-storage:  40302856Ki
  hugepages-2Mi:      0
  memory:             9846432Ki
  pods:               110
System Info:
  Machine ID:                 3a83c566c8dd40f9a30597a5fce8214d
  System UUID:                d244ed19-f155-4c1a-badb-22b2b131c04f
  Boot ID:                    6ad8ab6e-6d88-45f8-a0e1-0aa7e1c6ccd3
  Kernel Version:             4.18.0-477.27.1.el8_8.x86_64
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.2.0/24
PodCIDRs:                     10.244.2.0/24
Non-terminated Pods:          (2 in total)
  Namespace                   Name                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                ------------  ----------  ---------------  -------------  ---
  kube-system                 kindnet-8bwpb       100m (1%!)(MISSING)     100m (1%!)(MISSING)   50Mi (0%!)(MISSING)        50Mi (0%!)(MISSING)      31m
  kube-system                 kube-proxy-dh69n    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         31m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests   Limits
  --------           --------   ------
  cpu                100m (1%!)(MISSING)  100m (1%!)(MISSING)
  memory             50Mi (0%!)(MISSING)  50Mi (0%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)     0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)     0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 31m                kube-proxy       
  Normal  Starting                 31m                kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  31m                kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           31m                node-controller  Node minikube-m03 event: Registered Node minikube-m03 in Controller
  Normal  NodeHasSufficientMemory  31m (x7 over 31m)  kubelet          Node minikube-m03 status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    31m (x7 over 31m)  kubelet          Node minikube-m03 status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     31m (x7 over 31m)  kubelet          Node minikube-m03 status is now: NodeHasSufficientPID
  Normal  NodeReady                31m (x2 over 31m)  kubelet          Node minikube-m03 status is now: NodeReady


Name:               minikube-m04
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube-m04
                    kubernetes.io/os=linux
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 14 Nov 2023 19:41:46 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube-m04
  AcquireTime:     <unset>
  RenewTime:       Tue, 14 Nov 2023 20:10:44 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 14 Nov 2023 20:07:10 +0000   Tue, 14 Nov 2023 19:41:46 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 14 Nov 2023 20:07:10 +0000   Tue, 14 Nov 2023 19:41:46 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 14 Nov 2023 20:07:10 +0000   Tue, 14 Nov 2023 19:41:46 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 14 Nov 2023 20:07:10 +0000   Tue, 14 Nov 2023 19:41:47 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.5
  Hostname:    minikube-m04
Capacity:
  cpu:                6
  ephemeral-storage:  40302856Ki
  hugepages-2Mi:      0
  memory:             9846432Ki
  pods:               110
Allocatable:
  cpu:                6
  ephemeral-storage:  40302856Ki
  hugepages-2Mi:      0
  memory:             9846432Ki
  pods:               110
System Info:
  Machine ID:                 cf2a4b04282c4c8394106b8c26a8d897
  System UUID:                4710dccd-50a8-46f0-b6bb-c0a3acba9df9
  Boot ID:                    6ad8ab6e-6d88-45f8-a0e1-0aa7e1c6ccd3
  Kernel Version:             4.18.0-477.27.1.el8_8.x86_64
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.3.0/24
PodCIDRs:                     10.244.3.0/24
Non-terminated Pods:          (2 in total)
  Namespace                   Name                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                ------------  ----------  ---------------  -------------  ---
  kube-system                 kindnet-vnsk8       100m (1%!)(MISSING)     100m (1%!)(MISSING)   50Mi (0%!)(MISSING)        50Mi (0%!)(MISSING)      28m
  kube-system                 kube-proxy-ss8rk    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         28m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests   Limits
  --------           --------   ------
  cpu                100m (1%!)(MISSING)  100m (1%!)(MISSING)
  memory             50Mi (0%!)(MISSING)  50Mi (0%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)     0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)     0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 28m                kube-proxy       
  Normal  NodeHasSufficientMemory  28m (x3 over 28m)  kubelet          Node minikube-m04 status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    28m (x3 over 28m)  kubelet          Node minikube-m04 status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     28m (x3 over 28m)  kubelet          Node minikube-m04 status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  28m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeReady                28m                kubelet          Node minikube-m04 status is now: NodeReady
  Normal  RegisteredNode           28m                node-controller  Node minikube-m04 event: Registered Node minikube-m04 in Controller


Name:               minikube-m05
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube-m05
                    kubernetes.io/os=linux
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 14 Nov 2023 19:44:18 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube-m05
  AcquireTime:     <unset>
  RenewTime:       Tue, 14 Nov 2023 20:10:43 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 14 Nov 2023 20:09:44 +0000   Tue, 14 Nov 2023 19:44:18 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 14 Nov 2023 20:09:44 +0000   Tue, 14 Nov 2023 19:44:18 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 14 Nov 2023 20:09:44 +0000   Tue, 14 Nov 2023 19:44:18 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 14 Nov 2023 20:09:44 +0000   Tue, 14 Nov 2023 19:44:21 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.6
  Hostname:    minikube-m05
Capacity:
  cpu:                6
  ephemeral-storage:  40302856Ki
  hugepages-2Mi:      0
  memory:             9846432Ki
  pods:               110
Allocatable:
  cpu:                6
  ephemeral-storage:  40302856Ki
  hugepages-2Mi:      0
  memory:             9846432Ki
  pods:               110
System Info:
  Machine ID:                 5f3c93c6e78b4d79932083385a109900
  System UUID:                4d19fb87-84c8-456f-9c0d-745603156173
  Boot ID:                    6ad8ab6e-6d88-45f8-a0e1-0aa7e1c6ccd3
  Kernel Version:             4.18.0-477.27.1.el8_8.x86_64
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.4.0/24
PodCIDRs:                     10.244.4.0/24
Non-terminated Pods:          (2 in total)
  Namespace                   Name                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                ------------  ----------  ---------------  -------------  ---
  kube-system                 kindnet-p8llj       100m (1%!)(MISSING)     100m (1%!)(MISSING)   50Mi (0%!)(MISSING)        50Mi (0%!)(MISSING)      26m
  kube-system                 kube-proxy-p8lzh    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         26m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests   Limits
  --------           --------   ------
  cpu                100m (1%!)(MISSING)  100m (1%!)(MISSING)
  memory             50Mi (0%!)(MISSING)  50Mi (0%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)     0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)     0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 26m                kube-proxy       
  Normal  Starting                 26m                kubelet          Starting kubelet.
  Normal  RegisteredNode           26m                node-controller  Node minikube-m05 event: Registered Node minikube-m05 in Controller
  Normal  NodeHasSufficientMemory  26m (x2 over 26m)  kubelet          Node minikube-m05 status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    26m (x2 over 26m)  kubelet          Node minikube-m05 status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     26m (x2 over 26m)  kubelet          Node minikube-m05 status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  26m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeReady                26m                kubelet          Node minikube-m05 status is now: NodeReady

* 
* ==> dmesg <==
* [Nov14 19:23] Booted with the nomodeset parameter. Only the system framebuffer will be available
[  +0.000000] APIC calibration not consistent with PM-Timer: 141ms instead of 100ms
[  +0.000258] TSC synchronization [CPU#0 -> CPU#1]:
[  +0.000000] Measured 44392 cycles TSC warp between CPUs, turning off TSC clock.
[  +0.000380]  #2 #3 #4 #5
[  +0.037614] acpi PNP0A03:00: fail to add MMCONFIG information, can't access extended PCI configuration space under this bridge.
[  +0.701201] hrtimer: interrupt took 3374965 ns
[  +0.803938] Warning: Unmaintained hardware is detected:  e1000:100E:8086 @ 0000:00:03.0
[  +3.284915] printk: systemd: 15 output lines suppressed due to ratelimiting

* 
* ==> etcd [d2fa38c41030] <==
